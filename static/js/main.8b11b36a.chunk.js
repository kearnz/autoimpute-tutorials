(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{33:function(e,t,n){e.exports=n(394)},39:function(e,t,n){},394:function(e,t,n){"use strict";n.r(t);var a=n(0),r=n.n(a),i=n(28),s=n.n(i),l=(n(39),n(2)),m=n(3),o=n(5),u=n(4),p=n(6),c=(n(40),n(12)),d=n.n(c),h=function(e){function t(){return Object(l.a)(this,t),Object(o.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(p.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return r.a.createElement("div",null,r.a.createElement("h2",null,"THIS IS THE CONTACT PAGE"))}}]),t}(a.Component),g=n(1),f=n.n(g),_=n(396),b=n(32),y=function(e){function t(){return Object(l.a)(this,t),Object(o.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(p.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){var e=this.props,t=e.language,n=e.value;return r.a.createElement(_.a,{language:t,style:b.docco},n)}}]),t}(a.PureComponent);y.defaultProps={language:null};var E=y,v=function(e){function t(){return Object(l.a)(this,t),Object(o.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(p.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return r.a.createElement(f.a,{source:'\n## Welcome to Autoimpute!\n[![PyPI version](https://badge.fury.io/py/autoimpute.svg)](https://badge.fury.io/py/autoimpute) [![Build Status](https://travis-ci.com/kearnz/autoimpute.svg?branch=master)](https://travis-ci.com/kearnz/autoimpute) [![Documentation Status](https://readthedocs.org/projects/autoimpute/badge/?version=latest)](https://autoimpute.readthedocs.io/en/latest/?badge=latest) [![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/) [![Python 3.6+](https://img.shields.io/badge/python-3.6+-blue.svg)](https://www.python.org/downloads/release/python-360/)\n\n[Autoimpute](https://pypi.org/project/autoimpute/) is a Python package for analysis and implementation of <b>Imputation Methods!</b>\n\n[Check out our docs](https://autoimpute.readthedocs.io/en/latest/) to pick up the developer guide.\n\n### Installation\n* `Autoimpute` is now **registered with PyPI!** Download with `pip install autoimpute`.\n* The latest version of `Autoimpute` is `0.11.3`.\n* If `pip` cached an older version, try `pip install --no-cache-dir --upgrade autoimpute`.\n* If you want to work with the development branch, use the script below:\n\n### Motivation\nMost machine learning algorithms expect clean and complete datasets, but real-world data is messy and missing. Unfortunately, handling missing data is quite complex, so programming languages generally punt this responsibility to the end user. By default, R drops all records with missing data - a method that is easy to implement but often problematic in practice. For richer imputation strategies, R has multiple packages to deal with missing data (`MICE`, `Amelia`, `TSImpute`, etc.). Python users are not as fortunate. Python\'s `scikit-learn` throws a runtime error when an end user deploys models on datasets with missing records, and few third-party packages exist to handle imputation end-to-end.\n\nTherefore, this package aids the Python user by providing more clarity to the imputation process, making imputation methods more accessible, and measuring the impact imputation methods have in supervised regression and classification. In doing so, this package brings missing data imputation methods to the Python world and makes them work nicely in Python machine learning projects (and specifically ones that utilize `scikit-learn`). Lastly, this package provides its own implementation of supervised machine learning methods that extend both `scikit-learn` and `statsmodels` to mutiply imputed datasets.\n\n### Main Features\n* Utility functions to examine patterns in missing data and decide on relevant features for imputation\n* Missingness classifier and automatic missing data test set generator\n* Native handling for categorical variables (as predictors and targets of imputation)\n* Single and multiple imputation classes for `pandas` `DataFrames`\n* Custom visualization support for utility functions and imputation methods\n* Analysis methods and pooled parameter inference using multiply imputed datasets\n* Numerous imputation methods, as specified in the table below:\n\n### Imputation Methods Supported\n\n| Univariate                  | Multivariate                        | Time Series / Interpolation\n| :-------------------------- | :---------------------------------- | ---------------------------\n| Mean                        | Linear Regression                   | Linear \n| Median                      | Binomial Logistic Regression        | Quadratic \n| Mode                        | Multinomial Logistic Regression     | Cubic\n| Random                      | Stochastic Regression               | Polynomial\n| Norm                        | Bayesian Linear Regression          | Spline\n| Categorical                 | Bayesian Binary Logistic Regression | Time-weighted\n|                             | Predictive Mean Matching            | Next Obs Carried Backward\n|                             | Local Residual Draws                | Last Obs Carried Forward\n\n### Todo\n* Additional cross-sectional methods, including random forest, KNN, EM, and maximum likelihood\n* Additional time-series methods, including EWMA, ARIMA, Kalman filters, and state-space models\n* Extended support for visualization of missing data patterns, imputation methods, and analysis models\n* Additional support for analysis metrics and analyis models after multiple imputation\n* Multiprocessing and GPU support for larger datasets, as well as integration with `dask` DataFrames\n\n### Example Usage\nAutoimpute is designed to be user friendly and flexible. When performing imputation, Autoimpute fits directly into `scikit-learn` machine learning projects. Imputers inherit from sklearn\'s `BaseEstimator` and `TransformerMixin` and implement `fit` and `transform` methods, making them valid Transformers in an sklearn pipeline.\n\nRight now, there are two `Imputer` classes we\'ll work with:\n```python\nfrom autoimpute.imputations import SingleImputer, MultipleImputer\nsi = SingleImputer() # imputation methods, passing through the data once\nmi = MultipleImputer() # imputation methods, passing through the data multiple times\n```\n\nImputations can be as simple as:\n```python\n# simple example using default instance of MultipleImputer\nimp = MultipleImputer()\n\n# fit transform returns a generator by default, calculating each imputation method lazily\nimp.fit_transform(data)\n```\n\nOr quite complex, such as:\n```python\n# create a complex instance of the MultipleImputer\n# Here, we specify strategies by column and predictors for each column\n# We also specify what additional arguments any pmm strategies should take\nimp = MultipleImputer(\n    n=10,\n    strategy={"salary": "pmm", "gender": "bayesian binary logistic", "age": "norm"},\n    predictors={"salary": "all", "gender": ["salary", "education", "weight"]},\n    imp_kwgs={"pmm": {"fill_value": "random"}},\n    visit="left-to-right",\n    return_list=True\n)\n\n# Because we set return_list=True, imputations are done all at once, not evaluated lazily.\n# This will return M*N, where M is the number of imputations and N is the size of original dataframe.\nimp.fit_transform(data)\n```\n\nAutoimpute also extends supervised machine learning methods from `scikit-learn` and `statsmodels` to apply them to multiply imputed datasets (using the `MultipleImputer` under the hood). Right now, Autoimpute supports linear regression and binary logistic regression. Additional supervised methods are currently under development.\n\nAs with Imputers, Autoimpute\'s analysis methods can be simple or complex:\n```python\nfrom autoimpute.analysis import MiLinearRegression\n\n# By default, use statsmodels OLS and MultipleImputer()\nsimple_lm = MiLinearRegression()\n\n# fit the model on each multiply imputed dataset and pool parameters\nsimple_lm.fit(X_train, y_train)\n\n# get summary of fit, which includes pooled parameters under Rubin\'s rules\n# also provides diagnostics related to analysis after multiple imputation\nsimple_lm.summary()\n\n# make predictions on a new dataset using pooled parameters\npredictions = simple_lm.predict(X_test)\n\n# Control both the regression used and the MultipleImputer itself\nmultiple_imputer_arguments = dict(\n    n=3,\n    strategy={"salary": "pmm", "gender": "bayesian binary logistic", "age": "norm"},\n    predictors={"salary": "all", "gender": ["salary", "education", "weight"]},\n    imp_kwgs={"pmm": {"fill_value": "random"}},\n    visit="left-to-right"\n)\ncomplex_lm = MiLinearRegression(\n    model_lib="sklearn", # use sklearn linear regression\n    mi_kwgs=multiple_imputer_arguments # control the multiple imputer\n)\n\n# fit the model on each multiply imputed dataset\ncomplex_lm.fit(X_train, y_train)\n\n# get summary of fit, which includes pooled parameters under Rubin\'s rules\n# also provides diagnostics related to analysis after multiple imputation\ncomplex_lm.summary()\n\n# make predictions on new dataset using pooled parameters\npredictions = complex_lm.predict(X_test)\n```\n\nNote that we can also pass a pre-specified `MultipleImputer` to either analysis model instead of using `mi_kwgs`. The option is ours, and it\'s a matter of preference. If we pass a pre-specified `MultipleImputer`, anything in `mi_kwgs` is ignored, although the `mi_kwgs` argument is still validated.\n\n```python\nfrom autoimpute.imputations import MultipleImputer\nfrom autoimpute.analysis import MiLinearRegression\n\n# create a multiple imputer first\ncustom_imputer = MultipleImputer(n=3, strategy="pmm", return_list=True)\n\n# pass the imputer to a linear regression model\ncomplex_lm = MiLinearRegression(mi=custom_imputer, model_lib="statsmodels")\n\n# proceed the same as the previous examples\ncomplex_lm.fit(X_train, y_train).predict(X_test)\ncomplex_lm.summary()\n```\n\n### Versions and Dependencies\n* Python 3.6+\n* Dependencies:\n    - `numpy` >= 1.15.4\n    - `scipy` >= 1.2.1\n    - `pandas` >= 0.20.3\n    - `statsmodels` >= 0.9.0\n    - `scikit-learn` >= 0.20.2\n    - `xgboost` >= 0.83\n    - `pymc3` >= 3.5\n    - `seaborn` >= 0.9.0\n    - `missingno` >= 0.4.1\n\n*A note for Windows Users*:\n* Autoimpute works on Windows but users may have trouble with pymc3 for bayesian methods. [(See discourse)](https://discourse.pymc.io/t/an-error-message-about-cant-pickle-fortran-objects/1073)\n* Users may receive a runtime error `\u2018can\u2019t pickle fortran objects\u2019` when sampling using multiple chains.\n* There are a couple of things to do to try to overcome this error:\n    - Reinstall theano and pymc3. Make sure to delete .theano cache in your home folder.\n    - Upgrade joblib in the process, which is reponsible for generating the error (pymc3 uses joblib under the hood).\n    - Set `cores=1` in `pm.sample`. This should be a last resort, as it means posterior sampling will use 1 core only. Not using multiprocessing will slow down bayesian imputation methods significantly.\n* Reach out and let us know if you\'ve worked through this issue successfully on Windows and have a better solution!\n\n### License\nDistributed under the MIT license. See [LICENSE](https://github.com/kearnz/autoimpute/blob/master/LICENSE) for more information.\n\n### Contributing\nGuidelines for contributing to our project. See [CONTRIBUTING](https://github.com/kearnz/autoimpute/blob/master/CONTRIBUTING.md) for more information.\n\n### Contributor Code of Conduct\nAdapted from Contributor Covenant, version 1.0.0. See [Code of Conduct](https://github.com/kearnz/autoimpute/blob/master/CODE_OF_CONDUCT.md) for more information.\n',escapeHtml:!1,renderers:{code:E}})}}]),t}(a.Component),w=function(e){function t(){return Object(l.a)(this,t),Object(o.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(p.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return r.a.createElement("div",{className:"comparing-imputation-methods"},r.a.createElement("div",{className:"cim-header-01"},r.a.createElement(f.a,{source:"\n## Using Autoimpute to Compare Imputation Methods\n\nThis tutorial examimes the effect of imputation methods from the `Autoimpute` package. The tutorial includes:  \n\n1. Generating Data  \n2. Exploring Missingness  \n3. Imputation Methods  \n4. Impact of imputation on Covariance and Correlation  \n\n### 1. Generating Data\n* In the section below, we generate two variables, **x** and **y**, that are positively correlated but display heteroscedasticity.\n* Thus, **x** and **y** get larger (or smaller) together. As a result, the variance between the two variables is not constant.     \n* We then introduce 30% missingness within **y**. **x** remains completely observed.  \n* The code on the left generates the data and creates a function to visualize the data.  \n* The code on the right displays the resulting dataframe and its associated plot.   \n",escapeHtml:!1,renderers:{code:E}})),r.a.createElement("div",{className:"cim-code-1"},r.a.createElement(f.a,{source:'\n```python\n\'\'\'Simulating data and defining a joint plot b/w two variables\'\'\'\n\n# plotting specification and imports needed\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm, binom\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings("ignore")\n\n# seed to follow along\nnp.random.seed(654654)\n\n# generate 1500 data points\nN = np.arange(1500)\n\n# create correlated, heteroskedastic random variables\na = 0\nb = 1\neps = np.array([norm(0, n).rvs() for n in N])\ny = (a + b*N + eps) / 100                         \nx = (N + norm(10, 10).rvs(len(N))) / 100\n \n# 30% missingness created artificially\ny[binom(1, 0.3).rvs(len(N)) == 1] = np.nan\n\n# collect results in a dataframe \ndata_het_miss = pd.DataFrame({"y": y, "x": x})\n\n# create a scatter plot function to display the results of our dataframe\ndef scatter_dists(data, x="x", y="y", a=0.5, joints_color="navy", \n                  markers="o", marginals=dict(rug=True, kde=True)):\n\n    sns.set(context="talk")\n    joint_kws = dict(\n        facecolor=joints_color,\n        edgecolor=joints_color,\n        marker=markers\n    )\n    sns.jointplot(\n        x=x, y=y, data=data, alpha=a, height=8.27,\n        joint_kws=joint_kws, marginal_kws=marginals\n    )\n```\n',escapeHtml:!1,renderers:{code:E}})),r.a.createElement("div",{className:"cim-table-code-1"},r.a.createElement(f.a,{source:"\n```python\ndata_het_miss.head(7)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,"0.295288"),r.a.createElement("td",null,"0.000000")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.086210"),r.a.createElement("td",null,"0.002255")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.227369"),r.a.createElement("td",null,"0.003869")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.194216"),r.a.createElement("td",null,"0.046370")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.094630"),r.a.createElement("td",null,"0.081440")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.292320"),r.a.createElement("td",null,"NaN")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.198131"),r.a.createElement("td",null,"0.067222"))))),r.a.createElement("div",{className:"cim-df-scatter-01"},r.a.createElement(f.a,{source:"\n```python\nscatter_dists(data_het_miss)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"scatter01",src:"https://kearnz.github.io/autoimpute-tutorials/img/comparing/cim-df-scatter01.png"})))}}]),t}(a.Component),I=function(e){function t(){return Object(l.a)(this,t),Object(o.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(p.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return r.a.createElement("div",{className:"imputer-I"},r.a.createElement(f.a,{source:"\n## Getting the Most out of the Imputer Classes: Part I\n---\nThis tutorial is part I of a comprehensive overview of `Autoimpute` Imputers. It includes:  \n1. Motivation for Imputation in the First Place  \n2. The Design Considerations behind Autoimpute Imputers  \n\n### 1. Motivation for Imputation in the First Place\nLet's revisit why multiple imputation is necessary. A user wants to perform analysis on a dataset using some sort of **analysis model** such as linear regression or logistic regression. The dataset of interest contains one or more predictors, **X**, and some response **y**. The analysis model produces a function that best explains the relationship between **X** and **y**. Let's generate some sample data below. To keep things simple, our data set contains just one predictor, **x**, and a response **y**.\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:'\n\n```python\n# imports\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm, binom\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings("ignore")\nsns.set(context="talk", rc={\'figure.figsize\':(11.7,8.27)})\n\n# helper functions used throughout this project\nprint_header = lambda msg: print(f"{msg}\n{\'-\'*len(msg)}")\n\n# seed to follow along\nnp.random.seed(654654)\n\n# generate 1500 data points\nN = np.arange(1500)\n\n# helper function for this data\nvary = lambda v: np.random.choice(np.arange(v))\n\n# create correlated, random variables\na = 2\nb = 1/2\neps = np.array([norm(0, vary(50)).rvs() for n in N])\ny = (a + b*N + eps) / 100                         \nx = (N + norm(10, vary(250)).rvs(len(N))) / 100\n \n# 20% missing in x, 30% missing in y\nx[binom(1, 0.2).rvs(len(N)) == 1] = np.nan\ny[binom(1, 0.3).rvs(len(N)) == 1] = np.nan\n\n# collect results in a dataframe \ndata_miss = pd.DataFrame({"y": y, "x": x})\nsns.scatterplot(x="x", y="y", data=data_miss)\nplt.show()\n```\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"imputer-I-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/imputer/imputer-I-scatter.png"}),r.a.createElement(f.a,{source:"\nThe plot suggests a linear relationship may exist between **x** and **y**. Let's fit a linear model to estimate that relationship.\n\n\n```python\nfrom sklearn.linear_model import LinearRegression\n\n# prep for regression\nX = data_miss.x.values.reshape(-1, 1) # reshape because one feature only\ny = data_miss.y\nlm = LinearRegression()\n\n# try to fit the model\nprint_header(\"Fitting linear model to estimate relationship between X and y\")\ntry:\n    lm.fit(X, y)\nexcept ValueError as ve:\n    print(f\"{ve.__class__.__name__}: {ve}\")\n```\n\n    Fitting linear model to estimate relationship between X and y\n    -------------------------------------------------------------\n    ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n\n\n#### What Happened?\n`sklearn` threw a ValueError when we tried to fit a linear regression. The error occurred because our **dataset has missing data**. In our case, 20% of observations are missing in our predictor, and 30% of observations are missing in our response. `sklearn` cannot fit the analysis model unless our dataset is complete! That's no good - we can't model the relationship in our data, nor can we make predictions when new data arrives.\n\n#### What do we Do?\nIn order to proceed, we **need to handle the missing data in some way**. One option is simply removing records with missing data. This strategy allows the analysis model to run, but it may negatively affect the inference from that model. For now, we'll recommend against dropping missing records.\n\nIf we don't drop missing records, **then we must impute them**. Imputing data means coming up with plausible values to fill in missing records, which we must do to enable our analysis model to run. **Performing imputations is the primary concern of Autoimpute**. The next section introduces Autoimpute Imputers and familiarizes the reader with Autoimpute's package design.\n\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n\n### 2. The Design Considerations behind Autoimpute Imputers\nWe designed Autoimpute Imputers with three goals in mind:  \n* **Make Imputation Easy**. Imputation can be done in one line of code.  \n* **Make Imputation Familiar to Python Users.** Autoimpute Imputers follows the design patterns of sklearn.  \n* **Make Imputation Flexible**. Use an Imputer's default arguments or fine-tune Imputers case-by-case.  \n\nLet's explore each objective. We'll use the same dataset as above and stick with the `SingleImputer` for now. The `MutlipleImputer` extends the `SingleImputer` and therefore contains all the arguments that the `SingleImputer` does. Therefore, we will explain design considerations using the `SingleImputer`. In Part III of this series, we'll address additional arguments of and considerations for the `MultipleImputer`.\n\n#### 2.1. Make Imputation Easy\n\nAs promised, we can impute a dataset with exactly one line of code. In the code section below, we'll demonstrate the one line of code that imputes all missing data in a dataset. First, we'll observe how many records are missing. Then, we'll perform imputation and verify that missing records have been imputed.\n\n\n```python\n# amount of missing data before imputation\nprint_header(\"Amount of data missing before imputation takes place\")\npd.DataFrame(data_miss.isnull().sum(), columns=[\"records missing\"]).T\n```\n\n    Amount of data missing before imputation takes place\n    ----------------------------------------------------\n\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"label"),r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"records missing")),r.a.createElement("td",null,"285"),r.a.createElement("td",null,"491")))),r.a.createElement(f.a,{source:'\n\n```python\nfrom autoimpute.imputations import SingleImputer\nprint_header("Imputing missing data in one line of code with the default SingleImputer")\ndata_imputed_once = SingleImputer().fit_transform(data_miss)\nprint("Imputation Successful!")\n```\n\n    Imputing missing data in one line of code with the default SingleImputer\n    ------------------------------------------------------------------------\n\n\n    Auto-assigning NUTS sampler...\n    Initializing NUTS using jitter+adapt_diag...\n    Multiprocess sampling (4 chains in 4 jobs)\n    NUTS: [\u03c3, beta, alpha]\n    Sampling 4 chains: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [00:03<00:00, 1912.67draws/s]\n    Auto-assigning NUTS sampler...\n    Initializing NUTS using jitter+adapt_diag...\n    Multiprocess sampling (4 chains in 4 jobs)\n    NUTS: [\u03c3, beta, alpha]\n    Sampling 4 chains: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [00:03<00:00, 1722.73draws/s]\n\n\n    Imputation Successful!\n\n\n\n```python\n# amount of missing data before imputation\nprint_header("Amount of data missing after imputation takes place")\npd.DataFrame(data_imputed_once.isnull().sum(), columns=["records missing"]).T\n```\n\n    Amount of data missing after imputation takes place\n    ---------------------------------------------------\n\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"label"),r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"records missing")),r.a.createElement("td",null,"0"),r.a.createElement("td",null,"0")))),r.a.createElement(f.a,{source:"\n\n### 2.2 Make Imputation Familiar to Python Users\nAutoimpute follows `sklearn` API design. This design choice comes with a number of benefits:  \n* If you are familiar with `sklearn`, there is essentially no learning curve to start using `Autoimpute`.  \n* Imputers inherit from sklearn's BaseEstimator and TransformerMixin and leverage methods from these Parent classes.  \n* Imputers can `fit_transform` the same dataset, or `fit` and `Imputer` to `transform` new data with the same features.  \n* Imputers fit in sklearn Pipelines (although Autoimpute includes ML models designed for multiply imputed data).  \n\nThe code segments below demonstrate Autoimpute's ease of use and familiarity.\n\nFirst, Autoimpute Imputers inherit from `BaseEstimator` and `TransformerMixin` classes from `sklearn`.\n\n\n```python\nprint_header(\"Parent Classes to SingleImputer and MultipleImpuer\")\nprint(list(map(lambda cls_: cls_.__name__, SingleImputer().__class__.__bases__)))\n```\n\n    Parent Classes to SingleImputer and MultipleImpuer\n    --------------------------------------------------\n    ['BaseImputer', 'BaseEstimator', 'TransformerMixin']\n\n\nAs with `sklearn` `Transformers`, Autoimpute Imputers set smart defaults for all its arguments. Imputers leverage the `__repr__` special method inherited from the `BaseEstimator`, so users can quickly examine the default values an `Imputer` sets.\n\n\n```python\nSingleImputer()\n```\n\n\n\n\n    SingleImputer(copy=True, imp_kwgs=None, predictors='all', seed=None,\n           strategy='default predictive', visit='default')\n\n\n\nBecause Autoimpute Imputers are valid `sklearn` `Transformers`, they implement the `fit`, `transform`, and `fit_transform` methods, which should be familiar to anyone who has preprocessed data using `sklearn`. The `fit` step returns an instance of the `Imputer` class, and the `transform` step returns a **transformed dataset**. Imputers can fit and transform a dataset in one go by using `fit_transform`. We demonstrate this process below, transforming our missing dataset using mean imputation.\n\n\n```python\nprint_header(\"Original dataset with missing values\")\ndata_miss.head(10)\n```\n\n    Original dataset with missing values\n    ------------------------------------\n\n\n\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,"2.534781"),r.a.createElement("td",null,"0.257901")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.118755"),r.a.createElement("td",null,"-0.114591")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.184612"),r.a.createElement("td",null,"0.018801")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.442059"),r.a.createElement("td",null,"0.047037")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.109537"),r.a.createElement("td",null,"0.229042")),r.a.createElement("tr",null,r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"NaN")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.962892"),r.a.createElement("td",null,"0.000090")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.028426"),r.a.createElement("td",null,"NaN")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.949358"),r.a.createElement("td",null,"NaN")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.728996"),r.a.createElement("td",null,"0.068058")))),r.a.createElement(f.a,{source:'\n\n```python\nprint_header("Transforming the missing dataset with mean imputation")\nimputer = SingleImputer(strategy="mean")\ndata_imputed = imputer.fit_transform(data_miss)\ndata_imputed.head(10)\n```\n\n    Transforming the missing dataset with mean imputation\n    -----------------------------------------------------\n\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,"2.534781"),r.a.createElement("td",null,"0.257901")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.118755"),r.a.createElement("td",null,"-0.114591")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.184612"),r.a.createElement("td",null,"0.018801")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.442059"),r.a.createElement("td",null,"0.047037")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.109537"),r.a.createElement("td",null,"0.229042")),r.a.createElement("tr",null,r.a.createElement("td",null,"7.577981"),r.a.createElement("td",null,"3.747201")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.962892"),r.a.createElement("td",null,"0.000090")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.028426"),r.a.createElement("td",null,"3.747201")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.949358"),r.a.createElement("td",null,"3.747201")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.728996"),r.a.createElement("td",null,"0.068058")))),r.a.createElement(f.a,{source:"\n\nThe transformed dataset contains imputations in place of previously missing values. Here, we used mean imputation, although we offer many imputation strategies that may be more appropriate. We show these strategies in Part II of this series. We can **easily retrieve the index of the the imputed values** should we want to assess the imputations themselves. The index of imputed values live in the Imputer's `imputed_` attribute. As with `sklearn`, public attributes of Imputers contain an underscore suffix.\n\nThe `imputed_` attribute returns a dictionary, where each key is a column and its value is a list with the index of each imputation for that column. These indices represent where data was originally missing but is now imputed. We can use these indices to find the location of imputations within a transformed dataset. The code below accesses the first 5 imputations for **x** and **y**. Because we used mean imputation, the imputation values are the same. In this case, the **record with index 5** had missing values for both **x** and **y**.\n\n\n```python\nprint_header(\"Showing the first 5 imputations for column x\")\ndata_imputed.loc[imputer.imputed_['x'], 'x'].head()\n```\n\n    Showing the first 5 imputations for column x\n    --------------------------------------------\n\n    5     7.577981\n    13    7.577981\n    24    7.577981\n    29    7.577981\n    30    7.577981\n    Name: x, dtype: float64\n\n\n\n\n```python\nprint_header(\"Showing the first 5 imputations for column y\")\ndata_imputed.loc[imputer.imputed_['y'], 'y'].head()\n```\n\n    Showing the first 5 imputations for column y\n    --------------------------------------------\n\n    5     3.747201\n    7     3.747201\n    8     3.747201\n    13    3.747201\n    18    3.747201\n    Name: y, dtype: float64\n\n\n\n#### 2.3. Make Imputation Flexible\nTo make Imputers powerful, we need to make them flexible. While Autoimpute Imputers set default values for their arguments, the Imputers' arguments give users full control of how to impute each column within a dataset. Let's see what arguments Imputers take. The code below prints each argument's name and its default value.\n\n\n```python\nprint_header(\"Printing arguments for the SingleImputer as well as their default values\")\nfor k,v in SingleImputer().get_params().items():\n    print(f\"Argument: {k}; Default: {v}\")\n```\n\n    Printing arguments for the SingleImputer as well as their default values\n    ------------------------------------------------------------------------\n    Argument: copy; Default: True\n    Argument: imp_kwgs; Default: None\n    Argument: predictors; Default: all\n    Argument: seed; Default: None\n    Argument: strategy; Default: default predictive\n    Argument: visit; Default: default\n\n\nBelow is a brief description of each argument. Part II of this series explains how to use these arguments to control imputation.  \n* **copy**: Whether or not to copy the dataset during the transform place. If False, imputations happen in place.  \n* **imp_kwgs**: Dictionary of arguments to fine-tune imputation on a specific column or for a specific imputation strategy.  \n* **predictors**: Which columns to use to predict imputations for a given column's imputation model.  \n* **seed**: Seed number makes imputations reproducible.  \n* **strategy**: The imputation strategy to use. Can specify for all columns or each column individually.  \n* **visit**: The order in which columns should be \"visited\" or imputed.  \n\n\nThis concludes Part I of this series. This tutorial is merely an introduction to the `Autoimpute` Imputers. We motivated the need for Autoimpute Imputers and introduced the package design at the highest level, but we barely scratched the surface for what Imputers can do. In the code segment above, we peeked at the arguments an `Imputer` takes. These arguments give the user full control over the imputation process and hold the power and flexibility behind the `Imputer` classes. Part II of this series walks through these arguments in depth, giving users examples of how the arguments tailor imputation to fit specific needs.\n\n\n",escapeHtml:!1,renderers:{code:E}}))}}]),t}(a.Component),M=function(e){function t(){return Object(l.a)(this,t),Object(o.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(p.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return r.a.createElement("h1",null,"Imputer Mechanics II")}}]),t}(a.Component),k=function(e){function t(){return Object(l.a)(this,t),Object(o.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(p.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return r.a.createElement("h1",null,"Imputer Mechanics III")}}]),t}(a.Component),x=function(e){function t(){return Object(l.a)(this,t),Object(o.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(p.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return r.a.createElement("div",{className:"end-to-end"},r.a.createElement("div",{className:"ete-header-01"},r.a.createElement(f.a,{source:"\n## An End-to-End Analysis of Missing Data using Autoimpute\n\n* This tutorial demonstrates how an analyst can utilize the `Autoimpute` package to handle missing data from exploration through supervised learning.  \n* It presents **two examples, side by side**. The process for each example is the same, but the underlying datasets (and their missingness) differ.  \n* The example on the left explores data with **MCAR** missingness, while the example on the right examines data with **MAR** missingness.  \n",escapeHtml:!1,renderers:{code:E}})),r.a.createElement("div",{className:"ete-mcar"},r.a.createElement(f.a,{source:"\n## MCAR\n---\n* 500 observations for two features, **predictor x** and **response y**  \n* Correlation between the variables is **0.7**\n* Predictor **x** is fully observed, while response **y** is missing **40%** of the observations\n* The underlying missingness mechanism is **MCAR**  \n* Imputation methods explored: **mean**, **least squares**, **PMM**    \n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n### Imports and Data Preparation\n---\n```python\n'''Handling imports for analysis'''\n%matplotlib inline\n\n# general modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# autoimpute imports - utilities & visuals\nfrom autoimpute.utils import md_pattern, proportions\nfrom autoimpute.visuals import plot_md_locations, plot_md_percent\nfrom autoimpute.visuals import plot_imp_dists, plot_imp_boxplots\nfrom autoimpute.visuals import plot_imp_swarm, plot_imp_strip\nfrom autoimpute.visuals import plot_imp_scatter\n\n# autoimpute imports - imputations & analysis\nfrom autoimpute.imputations import MultipleImputer\nfrom autoimpute.analysis import MiLinearRegression\n\n# reading the full dataset and mcar dataset\nfull = pd.read_csv(\"full.csv\")\nmcar = pd.read_csv(\"mcar.csv\")\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n### Percent Missing by Feature\n---\n```python\n'''MCAR percent plot'''\nplot_md_percent(mcar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarPercent",className:"ete-percent",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-plot-md-percent.png"}),r.a.createElement(f.a,{source:"\n### Location of Missingness by Feature\n---\n```python\n'''MCAR location plot'''\nplot_md_locations(mcar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarLocation",className:"ete-locations",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-plot-md-locations.png"}),r.a.createElement(f.a,{source:"\n### Mean Imputation\n---\n```python\n'''MCAR mean imputation'''\n\n# create the mean imputer\nmi_mean_mcar = MultipleImputer(\n    strategy=\"mean\", n=5, return_list=True, seed=101\n)\n\n# print the mean imputer to console\nprint(mi_mean_mcar)\n\n# perform mean imputation procedure\nimp_mean_mcar = mi_mean_mcar.fit_transform(mcar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarMeanImputer",className:"ete-mean-imputer",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mean-imputer.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after Mean Imputation\n---\n```python\n\'\'\'MCAR distribution plots after mean imputation\'\'\'\n\n# distribution plot for mean imputation\nplot_imp_dists(\n    d=imp_mean_mcar,\n    mi=mi_mean_mcar, \n    col="y",\n    title="Distributions after Mean Imputation: MCAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for mean imputation\nplot_imp_boxplots(\n    d=imp_mean_mcar,\n    mi=mi_mean_mcar,\n    col="y",\n    title="Boxplots after Mean Imputation: MCAR"\n)\n\n# strip plot for mean imputation\nplot_imp_strip(\n    d=imp_mean_mcar,\n    mi=mi_mean_mcar,\n    col="y",\n    title="Imputed vs Observed Dists after Mean Imputation: MCAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarMeanImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-dist.png"}),r.a.createElement("img",{alt:"mcarMeanImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-box.png"}),r.a.createElement("img",{alt:"mcarMeanImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-strip.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after Least Squares Imputation\n---\n```python\n\'\'\'MCAR distribution plots after least squares imputation\'\'\'\n\n# create the least squares imputer\nmi_ls_mcar = MultipleImputer(\n    strategy="least squares", n=5, return_list=True, seed=101\n)\n\n# perform least squares imputation procedure\nimp_ls_mcar = mi_ls_mcar.fit_transform(mcar)\n\n# distribution plot for least squares imputation\nplot_imp_dists(\n    d=imp_ls_mcar,\n    mi=mi_ls_mcar, \n    col="y",\n    title="Distributions after Least Squares Imputation: MCAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for least squares imputation\nplot_imp_boxplots(\n    d=imp_ls_mcar,\n    mi=mi_ls_mcar,\n    col="y",\n    title="Boxplots after Least Squares Imputation: MCAR"\n)\n\n# strip plot for least squares imputation\nplot_imp_strip(\n    d=imp_ls_mcar,\n    mi=mi_ls_mcar,\n    col="y",\n    title="Imputed vs Observed Dists after Least Squares Imputation: MCAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarLsImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-dist.png"}),r.a.createElement("img",{alt:"mcarLsImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-box.png"}),r.a.createElement("img",{alt:"mcarLsImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-strip.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after PMM Imputation\n---\n```python\n\'\'\'MCAR distribution plots after PMM imputation\'\'\'\n\n# create the PMM imputer\nmi_pmm_mcar = MultipleImputer(\n    strategy="pmm", n=5, return_list=True, seed=101\n)\n\n# perform PMM imputation procedure\nimp_pmm_mcar = mi_pmm_mcar.fit_transform(mcar)\n\n# distribution plot for PMM imputation\nplot_imp_dists(\n    d=imp_pmm_mcar,\n    mi=mi_pmm_mcar, \n    col="y",\n    title="Distributions after PMM Imputation: MCAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for PMM imputation\nplot_imp_boxplots(\n    d=imp_pmm_mcar,\n    mi=mi_pmm_mcar,\n    col="y",\n    title="Boxplots after PMM Imputation: MCAR"\n)\n\n# swarm plot for PMM imputation\nplot_imp_swarm(\n    d=imp_pmm_mcar,\n    mi=mi_pmm_mcar,\n    col="y",\n    title="Imputed vs Observed Dists after PMM Imputation: MCAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarPmmImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-dist.png"}),r.a.createElement("img",{alt:"mcarPmmImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-box.png"}),r.a.createElement("img",{alt:"mcarPmmImputerSwarm",className:"ete-swarm",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-swarm.png"}),r.a.createElement(f.a,{source:'\n### Linear Regression on Multiply Imputed Data\n---\n```python\n\'\'\'Regression after Multiple Imputation on MCAR\'\'\'\n\n# NOTE: Full & Listwise delete code not included but appear in output\n\n# create the regression using custom imputers\nlm_mean_mcar = MiLinearRegression(mi=mi_mean_mcar)\nlm_ls_mcar = MiLinearRegression(mi=mi_ls_mcar)\nlm_pmm_mcar = MiLinearRegression(mi=mi_pmm_mcar)\nmodels_mcar = [lm_mean_mcar, lm_ls_mcar, lm_pmm_mcar]\n\n# a bit of manipulation to create one dataframe\nget_coeff = lambda lm_, x: lm_.summary().loc["x"].to_frame()\nres_mcar = pd.concat([get_coeff(lm, "x") for lm in models_mcar], axis=1)\nres_mcar.columns = ["mean", "least squares", "pmm"]\nres_mcar = res_mcar.T[["coefs", "std", "vw", "vb", "vt"]]\n\n# show the results\nres_mcar\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"method"),r.a.createElement("th",null,"coefs"),r.a.createElement("th",null,"std"),r.a.createElement("th",null,"vw"),r.a.createElement("th",null,"vb"),r.a.createElement("th",null,"vt"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"full")),r.a.createElement("td",null,"0.70000"),r.a.createElement("td",null,"0.03200"),r.a.createElement("td",null,"0.00102"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00102")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"listwise delete")),r.a.createElement("td",null,"0.73915"),r.a.createElement("td",null,"0.04682"),r.a.createElement("td",null,"0.00219"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00219")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"mean")),r.a.createElement("td",null,"0.39330"),r.a.createElement("td",null,"0.03056"),r.a.createElement("td",null,"0.00093"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00093")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"least squares")),r.a.createElement("td",null,"0.73915"),r.a.createElement("td",null,"0.02570"),r.a.createElement("td",null,"0.00066"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00066")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"pmm")),r.a.createElement("td",null,"0.67692"),r.a.createElement("td",null,"0.05404"),r.a.createElement("td",null,"0.00128"),r.a.createElement("td",null,"0.00136"),r.a.createElement("td",null,"0.00292")))),r.a.createElement(f.a,{source:'\n### Scatterplots after Imputation Methods\n---\n```python\n\'\'\'Visualizing effect of imputation on regression for scatter\'\'\'\n\n# scatterplot for mean\nplot_imp_scatter(\n    d=mcar, x="x", y="y", strategy="mean", color="y",\n    title="Scatter after Mean Imputation: MCAR"\n)\n\n# scatterplot for least squares\nplot_imp_scatter(\n    d=mcar, x="x", y="y", strategy="least squares", color="y",\n    title="Scatter after Least Squares Imputation: MCAR"\n)\n\n# scatterplot for pmm\nplot_imp_scatter(\n    d=mcar, x="x", y="y", strategy="pmm", color="y",\n    title="Scatter after PMM Imputation: MCAR"\n)\n\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarMeanScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-scatter.png"}),r.a.createElement("img",{alt:"mcarLsScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-scatter.png"}),r.a.createElement("img",{alt:"mcarPmmScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-scatter.png"})),r.a.createElement("div",{className:"ete-mar"},r.a.createElement(f.a,{source:"\n## MAR\n---\n* 500 observations for two features, **predictor x** and **response y**  \n* Correlation between the variables is **0.7**\n* Predictor **x** is missing **40%** of the observations, while response **y** is fully observed  \n* The underlying missingness mechanism is **MAR**  \n* Imputation methods explored: **mean**, **least squares**, **PMM**  \n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n### Imports and Data Preparation\n---\n```python\n'''Handling imports for analysis'''\n%matplotlib inline\n\n# general modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# autoimpute imports - utilities & visuals\nfrom autoimpute.utils import md_pattern, proportions\nfrom autoimpute.visuals import plot_md_locations, plot_md_percent\nfrom autoimpute.visuals import plot_imp_dists, plot_imp_boxplots\nfrom autoimpute.visuals import plot_imp_swarm, plot_imp_strip\nfrom autoimpute.visuals import plot_imp_scatter\n\n# autoimpute imports - imputations & analysis\nfrom autoimpute.imputations import MultipleImputer\nfrom autoimpute.analysis import MiLinearRegression\n\n# reading the full dataset and mar dataset\nfull = pd.read_csv(\"full.csv\")\nmar = pd.read_csv(\"mar.csv\")\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n### Percent Missing by Feature\n---\n```python\n'''MAR percent plot'''\nplot_md_percent(mar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marPercent",className:"ete-percent",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-plot-md-percent.png"}),r.a.createElement(f.a,{source:"\n### Location of Missingness by Feature\n---\n```python\n'''MAR location plot'''\nplot_md_locations(mar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marLocation",className:"ete-locations",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-plot-md-locations.png"}),r.a.createElement(f.a,{source:"\n### Mean Imputation\n---\n```python\n'''MAR mean imputation'''\n\n# create the mean imputer\nmi_mean_mar = MultipleImputer(\n    strategy=\"mean\", n=5, return_list=True, seed=101\n)\n\n# print the mean imputer to console\nprint(mi_mean_mar)\n\n# perform mean imputation procedure\nimp_mean_mar = mi_mean_mar.fit_transform(mar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marMeanImputer",className:"ete-mean-imputer",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mean-imputer.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after Mean Imputation\n---\n```python\n\'\'\'MAR distribution plots after mean imputation\'\'\'\n\n# distribution plot for mean imputation\nplot_imp_dists(\n    d=imp_mean_mar,\n    mi=mi_mean_mar, \n    col="x",\n    title="Distributions after Mean Imputation: MAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for mean imputation\nplot_imp_boxplots(\n    d=imp_mean_mar,\n    mi=mi_mean_mar,\n    col="x",\n    title="Boxplots after Mean Imputation: MAR"\n)\n\n# strip plot for mean imputation\nplot_imp_strip(\n    d=imp_mean_mar,\n    mi=mi_mean_mar,\n    col="x",\n    title="Imputed vs Observed Dists after Mean Imputation: MAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marMeanImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-dist.png"}),r.a.createElement("img",{alt:"marMeanImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-box.png"}),r.a.createElement("img",{alt:"marMeanImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-strip.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after Least Squares Imputation\n---\n```python\n\'\'\'MAR distribution plots after least squares imputation\'\'\'\n\n# create the least squares imputer\nmi_ls_mar = MultipleImputer(\n    strategy="least squares", n=5, return_list=True, seed=101\n)\n\n# perform least squares imputation procedure\nimp_ls_mar = mi_ls_mar.fit_transform(mar)\n\n# distribution plot for least squares imputation\nplot_imp_dists(\n    d=imp_ls_mar,\n    mi=mi_ls_mar, \n    col="x",\n    title="Distributions after Least Squares Imputation: MAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for least squares imputation\nplot_imp_boxplots(\n    d=imp_ls_mar,\n    mi=mi_ls_mar,\n    col="x",\n    title="Boxplots after Least Squares Imputation: MAR"\n)\n\n# strip plot for least squares imputation\nplot_imp_strip(\n    d=imp_ls_mar,\n    mi=mi_ls_mar,\n    col="x",\n    title="Imputed vs Observed Dists after Least Squares Imputation: MAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marLsImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-dist.png"}),r.a.createElement("img",{alt:"marLsImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-box.png"}),r.a.createElement("img",{alt:"marLsImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-strip.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after PMM Imputation\n---\n```python\n\'\'\'MAR distribution plots after PMM imputation\'\'\'\n\n# create the PMM imputer\nmi_pmm_mar = MultipleImputer(\n    strategy="pmm", n=5, return_list=True, seed=101\n)\n\n# perform PMM imputation procedure\nimp_pmm_mar = mi_pmm_mar.fit_transform(mar)\n\n# distribution plot for PMM imputation\nplot_imp_dists(\n    d=imp_pmm_mar,\n    mi=mi_pmm_mar, \n    col="x",\n    title="Distributions after PMM Imputation: MAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for PMM imputation\nplot_imp_boxplots(\n    d=imp_pmm_mar,\n    mi=mi_pmm_mar,\n    col="x",\n    title="Boxplots after PMM Imputation: MAR"\n)\n\n# swarm plot for PMM imputation\nplot_imp_swarm(\n    d=imp_pmm_mar,\n    mi=mi_pmm_mar,\n    col="x",\n    title="Imputed vs Observed Dists after PMM Imputation: MAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marPmmImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-dist.png"}),r.a.createElement("img",{alt:"marPmmImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-box.png"}),r.a.createElement("img",{alt:"marPmmImputerSwarm",className:"ete-swarm",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-swarm.png"}),r.a.createElement(f.a,{source:'\n### Linear Regression on Multiply Imputed Data\n---\n```python\n\'\'\'Regression after Multiple Imputation on MAR\'\'\'\n\n# NOTE: Full & Listwise delete code not included but appear in output\n\n# create the regression using custom imputers\nlm_mean_mar = MiLinearRegression(mi=mi_mean_mar)\nlm_ls_mar = MiLinearRegression(mi=mi_ls_mar)\nlm_pmm_mar = MiLinearRegression(mi=mi_pmm_mar)\nmodels_mar = [lm_mean_mar, lm_ls_mar, lm_pmm_mar]\n\n# a bit of manipulation to create one dataframe\nget_coeff = lambda lm_, x: lm_.summary().loc["x"].to_frame()\nres_mar = pd.concat([get_coeff(lm, "x") for lm in models_mar], axis=1)\nres_mar.columns = ["mean", "least squares", "pmm"]\nres_mar = res_mar.T[["coefs", "std", "vw", "vb", "vt"]]\n\n# show the results\nres_mar\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"method"),r.a.createElement("th",null,"coefs"),r.a.createElement("th",null,"std"),r.a.createElement("th",null,"vw"),r.a.createElement("th",null,"vb"),r.a.createElement("th",null,"vt"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"full")),r.a.createElement("td",null,"0.70000"),r.a.createElement("td",null,"0.03200"),r.a.createElement("td",null,"0.00102"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00102")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"listwise delete")),r.a.createElement("td",null,"0.66180"),r.a.createElement("td",null,"0.03865"),r.a.createElement("td",null,"0.00149"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00149")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"mean")),r.a.createElement("td",null,"0.66180"),r.a.createElement("td",null,"0.04896"),r.a.createElement("td",null,"0.00240"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00240")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"least squares")),r.a.createElement("td",null,"0.86053"),r.a.createElement("td",null,"0.02889"),r.a.createElement("td",null,"0.00083"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00083")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"pmm")),r.a.createElement("td",null,"0.69690"),r.a.createElement("td",null,"0.03707"),r.a.createElement("td",null,"0.00096"),r.a.createElement("td",null,"0.00034"),r.a.createElement("td",null,"0.00137")))),r.a.createElement(f.a,{source:'\n### Scatterplots after Imputation Methods\n---\n```python\n\'\'\'Visualizing effect of imputation on regression for scatter\'\'\'\n\n# scatterplot for mean\nplot_imp_scatter(\n    d=mar, x="x", y="y", strategy="mean", color="x",\n    title="Scatter after Mean Imputation: MAR"\n)\n\n# scatterplot for least squares\nplot_imp_scatter(\n    d=mar, x="x", y="y", strategy="least squares", color="x",\n    title="Scatter after Least Squares Imputation: MAR"\n)\n\n# scatterplot for pmm\nplot_imp_scatter(\n    d=mar, x="x", y="y", strategy="pmm", color="x",\n    title="Scatter after PMM Imputation: MAR"\n)\n\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marMeanScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-scatter.png"}),r.a.createElement("img",{alt:"marLsScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-scatter.png"}),r.a.createElement("img",{alt:"marPmmScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-scatter.png"})))}}]),t}(a.Component),N=function(e){var t=window.location.pathname+window.location.search,n=e.path===t?"nav-item active":"nav-item",a=e.disabled?"nav-link disabled":"nav-link";return r.a.createElement("li",{className:n},r.a.createElement("a",{onClick:function(){return e.onClick()},href:e.path,className:a},e.name,e.path===t?r.a.createElement("span",{className:"sr-only"},"(current)"):""))},A=function(e){function t(e){var n;return Object(l.a)(this,t),(n=Object(o.a)(this,Object(u.a)(t).call(this,e))).state={isToggleOn:!1},n}return Object(p.a)(t,e),Object(m.a)(t,[{key:"showDropdown",value:function(e){e.preventDefault(),this.setState(function(e){return{isToggleOn:!e.isToggleOn}})}},{key:"render",value:function(){var e=this,t="dropdown-menu"+(this.state.isToggleOn?" show":"");return r.a.createElement("li",{className:"nav-item dropdown"},r.a.createElement("a",{className:"nav-link dropdown-toggle",id:"navbarDropdown",role:"button","data-toggle":"dropdown","aria-haspopup":"true","aria-expanded":"false",onClick:function(t){e.showDropdown(t)}},this.props.name),r.a.createElement("div",{className:t,"aria-labelledby":"navbarDropdown"},this.props.children))}}]),t}(a.Component),T=function(e){function t(e){var n;return Object(l.a)(this,t),(n=Object(o.a)(this,Object(u.a)(t).call(this,e))).state={activeKey:1,df:"/autoimpute-tutorials/"},n}return Object(p.a)(t,e),Object(m.a)(t,[{key:"handleClick",value:function(e){this.setState({activeKey:e}),d()(".dropdown-menu").hasClass("show")&&(d()(".dropdown-menu").trigger("click"),d()(".dropdown-menu").removeClass("show"))}},{key:"render",value:function(){return r.a.createElement("div",{className:"main-page"},r.a.createElement("nav",{className:"navbar navbar-expand-lg"},r.a.createElement("a",{className:"navbar-brand",href:this.state.df},"Autoimpute"),r.a.createElement("div",{className:"collapse navbar-collapse",id:"navbarSupportedContent"},r.a.createElement("ul",{className:"navbar-nav mr-auto"},r.a.createElement(N,{name:"Home",onClick:this.handleClick.bind(this,1)}),r.a.createElement(N,{name:"Contact",onClick:this.handleClick.bind(this,2)}),r.a.createElement(A,{name:"Tutorials"},r.a.createElement("a",{className:"dropdown-item",href:this.props.path,onClick:this.handleClick.bind(this,3.1)},"Exploring Missingness"),r.a.createElement("a",{className:"dropdown-item",href:this.props.path,onClick:this.handleClick.bind(this,3.2)},"Imputers: Part I"),r.a.createElement("a",{className:"dropdown-item",href:this.props.path,onClick:this.handleClick.bind(this,3.3)},"Imputers: Part II"),r.a.createElement("a",{className:"dropdown-item",href:this.state.path,onClick:this.handleClick.bind(this,3.4)},"Imputers: Part III"),r.a.createElement("a",{className:"dropdown-item",href:this.state.path,onClick:this.handleClick.bind(this,3.5)},"Comparing Imputation Methods"),r.a.createElement("a",{className:"dropdown-item",href:this.state.path,onClick:this.handleClick.bind(this,3.6)},"End-to-End Analysis"))))),r.a.createElement("div",{className:"content"},1===this.state.activeKey?r.a.createElement(v,null):null,2===this.state.activeKey?r.a.createElement(h,null):null,3.1===this.state.activeKey?"Coming Soon!":null,3.2===this.state.activeKey?r.a.createElement(I,null):null,3.3===this.state.activeKey?r.a.createElement(M,null):null,3.4===this.state.activeKey?r.a.createElement(k,null):null,3.5===this.state.activeKey?r.a.createElement(w,null):null,3.6===this.state.activeKey?r.a.createElement(x,null):null))}}]),t}(r.a.Component),C=function(e){function t(){return Object(l.a)(this,t),Object(o.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(p.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return r.a.createElement("div",null,r.a.createElement(T,null))}}]),t}(a.Component);Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));n(393);s.a.render(r.a.createElement(C,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then(function(e){e.unregister()})},40:function(e,t,n){}},[[33,1,2]]]);
//# sourceMappingURL=main.8b11b36a.chunk.js.map