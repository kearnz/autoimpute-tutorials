(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{33:function(e,t,n){e.exports=n(394)},39:function(e,t,n){},394:function(e,t,n){"use strict";n.r(t);var a=n(0),i=n.n(a),r=n(28),s=n.n(r),m=(n(39),n(2)),l=n(3),o=n(5),p=n(4),u=n(6),c=(n(40),n(12)),d=n.n(c),h=function(e){function t(){return Object(m.a)(this,t),Object(o.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement("div",null,i.a.createElement("h2",null,"THIS IS THE CONTACT PAGE"))}}]),t}(a.Component),g=n(1),_=n.n(g),f=n(396),b=n(32),E=function(e){function t(){return Object(m.a)(this,t),Object(o.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){var e=this.props,t=e.language,n=e.value;return i.a.createElement(f.a,{language:t,style:b.docco},n)}}]),t}(a.PureComponent);E.defaultProps={language:null};var y=E,v=function(e){function t(){return Object(m.a)(this,t),Object(o.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement(_.a,{source:'\n## Welcome to Autoimpute!\n[![PyPI version](https://badge.fury.io/py/autoimpute.svg)](https://badge.fury.io/py/autoimpute) [![Build Status](https://travis-ci.com/kearnz/autoimpute.svg?branch=master)](https://travis-ci.com/kearnz/autoimpute) [![Documentation Status](https://readthedocs.org/projects/autoimpute/badge/?version=latest)](https://autoimpute.readthedocs.io/en/latest/?badge=latest) [![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/) [![Python 3.6+](https://img.shields.io/badge/python-3.6+-blue.svg)](https://www.python.org/downloads/release/python-360/)\n\n[Autoimpute](https://pypi.org/project/autoimpute/) is a Python package for analysis and implementation of <b>Imputation Methods!</b>\n\n[Check out our docs](https://autoimpute.readthedocs.io/en/latest/) to pick up the developer guide.\n\n### Installation\n* `Autoimpute` is now **registered with PyPI!** Download with `pip install autoimpute`.\n* The latest version of `Autoimpute` is `0.11.3`.\n* If `pip` cached an older version, try `pip install --no-cache-dir --upgrade autoimpute`.\n* If you want to work with the development branch, use the script below:\n\n### Motivation\nMost machine learning algorithms expect clean and complete datasets, but real-world data is messy and missing. Unfortunately, handling missing data is quite complex, so programming languages generally punt this responsibility to the end user. By default, R drops all records with missing data - a method that is easy to implement but often problematic in practice. For richer imputation strategies, R has multiple packages to deal with missing data (`MICE`, `Amelia`, `TSImpute`, etc.). Python users are not as fortunate. Python\'s `scikit-learn` throws a runtime error when an end user deploys models on datasets with missing records, and few third-party packages exist to handle imputation end-to-end.\n\nTherefore, this package aids the Python user by providing more clarity to the imputation process, making imputation methods more accessible, and measuring the impact imputation methods have in supervised regression and classification. In doing so, this package brings missing data imputation methods to the Python world and makes them work nicely in Python machine learning projects (and specifically ones that utilize `scikit-learn`). Lastly, this package provides its own implementation of supervised machine learning methods that extend both `scikit-learn` and `statsmodels` to mutiply imputed datasets.\n\n### Main Features\n* Utility functions to examine patterns in missing data and decide on relevant features for imputation\n* Missingness classifier and automatic missing data test set generator\n* Native handling for categorical variables (as predictors and targets of imputation)\n* Single and multiple imputation classes for `pandas` `DataFrames`\n* Custom visualization support for utility functions and imputation methods\n* Analysis methods and pooled parameter inference using multiply imputed datasets\n* Numerous imputation methods, as specified in the table below:\n\n### Imputation Methods Supported\n\n| Univariate                  | Multivariate                        | Time Series / Interpolation\n| :-------------------------- | :---------------------------------- | ---------------------------\n| Mean                        | Linear Regression                   | Linear \n| Median                      | Binomial Logistic Regression        | Quadratic \n| Mode                        | Multinomial Logistic Regression     | Cubic\n| Random                      | Stochastic Regression               | Polynomial\n| Norm                        | Bayesian Linear Regression          | Spline\n| Categorical                 | Bayesian Binary Logistic Regression | Time-weighted\n|                             | Predictive Mean Matching            | Next Obs Carried Backward\n|                             | Local Residual Draws                | Last Obs Carried Forward\n\n### Todo\n* Additional cross-sectional methods, including random forest, KNN, EM, and maximum likelihood\n* Additional time-series methods, including EWMA, ARIMA, Kalman filters, and state-space models\n* Extended support for visualization of missing data patterns, imputation methods, and analysis models\n* Additional support for analysis metrics and analyis models after multiple imputation\n* Multiprocessing and GPU support for larger datasets, as well as integration with `dask` DataFrames\n\n### Example Usage\nAutoimpute is designed to be user friendly and flexible. When performing imputation, Autoimpute fits directly into `scikit-learn` machine learning projects. Imputers inherit from sklearn\'s `BaseEstimator` and `TransformerMixin` and implement `fit` and `transform` methods, making them valid Transformers in an sklearn pipeline.\n\nRight now, there are two `Imputer` classes we\'ll work with:\n```python\nfrom autoimpute.imputations import SingleImputer, MultipleImputer\nsi = SingleImputer() # imputation methods, passing through the data once\nmi = MultipleImputer() # imputation methods, passing through the data multiple times\n```\n\nImputations can be as simple as:\n```python\n# simple example using default instance of MultipleImputer\nimp = MultipleImputer()\n\n# fit transform returns a generator by default, calculating each imputation method lazily\nimp.fit_transform(data)\n```\n\nOr quite complex, such as:\n```python\n# create a complex instance of the MultipleImputer\n# Here, we specify strategies by column and predictors for each column\n# We also specify what additional arguments any pmm strategies should take\nimp = MultipleImputer(\n    n=10,\n    strategy={"salary": "pmm", "gender": "bayesian binary logistic", "age": "norm"},\n    predictors={"salary": "all", "gender": ["salary", "education", "weight"]},\n    imp_kwgs={"pmm": {"fill_value": "random"}},\n    visit="left-to-right",\n    return_list=True\n)\n\n# Because we set return_list=True, imputations are done all at once, not evaluated lazily.\n# This will return M*N, where M is the number of imputations and N is the size of original dataframe.\nimp.fit_transform(data)\n```\n\nAutoimpute also extends supervised machine learning methods from `scikit-learn` and `statsmodels` to apply them to multiply imputed datasets (using the `MultipleImputer` under the hood). Right now, Autoimpute supports linear regression and binary logistic regression. Additional supervised methods are currently under development.\n\nAs with Imputers, Autoimpute\'s analysis methods can be simple or complex:\n```python\nfrom autoimpute.analysis import MiLinearRegression\n\n# By default, use statsmodels OLS and MultipleImputer()\nsimple_lm = MiLinearRegression()\n\n# fit the model on each multiply imputed dataset and pool parameters\nsimple_lm.fit(X_train, y_train)\n\n# get summary of fit, which includes pooled parameters under Rubin\'s rules\n# also provides diagnostics related to analysis after multiple imputation\nsimple_lm.summary()\n\n# make predictions on a new dataset using pooled parameters\npredictions = simple_lm.predict(X_test)\n\n# Control both the regression used and the MultipleImputer itself\nmultiple_imputer_arguments = dict(\n    n=3,\n    strategy={"salary": "pmm", "gender": "bayesian binary logistic", "age": "norm"},\n    predictors={"salary": "all", "gender": ["salary", "education", "weight"]},\n    imp_kwgs={"pmm": {"fill_value": "random"}},\n    visit="left-to-right"\n)\ncomplex_lm = MiLinearRegression(\n    model_lib="sklearn", # use sklearn linear regression\n    mi_kwgs=multiple_imputer_arguments # control the multiple imputer\n)\n\n# fit the model on each multiply imputed dataset\ncomplex_lm.fit(X_train, y_train)\n\n# get summary of fit, which includes pooled parameters under Rubin\'s rules\n# also provides diagnostics related to analysis after multiple imputation\ncomplex_lm.summary()\n\n# make predictions on new dataset using pooled parameters\npredictions = complex_lm.predict(X_test)\n```\n\nNote that we can also pass a pre-specified `MultipleImputer` to either analysis model instead of using `mi_kwgs`. The option is ours, and it\'s a matter of preference. If we pass a pre-specified `MultipleImputer`, anything in `mi_kwgs` is ignored, although the `mi_kwgs` argument is still validated.\n\n```python\nfrom autoimpute.imputations import MultipleImputer\nfrom autoimpute.analysis import MiLinearRegression\n\n# create a multiple imputer first\ncustom_imputer = MultipleImputer(n=3, strategy="pmm", return_list=True)\n\n# pass the imputer to a linear regression model\ncomplex_lm = MiLinearRegression(mi=custom_imputer, model_lib="statsmodels")\n\n# proceed the same as the previous examples\ncomplex_lm.fit(X_train, y_train).predict(X_test)\ncomplex_lm.summary()\n```\n\n### Versions and Dependencies\n* Python 3.6+\n* Dependencies:\n    - `numpy` >= 1.15.4\n    - `scipy` >= 1.2.1\n    - `pandas` >= 0.20.3\n    - `statsmodels` >= 0.9.0\n    - `scikit-learn` >= 0.20.2\n    - `xgboost` >= 0.83\n    - `pymc3` >= 3.5\n    - `seaborn` >= 0.9.0\n    - `missingno` >= 0.4.1\n\n*A note for Windows Users*:\n* Autoimpute works on Windows but users may have trouble with pymc3 for bayesian methods. [(See discourse)](https://discourse.pymc.io/t/an-error-message-about-cant-pickle-fortran-objects/1073)\n* Users may receive a runtime error `\u2018can\u2019t pickle fortran objects\u2019` when sampling using multiple chains.\n* There are a couple of things to do to try to overcome this error:\n    - Reinstall theano and pymc3. Make sure to delete .theano cache in your home folder.\n    - Upgrade joblib in the process, which is reponsible for generating the error (pymc3 uses joblib under the hood).\n    - Set `cores=1` in `pm.sample`. This should be a last resort, as it means posterior sampling will use 1 core only. Not using multiprocessing will slow down bayesian imputation methods significantly.\n* Reach out and let us know if you\'ve worked through this issue successfully on Windows and have a better solution!\n\n### License\nDistributed under the MIT license. See [LICENSE](https://github.com/kearnz/autoimpute/blob/master/LICENSE) for more information.\n\n### Contributing\nGuidelines for contributing to our project. See [CONTRIBUTING](https://github.com/kearnz/autoimpute/blob/master/CONTRIBUTING.md) for more information.\n\n### Contributor Code of Conduct\nAdapted from Contributor Covenant, version 1.0.0. See [Code of Conduct](https://github.com/kearnz/autoimpute/blob/master/CODE_OF_CONDUCT.md) for more information.\n',escapeHtml:!1,renderers:{code:y}})}}]),t}(a.Component),M=function(e){function t(){return Object(m.a)(this,t),Object(o.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement("div",{className:"comparing-imputation-methods"},i.a.createElement("div",{className:"cim-header-01"},i.a.createElement(_.a,{source:"\n## Using Autoimpute to Compare Imputation Methods\n\nThis tutorial examimes the effect of imputation methods from the `Autoimpute` package. The tutorial includes:  \n\n1. Generating Data  \n2. Exploring Missingness  \n3. Imputation Methods  \n4. Impact of imputation on Covariance and Correlation  \n\n### 1. Generating Data\n* In the section below, we generate two variables, **x** and **y**, that are positively correlated but display heteroscedasticity.\n* Thus, **x** and **y** get larger (or smaller) together. As a result, the variance between the two variables is not constant.     \n* We then introduce 30% missingness within **y**. **x** remains completely observed.  \n* The code on the left generates the data and creates a function to visualize the data.  \n* The code on the right displays the resulting dataframe and its associated plot.   \n",escapeHtml:!1,renderers:{code:y}})),i.a.createElement("div",{className:"cim-code-1"},i.a.createElement(_.a,{source:'\n```python\n\'\'\'Simulating data and defining a joint plot b/w two variables\'\'\'\n\n# plotting specification and imports needed\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm, binom\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings("ignore")\n\n# seed to follow along\nnp.random.seed(654654)\n\n# generate 1500 data points\nN = np.arange(1500)\n\n# create correlated, heteroskedastic random variables\na = 0\nb = 1\neps = np.array([norm(0, n).rvs() for n in N])\ny = (a + b*N + eps) / 100                         \nx = (N + norm(10, 10).rvs(len(N))) / 100\n \n# 30% missingness created artificially\ny[binom(1, 0.3).rvs(len(N)) == 1] = np.nan\n\n# collect results in a dataframe \ndata_het_miss = pd.DataFrame({"y": y, "x": x})\n\n# create a scatter plot function to display the results of our dataframe\ndef scatter_dists(data, x="x", y="y", a=0.5, joints_color="navy", \n                  markers="o", marginals=dict(rug=True, kde=True)):\n\n    sns.set(context="talk")\n    joint_kws = dict(\n        facecolor=joints_color,\n        edgecolor=joints_color,\n        marker=markers\n    )\n    sns.jointplot(\n        x=x, y=y, data=data, alpha=a, height=8.27,\n        joint_kws=joint_kws, marginal_kws=marginals\n    )\n```\n',escapeHtml:!1,renderers:{code:y}})),i.a.createElement("div",{className:"cim-table-code-1"},i.a.createElement(_.a,{source:"\n```python\ndata_het_miss.head(7)\n```\n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement("table",{border:"1",className:"dataframe"},i.a.createElement("thead",null,i.a.createElement("tr",null,i.a.createElement("th",null,"index"),i.a.createElement("th",null,"x"),i.a.createElement("th",null,"y"))),i.a.createElement("tbody",null,i.a.createElement("tr",null,i.a.createElement("th",null,"0"),i.a.createElement("td",null,"0.295288"),i.a.createElement("td",null,"0.000000")),i.a.createElement("tr",null,i.a.createElement("th",null,"1"),i.a.createElement("td",null,"0.086210"),i.a.createElement("td",null,"0.002255")),i.a.createElement("tr",null,i.a.createElement("th",null,"2"),i.a.createElement("td",null,"0.227369"),i.a.createElement("td",null,"0.003869")),i.a.createElement("tr",null,i.a.createElement("th",null,"3"),i.a.createElement("td",null,"0.194216"),i.a.createElement("td",null,"0.046370")),i.a.createElement("tr",null,i.a.createElement("th",null,"4"),i.a.createElement("td",null,"0.094630"),i.a.createElement("td",null,"0.081440")),i.a.createElement("tr",null,i.a.createElement("th",null,"5"),i.a.createElement("td",null,"0.292320"),i.a.createElement("td",null,"NaN")),i.a.createElement("tr",null,i.a.createElement("th",null,"6"),i.a.createElement("td",null,"0.198131"),i.a.createElement("td",null,"0.067222"))))),i.a.createElement("div",{className:"cim-df-scatter-01"},i.a.createElement(_.a,{source:"\n```python\nscatter_dists(data_het_miss)\n```\n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"scatter01",src:"https://kearnz.github.io/autoimpute-tutorials/img/comparing/cim-df-scatter01.png"})))}}]),t}(a.Component),w=function(e){function t(){return Object(m.a)(this,t),Object(o.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement("div",{className:"end-to-end"},i.a.createElement("div",{className:"ete-header-01"},i.a.createElement(_.a,{source:"\n## An End-to-End Analysis of Missing Data using Autoimpute\n\nThis tutorial walks through all the features of the the `Autoimpute` package. \n\nIt demonstrates how an end-user can utilize `Autoimpute` to handle missing data from exploration to supervised learning. The tutorial includes:  \n\n* Exploring Missing Data and the underlying Missingness Mechanisms  \n* Deletion and Imputation Methods to Account for Missing Data  \n* Supervised Learning on imputed data via Linear Regression  \n\nThe tutorial presents **two examples, side by side**. The process for each example is the same, but the underlying datasets (and their missingness) differ.\n\nThe dataset on the left has missingness that is **MCAR**, while the dataset on the right has misingness that is **MAR**\n",escapeHtml:!1,renderers:{code:y}})),i.a.createElement("div",{className:"ete-mcar"},i.a.createElement(_.a,{source:"\n## MCAR\n---\n* 500 observations for two features, **predictor x** and **response y**  \n* Correlation between the variables is **0.7**\n* Predictor **x** is fully observed, while response **y** is missing **40%** of the observations\n* The underlying missingness mechanism is **MCAR**  \n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement(_.a,{source:"\n### Imports and Data Preparation\n---\n```python\n'''Handling imports for analysis'''\n%matplotlib inline\n\n# general modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# autoimpute imports - utilities & visuals\nfrom autoimpute.utils import md_pattern, proportions\nfrom autoimpute.visuals import plot_md_locations, plot_md_percent\nfrom autoimpute.visuals import plot_imp_dists, plot_imp_boxplots\nfrom autoimpute.visuals import plot_imp_swarm, plot_imp_strip\nfrom autoimpute.visuals import plot_imp_scatter\n\n# autoimpute imports - imputations & analysis\nfrom autoimpute.imputations import MultipleImputer\nfrom autoimpute.analysis import MiLinearRegression\n\n# reading the full dataset and mcar dataset\nfull = pd.read_csv(\"full.csv\")\nmcar = pd.read_csv(\"mcar.csv\")\n```\n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement(_.a,{source:"\n### Percent Missing by Feature\n---\n```python\n'''MCAR percent plot'''\nplot_md_percent(mcar)\n```\n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"mcarPercent",className:"ete-percent",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-plot-md-percent.png"}),i.a.createElement(_.a,{source:"\n### Location of Missingness by Feature\n---\n```python\n'''MCAR location plot'''\nplot_md_locations(mcar)\n```\n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"mcarLocation",className:"ete-locations",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-plot-md-locations.png"}),i.a.createElement(_.a,{source:"\n### Mean Imputation\n---\n```python\n'''MCAR mean imputation'''\n\n# create the mean imputer\nmi_mean_mcar = MultipleImputer(\n    strategy=\"mean\", n=5, return_list=True, seed=101\n)\n\n# print the mean imputer to console\nprint(mi_mean_mcar)\n\n# perform mean imputation procedure\nimp_mean_mcar = mi_mean_mcar.fit_transform(mcar)\n```\n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"mcarMeanImputer",className:"ete-mean-imputer",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mean-imputer.png"}),i.a.createElement(_.a,{source:'\n### Distribution Plots after Mean Imputation\n---\n```python\n\'\'\'MCAR distribution plots after mean imputation\'\'\'\n\n# distribution plot for mean imputation\nplot_imp_dists(\n    d=imp_mean_mcar,\n    mi=mi_mean_mcar, \n    col="y",\n    title="Distributions after Mean Imputation: MCAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for mean imputation\nplot_imp_boxplots(\n    d=imp_mean_mcar,\n    mi=mi_mean_mcar,\n    col="y",\n    title="Boxplots after Mean Imputation: MCAR"\n)\n\n# strip plot for mean imputation\nplot_imp_strip(\n    d=imp_mean_mcar,\n    mi=mi_mean_mcar,\n    col="y",\n    title="Imputed vs Observed Dists after Mean Imputation: MCAR"\n)\n```\n',escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"mcarMeanImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-dist.png"}),i.a.createElement("img",{alt:"mcarMeanImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-box.png"}),i.a.createElement("img",{alt:"mcarMeanImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-strip.png"}),i.a.createElement(_.a,{source:'\n### Distribution Plots after Least Squares Imputation\n---\n```python\n\'\'\'MCAR distribution plots after least squares imputation\'\'\'\n\n# create the least squares imputer\nmi_ls_mcar = MultipleImputer(\n    strategy="least squares", n=5, return_list=True, seed=101\n)\n\n# perform least squares imputation procedure\nimp_ls_mcar = mi_ls_mcar.fit_transform(mcar)\n\n# distribution plot for least squares imputation\nplot_imp_dists(\n    d=imp_ls_mcar,\n    mi=mi_ls_mcar, \n    col="y",\n    title="Distributions after Least Squares Imputation: MCAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for least squares imputation\nplot_imp_boxplots(\n    d=imp_ls_mcar,\n    mi=mi_ls_mcar,\n    col="y",\n    title="Boxplots after Least Squares Imputation: MCAR"\n)\n\n# strip plot for least squares imputation\nplot_imp_strip(\n    d=imp_ls_mcar,\n    mi=mi_ls_mcar,\n    col="y",\n    title="Imputed vs Observed Dists after Least Squares Imputation: MCAR"\n)\n```\n',escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"mcarLsImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-dist.png"}),i.a.createElement("img",{alt:"mcarLsImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-box.png"}),i.a.createElement("img",{alt:"mcarLsImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-strip.png"}),i.a.createElement(_.a,{source:'\n### Distribution Plots after PMM Imputation\n---\n```python\n\'\'\'MCAR distribution plots after PMM imputation\'\'\'\n\n# create the PMM imputer\nmi_pmm_mcar = MultipleImputer(\n    strategy="pmm", n=5, return_list=True, seed=101\n)\n\n# perform PMM imputation procedure\nimp_pmm_mcar = mi_pmm_mcar.fit_transform(mcar)\n\n# distribution plot for PMM imputation\nplot_imp_dists(\n    d=imp_pmm_mcar,\n    mi=mi_pmm_mcar, \n    col="y",\n    title="Distributions after PMM Imputation: MCAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for PMM imputation\nplot_imp_boxplots(\n    d=imp_pmm_mcar,\n    mi=mi_pmm_mcar,\n    col="y",\n    title="Boxplots after PMM Imputation: MCAR"\n)\n\n# swarm plot for PMM imputation\nplot_imp_swarm(\n    d=imp_pmm_mcar,\n    mi=mi_pmm_mcar,\n    col="y",\n    title="Imputed vs Observed Dists after PMM Imputation: MCAR"\n)\n```\n',escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"mcarPmmImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-dist.png"}),i.a.createElement("img",{alt:"mcarPmmImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-box.png"}),i.a.createElement("img",{alt:"mcarPmmImputerSwarm",className:"ete-swarm",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-swarm.png"}),i.a.createElement(_.a,{source:'\n### Linear Regression on Multiply Imputed Data\n---\n```python\n\'\'\'Regression after Multiple Imputation on MCAR\'\'\'\n\n# NOTE - FULL & LISTWISE DELETE CODE NOT INCLUDED\n# NOTE - FULL & LISTWISE DELETE STILL INCLUDED IN OUTPUT\n\n# create the regression using custom imputers\nlm_mean_mcar = MiLinearRegression(mi=mi_mean_mcar)\nlm_ls_mcar = MiLinearRegression(mi=mi_ls_mcar)\nlm_pmm_mcar = MiLinearRegression(mi=mi_pmm_mcar)\nmodels_mcar = [lm_mean_mcar, lm_ls_mcar, lm_pmm_mcar]\n\n# a bit of manipulation to create one dataframe\nget_coeff = lambda lm_, x: lm_.summary().loc["x"].to_frame()\nres_mcar = pd.concat([get_coeff(lm, "x") for lm in models_mcar], axis=1)\nres_mcar.columns = ["mean", "least squares", "pmm"]\nres_mcar = res_mcar.T[["coefs", "std", "vw", "vb", "vt"]]\n\n# show the results\nres_mcar\n```\n',escapeHtml:!1,renderers:{code:y}}),i.a.createElement("table",{border:"1",class:"dataframe"},i.a.createElement("thead",null,i.a.createElement("tr",null,i.a.createElement("th",null),i.a.createElement("th",null,"coefs"),i.a.createElement("th",null,"std"),i.a.createElement("th",null,"vw"),i.a.createElement("th",null,"vb"),i.a.createElement("th",null,"vt"))),i.a.createElement("tbody",null,i.a.createElement("tr",null,i.a.createElement("th",null,"full"),i.a.createElement("td",null,"0.70000"),i.a.createElement("td",null,"0.03200"),i.a.createElement("td",null,"0.00102"),i.a.createElement("td",null,"0.00000"),i.a.createElement("td",null,"0.00102")),i.a.createElement("tr",null,i.a.createElement("th",null,"listwise delete"),i.a.createElement("td",null,"0.73915"),i.a.createElement("td",null,"0.04682"),i.a.createElement("td",null,"0.00219"),i.a.createElement("td",null,"0.00000"),i.a.createElement("td",null,"0.00219")),i.a.createElement("tr",null,i.a.createElement("th",null,"mean"),i.a.createElement("td",null,"0.39330"),i.a.createElement("td",null,"0.03056"),i.a.createElement("td",null,"0.00093"),i.a.createElement("td",null,"0.00000"),i.a.createElement("td",null,"0.00093")),i.a.createElement("tr",null,i.a.createElement("th",null,"least squares"),i.a.createElement("td",null,"0.73915"),i.a.createElement("td",null,"0.02570"),i.a.createElement("td",null,"0.00066"),i.a.createElement("td",null,"0.00000"),i.a.createElement("td",null,"0.00066")),i.a.createElement("tr",null,i.a.createElement("th",null,"pmm"),i.a.createElement("td",null,"0.67692"),i.a.createElement("td",null,"0.05404"),i.a.createElement("td",null,"0.00128"),i.a.createElement("td",null,"0.00136"),i.a.createElement("td",null,"0.00292"))))),i.a.createElement("div",{className:"ete-mar"},i.a.createElement(_.a,{source:"\n## MAR\n---\n* 500 observations for two features, **predictor x** and **response y**  \n* Correlation between the variables is **0.7**\n* Predictor **x** is missing **40%** of the observations, while response **y** is fully observed  \n* The underlying missingness mechanism is **MAR**  \n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement(_.a,{source:"\n### Imports and Data Preparation\n---\n```python\n'''Handling imports for analysis'''\n%matplotlib inline\n\n# general modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# autoimpute imports - utilities & visuals\nfrom autoimpute.utils import md_pattern, proportions\nfrom autoimpute.visuals import plot_md_locations, plot_md_percent\nfrom autoimpute.visuals import plot_imp_dists, plot_imp_boxplots\nfrom autoimpute.visuals import plot_imp_swarm, plot_imp_strip\nfrom autoimpute.visuals import plot_imp_scatter\n\n# autoimpute imports - imputations & analysis\nfrom autoimpute.imputations import MultipleImputer\nfrom autoimpute.analysis import MiLinearRegression\n\n# reading the full dataset and mar dataset\nfull = pd.read_csv(\"full.csv\")\nmar = pd.read_csv(\"mar.csv\")\n```\n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement(_.a,{source:"\n### Percent Missing by Feature\n---\n```python\n'''MAR percent plot'''\nplot_md_percent(mar)\n```\n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"marPercent",className:"ete-percent",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-plot-md-percent.png"}),i.a.createElement(_.a,{source:"\n### Location of Missingness by Feature\n---\n```python\n'''MAR location plot'''\nplot_md_locations(mar)\n```\n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"marLocation",className:"ete-locations",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-plot-md-locations.png"}),i.a.createElement(_.a,{source:"\n### Mean Imputation\n---\n```python\n'''MAR mean imputation'''\n\n# create the mean imputer\nmi_mean_mar = MultipleImputer(\n    strategy=\"mean\", n=5, return_list=True, seed=101\n)\n\n# print the mean imputer to console\nprint(mi_mean_mar)\n\n# perform mean imputation procedure\nimp_mean_mar = mi_mean_mar.fit_transform(mar)\n```\n",escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"marMeanImputer",className:"ete-mean-imputer",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mean-imputer.png"}),i.a.createElement(_.a,{source:'\n### Distribution Plots after Mean Imputation\n---\n```python\n\'\'\'MAR distribution plots after mean imputation\'\'\'\n\n# distribution plot for mean imputation\nplot_imp_dists(\n    d=imp_mean_mar,\n    mi=mi_mean_mar, \n    col="x",\n    title="Distributions after Mean Imputation: MAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for mean imputation\nplot_imp_boxplots(\n    d=imp_mean_mar,\n    mi=mi_mean_mar,\n    col="x",\n    title="Boxplots after Mean Imputation: MAR"\n)\n\n# strip plot for mean imputation\nplot_imp_strip(\n    d=imp_mean_mar,\n    mi=mi_mean_mar,\n    col="x",\n    title="Imputed vs Observed Dists after Mean Imputation: MAR"\n)\n```\n',escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"marMeanImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-dist.png"}),i.a.createElement("img",{alt:"marMeanImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-box.png"}),i.a.createElement("img",{alt:"marMeanImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-strip.png"}),i.a.createElement(_.a,{source:'\n### Distribution Plots after Least Squares Imputation\n---\n```python\n\'\'\'MAR distribution plots after least squares imputation\'\'\'\n\n# create the least squares imputer\nmi_ls_mar = MultipleImputer(\n    strategy="least squares", n=5, return_list=True, seed=101\n)\n\n# perform least squares imputation procedure\nimp_ls_mar = mi_ls_mar.fit_transform(mar)\n\n# distribution plot for least squares imputation\nplot_imp_dists(\n    d=imp_ls_mar,\n    mi=mi_ls_mar, \n    col="x",\n    title="Distributions after Least Squares Imputation: MAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for least squares imputation\nplot_imp_boxplots(\n    d=imp_ls_mar,\n    mi=mi_ls_mar,\n    col="x",\n    title="Boxplots after Least Squares Imputation: MAR"\n)\n\n# strip plot for least squares imputation\nplot_imp_strip(\n    d=imp_ls_mar,\n    mi=mi_ls_mar,\n    col="x",\n    title="Imputed vs Observed Dists after Least Squares Imputation: MAR"\n)\n```\n',escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"marLsImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-dist.png"}),i.a.createElement("img",{alt:"marLsImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-box.png"}),i.a.createElement("img",{alt:"marLsImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-strip.png"}),i.a.createElement(_.a,{source:'\n### Distribution Plots after PMM Imputation\n---\n```python\n\'\'\'MAR distribution plots after PMM imputation\'\'\'\n\n# create the PMM imputer\nmi_pmm_mar = MultipleImputer(\n    strategy="pmm", n=5, return_list=True, seed=101\n)\n\n# perform PMM imputation procedure\nimp_pmm_mar = mi_pmm_mar.fit_transform(mar)\n\n# distribution plot for PMM imputation\nplot_imp_dists(\n    d=imp_pmm_mar,\n    mi=mi_pmm_mar, \n    col="x",\n    title="Distributions after PMM Imputation: MAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for PMM imputation\nplot_imp_boxplots(\n    d=imp_pmm_mar,\n    mi=mi_pmm_mar,\n    col="x",\n    title="Boxplots after PMM Imputation: MAR"\n)\n\n# swarm plot for PMM imputation\nplot_imp_swarm(\n    d=imp_pmm_mar,\n    mi=mi_pmm_mar,\n    col="x",\n    title="Imputed vs Observed Dists after PMM Imputation: MAR"\n)\n```\n',escapeHtml:!1,renderers:{code:y}}),i.a.createElement("img",{alt:"marPmmImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-dist.png"}),i.a.createElement("img",{alt:"marPmmImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-box.png"}),i.a.createElement("img",{alt:"marPmmImputerSwarm",className:"ete-swarm",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-swarm.png"}),i.a.createElement(_.a,{source:'\n### Linear Regression on Multiply Imputed Data\n---\n```python\n\'\'\'Regression after Multiple Imputation on MAR\'\'\'\n\n# NOTE - FULL & LISTWISE DELETE CODE NOT INCLUDED\n# NOTE - FULL & LISTWISE DELETE STILL INCLUDED IN OUTPUT\n\n# create the regression using custom imputers\nlm_mean_mar = MiLinearRegression(mi=mi_mean_mar)\nlm_ls_mar = MiLinearRegression(mi=mi_ls_mar)\nlm_pmm_mar = MiLinearRegression(mi=mi_pmm_mar)\nmodels_mar = [lm_mean_mar, lm_ls_mar, lm_pmm_mar]\n\n# a bit of manipulation to create one dataframe\nget_coeff = lambda lm_, x: lm_.summary().loc["x"].to_frame()\nres_mar = pd.concat([get_coeff(lm, "x") for lm in models_mar], axis=1)\nres_mar.columns = ["mean", "least squares", "pmm"]\nres_mar = res_mar.T[["coefs", "std", "vw", "vb", "vt"]]\n\n# show the results\nres_mar\n```\n',escapeHtml:!1,renderers:{code:y}}),i.a.createElement("table",{border:"1",class:"dataframe"},i.a.createElement("thead",null,i.a.createElement("tr",null,i.a.createElement("th",null),i.a.createElement("th",null,"coefs"),i.a.createElement("th",null,"std"),i.a.createElement("th",null,"vw"),i.a.createElement("th",null,"vb"),i.a.createElement("th",null,"vt"))),i.a.createElement("tbody",null,i.a.createElement("tr",null,i.a.createElement("th",null,"full"),i.a.createElement("td",null,"0.70000"),i.a.createElement("td",null,"0.03200"),i.a.createElement("td",null,"0.00102"),i.a.createElement("td",null,"0.00000"),i.a.createElement("td",null,"0.00102")),i.a.createElement("tr",null,i.a.createElement("th",null,"listwise delete"),i.a.createElement("td",null,"0.66180"),i.a.createElement("td",null,"0.03865"),i.a.createElement("td",null,"0.00149"),i.a.createElement("td",null,"0.00000"),i.a.createElement("td",null,"0.00149")),i.a.createElement("tr",null,i.a.createElement("th",null,"mean"),i.a.createElement("td",null,"0.66180"),i.a.createElement("td",null,"0.04896"),i.a.createElement("td",null,"0.00240"),i.a.createElement("td",null,"0.00000"),i.a.createElement("td",null,"0.00240")),i.a.createElement("tr",null,i.a.createElement("th",null,"least squares"),i.a.createElement("td",null,"0.86053"),i.a.createElement("td",null,"0.02889"),i.a.createElement("td",null,"0.00083"),i.a.createElement("td",null,"0.00000"),i.a.createElement("td",null,"0.00083")),i.a.createElement("tr",null,i.a.createElement("th",null,"pmm"),i.a.createElement("td",null,"0.69690"),i.a.createElement("td",null,"0.03707"),i.a.createElement("td",null,"0.00096"),i.a.createElement("td",null,"0.00034"),i.a.createElement("td",null,"0.00137"))))))}}]),t}(a.Component),I=function(e){var t=window.location.pathname+window.location.search,n=e.path===t?"nav-item active":"nav-item",a=e.disabled?"nav-link disabled":"nav-link";return i.a.createElement("li",{className:n},i.a.createElement("a",{onClick:function(){return e.onClick()},href:e.path,className:a},e.name,e.path===t?i.a.createElement("span",{className:"sr-only"},"(current)"):""))},k=function(e){function t(e){var n;return Object(m.a)(this,t),(n=Object(o.a)(this,Object(p.a)(t).call(this,e))).state={isToggleOn:!1},n}return Object(u.a)(t,e),Object(l.a)(t,[{key:"showDropdown",value:function(e){e.preventDefault(),this.setState(function(e){return{isToggleOn:!e.isToggleOn}})}},{key:"render",value:function(){var e=this,t="dropdown-menu"+(this.state.isToggleOn?" show":"");return i.a.createElement("li",{className:"nav-item dropdown"},i.a.createElement("a",{className:"nav-link dropdown-toggle",id:"navbarDropdown",role:"button","data-toggle":"dropdown","aria-haspopup":"true","aria-expanded":"false",onClick:function(t){e.showDropdown(t)}},this.props.name),i.a.createElement("div",{className:t,"aria-labelledby":"navbarDropdown"},this.props.children))}}]),t}(a.Component),x=function(e){function t(e){var n;return Object(m.a)(this,t),(n=Object(o.a)(this,Object(p.a)(t).call(this,e))).state={activeKey:1,df:"/autoimpute-tutorials/"},n}return Object(u.a)(t,e),Object(l.a)(t,[{key:"handleClick",value:function(e){this.setState({activeKey:e}),d()(".dropdown-menu").hasClass("show")&&(d()(".dropdown-menu").trigger("click"),d()(".dropdown-menu").removeClass("show"))}},{key:"render",value:function(){return i.a.createElement("div",{className:"main-page"},i.a.createElement("nav",{className:"navbar navbar-expand-lg"},i.a.createElement("a",{className:"navbar-brand",href:this.state.df},"Autoimpute"),i.a.createElement("div",{className:"collapse navbar-collapse",id:"navbarSupportedContent"},i.a.createElement("ul",{className:"navbar-nav mr-auto"},i.a.createElement(I,{name:"Home",onClick:this.handleClick.bind(this,1)}),i.a.createElement(I,{name:"Contact",onClick:this.handleClick.bind(this,2)}),i.a.createElement(k,{name:"Tutorials"},i.a.createElement("a",{className:"dropdown-item",href:this.props.path,onClick:this.handleClick.bind(this,3.1)},"Exploring Missingness"),i.a.createElement("a",{className:"dropdown-item",href:this.props.path,onClick:this.handleClick.bind(this,3.2)},"Imputers: Part I"),i.a.createElement("a",{className:"dropdown-item",href:this.props.path,onClick:this.handleClick.bind(this,3.3)},"Imputers: Part II"),i.a.createElement("a",{className:"dropdown-item",href:this.state.path,onClick:this.handleClick.bind(this,3.4)},"Imputers: Part III"),i.a.createElement("a",{className:"dropdown-item",href:this.state.path,onClick:this.handleClick.bind(this,3.5)},"Comparing Imputation Methods"),i.a.createElement("a",{className:"dropdown-item",href:this.state.path,onClick:this.handleClick.bind(this,3.6)},"End-to-End Analysis"))))),i.a.createElement("div",{className:"content"},1===this.state.activeKey?i.a.createElement(v,null):null,2===this.state.activeKey?i.a.createElement(h,null):null,3.5===this.state.activeKey?i.a.createElement(M,null):null,3.6===this.state.activeKey?i.a.createElement(w,null):null,this.state.activeKey>2&&this.state.activeKey<3.5?"Tutorials coming soon! "+this.state.activeKey:null))}}]),t}(i.a.Component),N=function(e){function t(){return Object(m.a)(this,t),Object(o.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(u.a)(t,e),Object(l.a)(t,[{key:"render",value:function(){return i.a.createElement("div",null,i.a.createElement(x,null))}}]),t}(a.Component);Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));n(393);s.a.render(i.a.createElement(N,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then(function(e){e.unregister()})},40:function(e,t,n){}},[[33,1,2]]]);
//# sourceMappingURL=main.0f8eb33d.chunk.js.map