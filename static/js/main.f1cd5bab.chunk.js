(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{33:function(e,t,n){e.exports=n(394)},39:function(e,t,n){},394:function(e,t,n){"use strict";n.r(t);var a=n(0),r=n.n(a),i=n(28),s=n.n(i),l=(n(39),n(2)),o=n(3),m=n(5),u=n(4),c=n(6),p=(n(40),n(12)),d=n.n(p),h=function(e){function t(){return Object(l.a)(this,t),Object(m.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(c.a)(t,e),Object(o.a)(t,[{key:"render",value:function(){return r.a.createElement("div",null,r.a.createElement("h2",null,"THIS IS THE CONTACT PAGE"))}}]),t}(a.Component),g=n(1),f=n.n(g),y=n(396),b=n(32),w=function(e){function t(){return Object(l.a)(this,t),Object(m.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(c.a)(t,e),Object(o.a)(t,[{key:"render",value:function(){var e=this.props,t=e.language,n=e.value;return r.a.createElement(y.a,{language:t,style:b.docco},n)}}]),t}(a.PureComponent);w.defaultProps={language:null};var E=w,_=function(e){function t(){return Object(l.a)(this,t),Object(m.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(c.a)(t,e),Object(o.a)(t,[{key:"render",value:function(){return r.a.createElement("div",{className:"home-page"},r.a.createElement("div",{className:"home-page-left"},r.a.createElement(f.a,{source:"\n## Welcome to Autoimpute!\n---\n[![PyPI version](https://badge.fury.io/py/autoimpute.svg)](https://badge.fury.io/py/autoimpute) [![Build Status](https://travis-ci.com/kearnz/autoimpute.svg?branch=master)](https://travis-ci.com/kearnz/autoimpute) [![Documentation Status](https://readthedocs.org/projects/autoimpute/badge/?version=latest)](https://autoimpute.readthedocs.io/en/latest/?badge=latest) [![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/) [![Python 3.6+](https://img.shields.io/badge/python-3.6+-blue.svg)](https://www.python.org/downloads/release/python-360/)\n\n[Autoimpute](https://pypi.org/project/autoimpute/) is a Python package for **handling missing data**. Install it from PyPI with:\n\n```python\npip install autoimpute\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n\n### Getting Started\n---\nAutoimpute is designed to be user friendly and flexible.\n\nImputations can be as simple as:\n```python\n\nfrom autoimpute.imputations import MultipleImputer\nimp = MultipleImputer()\nimp.fit_transform(data)\n```\n\nAnalysis of multiply imputed data is easy too:\n\n```python\n\nfrom autoimpute.analysis import MiLinearRegression\nimp_lm = MiLinearRegression()\nimp_lm.fit(X_train, y_train)\nimp_lm.summary()\npredictions = imp_lm.predict(X_test)\n",escapeHtml:!1,renderers:{code:E}})),r.a.createElement("div",{className:"hompe-page-right"},r.a.createElement("img",{alt:"autoimpute-logo",className:"autoimpute-logo",src:"https://kearnz.github.io/autoimpute-tutorials/img/home/autoimpute-logo-transparent.png"}),r.a.createElement(f.a,{source:"\n\n### Main Features\n---\n* Utility functions to examine patterns in missing data\n* Missingness classifier and automatic missing data test set generator\n* Numerous imputation methods for continuous, categorical, and time-series data\n* Single and multiple imputation frameworks to apply imputation methods\n* Custom visualization support for utility functions and imputation methods\n* Analysis methods and pooled parameter inference using multiply imputed datasets\n* Adherence to `sklearn` API design for imputation and analysis classes\n* Direct integration with `pandas`, `statsmodels`, `pymc3`, and more\n",escapeHtml:!1,renderers:{code:E}})))}}]),t}(a.Component),I=function(e){function t(){return Object(l.a)(this,t),Object(m.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(c.a)(t,e),Object(o.a)(t,[{key:"render",value:function(){return r.a.createElement("div",{className:"comparing-imputation-methods"},r.a.createElement("div",{className:"cim-header-01"},r.a.createElement(f.a,{source:"\n## Using Autoimpute to Compare Imputation Methods\n\nThis tutorial examimes the effect of imputation methods from the `Autoimpute` package. The tutorial includes:  \n\n1. Generating Data  \n2. Exploring Missingness  \n3. Imputation Methods  \n4. Impact of imputation on Covariance and Correlation  \n\n### 1. Generating Data\n* In the section below, we generate two variables, **x** and **y**, that are positively correlated but display heteroscedasticity.\n* Thus, **x** and **y** get larger (or smaller) together. As a result, the variance between the two variables is not constant.     \n* We then introduce 30% missingness within **y**. **x** remains completely observed.  \n* The code on the left generates the data and creates a function to visualize the data.  \n* The code on the right displays the resulting dataframe and its associated plot.   \n",escapeHtml:!1,renderers:{code:E}})),r.a.createElement("div",{className:"cim-code-1"},r.a.createElement(f.a,{source:'\n```python\n\'\'\'Simulating data and defining a joint plot b/w two variables\'\'\'\n\n# plotting specification and imports needed\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm, binom\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings("ignore")\n\n# seed to follow along\nnp.random.seed(654654)\n\n# generate 1500 data points\nN = np.arange(1500)\n\n# create correlated, heteroskedastic random variables\na = 0\nb = 1\neps = np.array([norm(0, n).rvs() for n in N])\ny = (a + b*N + eps) / 100                         \nx = (N + norm(10, 10).rvs(len(N))) / 100\n \n# 30% missingness created artificially\ny[binom(1, 0.3).rvs(len(N)) == 1] = np.nan\n\n# collect results in a dataframe \ndata_het_miss = pd.DataFrame({"y": y, "x": x})\n\n# create a scatter plot function to display the results of our dataframe\ndef scatter_dists(data, x="x", y="y", a=0.5, joints_color="navy", \n                  markers="o", marginals=dict(rug=True, kde=True)):\n\n    sns.set(context="talk")\n    joint_kws = dict(\n        facecolor=joints_color,\n        edgecolor=joints_color,\n        marker=markers\n    )\n    sns.jointplot(\n        x=x, y=y, data=data, alpha=a, height=8.27,\n        joint_kws=joint_kws, marginal_kws=marginals\n    )\n```\n',escapeHtml:!1,renderers:{code:E}})),r.a.createElement("div",{className:"cim-table-code-1"},r.a.createElement(f.a,{source:"\n```python\ndata_het_miss.head(7)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,"0.295288"),r.a.createElement("td",null,"0.000000")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.086210"),r.a.createElement("td",null,"0.002255")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.227369"),r.a.createElement("td",null,"0.003869")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.194216"),r.a.createElement("td",null,"0.046370")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.094630"),r.a.createElement("td",null,"0.081440")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.292320"),r.a.createElement("td",null,"NaN")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.198131"),r.a.createElement("td",null,"0.067222"))))),r.a.createElement("div",{className:"cim-df-scatter-01"},r.a.createElement(f.a,{source:"\n```python\nscatter_dists(data_het_miss)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"scatter01",src:"https://kearnz.github.io/autoimpute-tutorials/img/comparing/cim-df-scatter01.png"})))}}]),t}(a.Component),v=function(e){function t(){return Object(l.a)(this,t),Object(m.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(c.a)(t,e),Object(o.a)(t,[{key:"render",value:function(){return r.a.createElement("div",{className:"imputer-I"},r.a.createElement(f.a,{source:"\n## Getting the Most out of the Imputer Classes: Part I\n---\nThis tutorial is part I of a comprehensive overview of `Autoimpute` Imputers. It includes:  \n1. Motivation for Imputation in the First Place  \n2. The Design Considerations behind Autoimpute Imputers  \n\n### 1. Motivation for Imputation in the First Place\nLet's revisit why multiple imputation is necessary. A user wants to perform analysis on a dataset using some sort of **analysis model** such as linear regression or logistic regression. The dataset of interest contains one or more predictors, **X**, and some response **y**. The analysis model produces a function that best explains the relationship between **X** and **y**. Let's generate some sample data below. To keep things simple, our data set contains just one predictor, **x**, and a response **y**.\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:'\n\n```python\n# imports\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm, binom\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings("ignore")\nsns.set(context="talk", rc={\'figure.figsize\':(11.7,8.27)})\n\n# helper functions used throughout this project\nprint_header = lambda msg: print(f"{msg}\n{\'-\'*len(msg)}")\n\n# seed to follow along\nnp.random.seed(654654)\n\n# generate 1500 data points\nN = np.arange(1500)\n\n# helper function for this data\nvary = lambda v: np.random.choice(np.arange(v))\n\n# create correlated, random variables\na = 2\nb = 1/2\neps = np.array([norm(0, vary(50)).rvs() for n in N])\ny = (a + b*N + eps) / 100                         \nx = (N + norm(10, vary(250)).rvs(len(N))) / 100\n \n# 20% missing in x, 30% missing in y\nx[binom(1, 0.2).rvs(len(N)) == 1] = np.nan\ny[binom(1, 0.3).rvs(len(N)) == 1] = np.nan\n\n# collect results in a dataframe \ndata_miss = pd.DataFrame({"y": y, "x": x})\nsns.scatterplot(x="x", y="y", data=data_miss)\nplt.show()\n```\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"imputer-I-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/imputer/imputer-I-scatter.png"}),r.a.createElement(f.a,{source:"\nThe plot suggests a linear relationship may exist between **x** and **y**. Let's fit a linear model to estimate that relationship.\n\n\n```python\nfrom sklearn.linear_model import LinearRegression\n\n# prep for regression\nX = data_miss.x.values.reshape(-1, 1) # reshape because one feature only\ny = data_miss.y\nlm = LinearRegression()\n\n# try to fit the model\nprint_header(\"Fitting linear model to estimate relationship between X and y\")\ntry:\n    lm.fit(X, y)\nexcept ValueError as ve:\n    print(f\"{ve.__class__.__name__}: {ve}\")\n```\n\n    Fitting linear model to estimate relationship between X and y\n    -------------------------------------------------------------\n    ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n\n\n#### What Happened?\n`sklearn` threw a ValueError when we tried to fit a linear regression. The error occurred because our **dataset has missing data**. In our case, 20% of observations are missing in our predictor, and 30% of observations are missing in our response. `sklearn` cannot fit the analysis model unless our dataset is complete! That's no good - we can't model the relationship in our data, nor can we make predictions when new data arrives.\n\n#### What do we Do?\nIn order to proceed, we **need to handle the missing data in some way**. One option is simply removing records with missing data. This strategy allows the analysis model to run, but it may negatively affect the inference from that model. For now, we'll recommend against dropping missing records.\n\nIf we don't drop missing records, **then we must impute them**. Imputing data means coming up with plausible values to fill in missing records, which we must do to enable our analysis model to run. **Performing imputations is the primary concern of Autoimpute**. The next section introduces Autoimpute Imputers and familiarizes the reader with Autoimpute's package design.\n\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n\n### 2. The Design Considerations behind Autoimpute Imputers\nWe designed Autoimpute Imputers with three goals in mind:  \n* **Make Imputation Easy**. Imputation can be done in one line of code.  \n* **Make Imputation Familiar to Python Users.** Autoimpute Imputers follows the design patterns of sklearn.  \n* **Make Imputation Flexible**. Use an Imputer's default arguments or fine-tune Imputers case-by-case.  \n\nLet's explore each objective. We'll use the same dataset as above and stick with the `SingleImputer` for now. The `MutlipleImputer` extends the `SingleImputer` and therefore contains all the arguments that the `SingleImputer` does. Therefore, we will explain design considerations using the `SingleImputer`. In Part III of this series, we'll address additional arguments of and considerations for the `MultipleImputer`.\n\n#### 2.1. Make Imputation Easy\n\nAs promised, we can impute a dataset with exactly one line of code. In the code section below, we'll demonstrate the one line of code that imputes all missing data in a dataset. First, we'll observe how many records are missing. Then, we'll perform imputation and verify that missing records have been imputed.\n\n\n```python\n# amount of missing data before imputation\nprint_header(\"Amount of data missing before imputation takes place\")\npd.DataFrame(data_miss.isnull().sum(), columns=[\"records missing\"]).T\n```\n\n    Amount of data missing before imputation takes place\n    ----------------------------------------------------\n\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"label"),r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"records missing")),r.a.createElement("td",null,"285"),r.a.createElement("td",null,"491")))),r.a.createElement(f.a,{source:'\n\n```python\nfrom autoimpute.imputations import SingleImputer\nprint_header("Imputing missing data in one line of code with the default SingleImputer")\ndata_imputed_once = SingleImputer().fit_transform(data_miss)\nprint("Imputation Successful!")\n```\n\n    Imputing missing data in one line of code with the default SingleImputer\n    ------------------------------------------------------------------------\n\n\n    Auto-assigning NUTS sampler...\n    Initializing NUTS using jitter+adapt_diag...\n    Multiprocess sampling (4 chains in 4 jobs)\n    NUTS: [\u03c3, beta, alpha]\n    Sampling 4 chains: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [00:03<00:00, 1912.67draws/s]\n    Auto-assigning NUTS sampler...\n    Initializing NUTS using jitter+adapt_diag...\n    Multiprocess sampling (4 chains in 4 jobs)\n    NUTS: [\u03c3, beta, alpha]\n    Sampling 4 chains: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [00:03<00:00, 1722.73draws/s]\n\n\n    Imputation Successful!\n\n\n\n```python\n# amount of missing data before imputation\nprint_header("Amount of data missing after imputation takes place")\npd.DataFrame(data_imputed_once.isnull().sum(), columns=["records missing"]).T\n```\n\n    Amount of data missing after imputation takes place\n    ---------------------------------------------------\n\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"label"),r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"records missing")),r.a.createElement("td",null,"0"),r.a.createElement("td",null,"0")))),r.a.createElement(f.a,{source:"\n\n### 2.2 Make Imputation Familiar to Python Users\nAutoimpute follows `sklearn` API design. This design choice comes with a number of benefits:  \n* If you are familiar with `sklearn`, there is essentially no learning curve to start using `Autoimpute`.  \n* Imputers inherit from sklearn's BaseEstimator and TransformerMixin and leverage methods from these Parent classes.  \n* Imputers can `fit_transform` the same dataset, or `fit` and `Imputer` to `transform` new data with the same features.  \n* Imputers fit in sklearn Pipelines (although Autoimpute includes ML models designed for multiply imputed data).  \n\nThe code segments below demonstrate Autoimpute's ease of use and familiarity.\n\nFirst, Autoimpute Imputers inherit from `BaseEstimator` and `TransformerMixin` classes from `sklearn`.\n\n\n```python\nprint_header(\"Parent Classes to SingleImputer and MultipleImpuer\")\nprint(list(map(lambda cls_: cls_.__name__, SingleImputer().__class__.__bases__)))\n```\n\n    Parent Classes to SingleImputer and MultipleImpuer\n    --------------------------------------------------\n    ['BaseImputer', 'BaseEstimator', 'TransformerMixin']\n\n\nAs with `sklearn` `Transformers`, Autoimpute Imputers set smart defaults for all its arguments. Imputers leverage the `__repr__` special method inherited from the `BaseEstimator`, so users can quickly examine the default values an `Imputer` sets.\n\n\n```python\nSingleImputer()\n```\n\n\n\n\n    SingleImputer(copy=True, imp_kwgs=None, predictors='all', seed=None,\n           strategy='default predictive', visit='default')\n\n\n\nBecause Autoimpute Imputers are valid `sklearn` `Transformers`, they implement the `fit`, `transform`, and `fit_transform` methods, which should be familiar to anyone who has preprocessed data using `sklearn`. The `fit` step returns an instance of the `Imputer` class, and the `transform` step returns a **transformed dataset**. Imputers can fit and transform a dataset in one go by using `fit_transform`. We demonstrate this process below, transforming our missing dataset using mean imputation.\n\n\n```python\nprint_header(\"Original dataset with missing values\")\ndata_miss.head(10)\n```\n\n    Original dataset with missing values\n    ------------------------------------\n\n\n\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,"2.534781"),r.a.createElement("td",null,"0.257901")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.118755"),r.a.createElement("td",null,"-0.114591")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.184612"),r.a.createElement("td",null,"0.018801")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.442059"),r.a.createElement("td",null,"0.047037")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.109537"),r.a.createElement("td",null,"0.229042")),r.a.createElement("tr",null,r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"NaN")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.962892"),r.a.createElement("td",null,"0.000090")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.028426"),r.a.createElement("td",null,"NaN")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.949358"),r.a.createElement("td",null,"NaN")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.728996"),r.a.createElement("td",null,"0.068058")))),r.a.createElement(f.a,{source:'\n\n```python\nprint_header("Transforming the missing dataset with mean imputation")\nimputer = SingleImputer(strategy="mean")\ndata_imputed = imputer.fit_transform(data_miss)\ndata_imputed.head(10)\n```\n\n    Transforming the missing dataset with mean imputation\n    -----------------------------------------------------\n\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,"2.534781"),r.a.createElement("td",null,"0.257901")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.118755"),r.a.createElement("td",null,"-0.114591")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.184612"),r.a.createElement("td",null,"0.018801")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.442059"),r.a.createElement("td",null,"0.047037")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.109537"),r.a.createElement("td",null,"0.229042")),r.a.createElement("tr",null,r.a.createElement("td",null,"7.577981"),r.a.createElement("td",null,"3.747201")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.962892"),r.a.createElement("td",null,"0.000090")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.028426"),r.a.createElement("td",null,"3.747201")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.949358"),r.a.createElement("td",null,"3.747201")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.728996"),r.a.createElement("td",null,"0.068058")))),r.a.createElement(f.a,{source:"\n\nThe transformed dataset contains imputations in place of previously missing values. Here, we used mean imputation, although we offer many imputation strategies that may be more appropriate. We show these strategies in Part II of this series. We can **easily retrieve the index of the the imputed values** should we want to assess the imputations themselves. The index of imputed values live in the Imputer's `imputed_` attribute. As with `sklearn`, public attributes of Imputers contain an underscore suffix.\n\nThe `imputed_` attribute returns a dictionary, where each key is a column and its value is a list with the index of each imputation for that column. These indices represent where data was originally missing but is now imputed. We can use these indices to find the location of imputations within a transformed dataset. The code below accesses the first 5 imputations for **x** and **y**. Because we used mean imputation, the imputation values are the same. In this case, the **record with index 5** had missing values for both **x** and **y**.\n\n\n```python\nprint_header(\"Showing the first 5 imputations for column x\")\ndata_imputed.loc[imputer.imputed_['x'], 'x'].head()\n```\n\n    Showing the first 5 imputations for column x\n    --------------------------------------------\n\n    5     7.577981\n    13    7.577981\n    24    7.577981\n    29    7.577981\n    30    7.577981\n    Name: x, dtype: float64\n\n\n\n\n```python\nprint_header(\"Showing the first 5 imputations for column y\")\ndata_imputed.loc[imputer.imputed_['y'], 'y'].head()\n```\n\n    Showing the first 5 imputations for column y\n    --------------------------------------------\n\n    5     3.747201\n    7     3.747201\n    8     3.747201\n    13    3.747201\n    18    3.747201\n    Name: y, dtype: float64\n\n\n\n#### 2.3. Make Imputation Flexible\nTo make Imputers powerful, we need to make them flexible. While Autoimpute Imputers set default values for their arguments, the Imputers' arguments give users full control of how to impute each column within a dataset. Let's see what arguments Imputers take. The code below prints each argument's name and its default value.\n\n\n```python\nprint_header(\"Printing arguments for the SingleImputer as well as their default values\")\nfor k,v in SingleImputer().get_params().items():\n    print(f\"Argument: {k}; Default: {v}\")\n```\n\n    Printing arguments for the SingleImputer as well as their default values\n    ------------------------------------------------------------------------\n    Argument: copy; Default: True\n    Argument: imp_kwgs; Default: None\n    Argument: predictors; Default: all\n    Argument: seed; Default: None\n    Argument: strategy; Default: default predictive\n    Argument: visit; Default: default\n\n\nBelow is a brief description of each argument. Part II of this series explains how to use these arguments to control imputation.  \n* **copy**: Whether or not to copy the dataset during the transform place. If False, imputations happen in place.  \n* **imp_kwgs**: Dictionary of arguments to fine-tune imputation on a specific column or for a specific imputation strategy.  \n* **predictors**: Which columns to use to predict imputations for a given column's imputation model.  \n* **seed**: Seed number makes imputations reproducible.  \n* **strategy**: The imputation strategy to use. Can specify for all columns or each column individually.  \n* **visit**: The order in which columns should be \"visited\" or imputed.  \n\n\nThis concludes Part I of this series. This tutorial is merely an introduction to the `Autoimpute` Imputers. We motivated the need for Autoimpute Imputers and introduced the package design at the highest level, but we barely scratched the surface for what Imputers can do. In the code segment above, we peeked at the arguments an `Imputer` takes. These arguments give the user full control over the imputation process and hold the power and flexibility behind the `Imputer` classes. Part II of this series walks through these arguments in depth, giving users examples of how the arguments tailor imputation to fit specific needs.\n\n\n",escapeHtml:!1,renderers:{code:E}}))}}]),t}(a.Component),M=function(e){function t(){return Object(l.a)(this,t),Object(m.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(c.a)(t,e),Object(o.a)(t,[{key:"render",value:function(){return r.a.createElement("div",{className:"imputer-II"},r.a.createElement(f.a,{source:'\n\n## Getting the Most out of the Imputer Classes: Part II\n---\nThis tutorial is part II of a comprehensive overview of `Autoimpute` Imputers. It includes:\n\n1. Creating a Toy Dataset for the `SingleImputer`  \n2. How the `SingleImputer` Works under the Hood  \n3. Customizing a `SingleImputer` through its Arguments  \n\n### 1. Creating a Toy Dataset for the SingleImputer\nThis tutorial utilizes the toy dataset created below. Note that the dataset has no missing values. This is fine because we will not perform imputations. Rather, we are interested in how the `SingleImputer` works under the hood and how we can control the `SingleImputer` to fit imputation models capable of imputing missing data. While the imputations themselves may be of primary interest to the end user, they are simply the output from a fitted imputation model. Therefore, this tutorial places emphasis on how `Autoimpute` Imputers **fit imputation models**.\n\n\n```python\n# imports to create toy df\nimport warnings\nimport numpy as np\nimport pandas as pd\nwarnings.filterwarnings(\'ignore\')\n\n# helper functions used throughout this project\nprint_header = lambda msg: print(f"{msg}\\n{\'-\'*len(msg)}")\n\n# dataframe with columns as random selections from various arrays\ntoy_df = pd.DataFrame({\n    "age": np.random.choice(np.arange(20,80), 50),\n    "gender": np.random.choice(["Male","Female"], 50),\n    "employment": np.random.choice(["Unemployed","Employed", "Part Time", "Self-Employed"], 50),\n    "salary": np.random.choice(np.arange(50_000, 1_000_000), 50),\n    "weight": np.random.choice(np.arange(100, 300, 0.1), 50),\n})\n\n# helper functions used throughout this project\nprint_header("Creating a toy dataset for demonstration purposes")\ntoy_df.head()\n```\n\n    Creating a toy dataset for demonstration purposes\n    -------------------------------------------------\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",class:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"age"),r.a.createElement("th",null,"employment"),r.a.createElement("th",null,"gender"),r.a.createElement("th",null,"salary"),r.a.createElement("th",null,"weight"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,"74"),r.a.createElement("td",null,"Unemployed"),r.a.createElement("td",null,"Female"),r.a.createElement("td",null,"484881"),r.a.createElement("td",null,"233.9")),r.a.createElement("tr",null,r.a.createElement("td",null,"26"),r.a.createElement("td",null,"Employed"),r.a.createElement("td",null,"Male"),r.a.createElement("td",null,"874459"),r.a.createElement("td",null,"110.1")),r.a.createElement("tr",null,r.a.createElement("td",null,"66"),r.a.createElement("td",null,"Self-Employed"),r.a.createElement("td",null,"Female"),r.a.createElement("td",null,"800823"),r.a.createElement("td",null,"231.1")),r.a.createElement("tr",null,r.a.createElement("td",null,"44"),r.a.createElement("td",null,"Part Time"),r.a.createElement("td",null,"Male"),r.a.createElement("td",null,"606560"),r.a.createElement("td",null,"144.7")),r.a.createElement("tr",null,r.a.createElement("td",null,"57"),r.a.createElement("td",null,"Self-Employed"),r.a.createElement("td",null,"Male"),r.a.createElement("td",null,"269862"),r.a.createElement("td",null,"220.0")))),r.a.createElement(f.a,{source:"\n\nOur toy dataframe has 5 columns of mixed types. `age`, `salary`, and `weight` are numeric, while `gender` and `employment` are categorical. For the numeric variables, `age` and `salary` take integer values, while `weight` is float. For the categorical variables, `gender` is binary while `employment` is multiclass.  As we'll see later in this tutorial, column types matter for imputation. Imputers handle numeric and categorical columns, but users must be aware of which imputation methods apply to which data types.\n\n### 2. How the SingleImputer Works under the Hood\nA lot has to happen behind the scenes for the `SingleImputer` to meet Autoimpute's design goals addressed in Part I, section 2. This section peeks under the hood of the `SingleImputer` to explore how it makes the imputation process easy, familiar, and flexible. Additionally, users are more adequately prepared to utilize Imputers if they have a solid understanding of `Imputer` mechanics. In this section, we cover:  \n* How the `SingleImputer` Fits the Imputation Model  \n* What are \"series-imputers\"?  \n* The Design Patterns behind Imputers  \n\n#### 2.1 How the SingleImputer Fits the Imputation Model\nRecall from Part I, section 2 that `Autoimpute` Imputers set smart defaults for each argument. Therefore, we can instantiate and fit an imputer that can handle any dataset we give it. Let's create a default instance of the `SingleImputer` class and fit it to our toy dataset.\n\n\n```python\nfrom autoimpute.imputations import SingleImputer\nsi = SingleImputer()\n\nprint_header(\"Fit Method returning instance of the SingleImputer class\")\nsi.fit(toy_df)\n```\n\n    Fit Method returning instance of the SingleImputer class\n    --------------------------------------------------------\n\n    SingleImputer(copy=True, imp_kwgs=None, predictors='all', seed=None,\n           strategy='default predictive', visit='default')\n\n\n\nIn the code above, we create a default instance of the `SingleImputer` and assign it the variable name `si`. Note that because we did not specify a strategy, the `SingleImputer` set the strategy to **default predictive** for each column in the dataset. We'll explore the `strategy` argument in more detail in section 3 of this tutorial.\n\nWe then fit `si` to our toy dataset. This fitting process is easy and should be quite familiar. As with `sklearn`, our `SingleImputer` uses default arguments to fit the dataset and returns an instance of the `Imputer` class itself when the fitting process is complete (The `BaseEstimator` `__repr__` prints the instance and its arguments to the console). **But what actually happened when we fit the Imputer?** Just like `sklearn` `Transformers`, the `SingleImputer` generates a `statistics_` attribute when the `fit` method is called. In our case, the `statistics_` attribute stores the imputation model built for each column we want to impute. We can access the `statistics_` of `si` to explore what happened after the `fit` process completes and the imputation model(s) are created.\n\n\n```python\nprint_header(\"Statistics generated from fit method of the SingleImputer\")\nsi.statistics_\n```\n\n    Statistics generated from fit method of the SingleImputer\n    ---------------------------------------------------------\n\n    {'age': DefaultPredictiveImputer(cat_imputer=MultinomialLogisticImputer(),\n                  cat_kwgs=None,\n                  num_imputer=PMMImputer(am=48.53533000546936, asd=10,\n           bm=array([-1.21117e-05,  8.27985e-03, -5.43102e+00,  1.04216e+01,\n             9.20132e+00,  7.98604e+00]),\n           bsd=10, fill_value='random', init='auto', neighbors=5, sample=1000,\n           sig=1, tune=1000),\n                  num_kwgs=None),\n     'employment': DefaultPredictiveImputer(cat_imputer=MultinomialLogisticImputer(),\n                  cat_kwgs=None,\n                  num_imputer=PMMImputer(am=None, asd=10, bm=None, bsd=10, fill_value='random', init='auto',\n           neighbors=5, sample=1000, sig=1, tune=1000),\n                  num_kwgs=None),\n     'gender': DefaultPredictiveImputer(cat_imputer=MultinomialLogisticImputer(),\n                  cat_kwgs=None,\n                  num_imputer=PMMImputer(am=None, asd=10, bm=None, bsd=10, fill_value='random', init='auto',\n           neighbors=5, sample=1000, sig=1, tune=1000),\n                  num_kwgs=None),\n     'salary': DefaultPredictiveImputer(cat_imputer=MultinomialLogisticImputer(),\n                  cat_kwgs=None,\n                  num_imputer=PMMImputer(am=887784.0482856919, asd=10,\n           bm=array([  -3475.07484,    -351.17341,  -90835.31078, -197110.63707,\n            -128389.04932,   83669.62554]),\n           bsd=10, fill_value='random', init='auto', neighbors=5, sample=1000,\n           sig=1, tune=1000),\n                  num_kwgs=None),\n     'weight': DefaultPredictiveImputer(cat_imputer=MultinomialLogisticImputer(),\n                  cat_kwgs=None,\n                  num_imputer=PMMImputer(am=203.908957825882, asd=10,\n           bm=array([ 9.18857e-02, -1.35828e-05, -1.92486e+00,  2.90271e+00,\n             2.08853e+00,  8.95211e+00]),\n           bsd=10, fill_value='random', init='auto', neighbors=5, sample=1000,\n           sig=1, tune=1000),\n                  num_kwgs=None)}\n\n\n\nWhile the `statistics_` attribute is quite dense in the default case, it demonstrates what a `SingleImputer` does by default after fitting an `Imputer`. `si.statistics_` returns a dictionary, where the key is a column we are imputing in our dataset and the value is a **series-imputer** that corresponds to the strategy set for the given column (or key). Each series-imputer contains within itself the **imputation model** for the given column it is called upon to fit. The next section discusses the concept of a series-imputer in more detail.\n\n#### 2.2 What are \"series-imputers\"?\nThe **series-imputer** is a critical component of how `Autoimpute` Imputers work under the hood. From the the end-user's perspective, series-imputers are simply workers behind the scenes that implement a given imputation model. From Autoimpute's pespective, series-imputers are classes that implement the \"imputation interface\" and therefore can be used interchangeably within the `SingleImputer` when different strategies are passed.\n\nThe `strategy` a `SingleImputer` assigns to each column in a dataset maps to a **series-imputer class prefixed by the strategy's name**. These series-imputers are in the `autoimpute.imputations.series` folder, but they are hidden from end users because they aren't meant for public use. The `SingleImptuer` and `MultipleImputer` are the main classes `Autoimpute` exposes to end users because they are robust and work with `DataFrames`. Under the hood, however, both of these `Imptuers` rely on series-imputers to do all of the actual work once a dataset is ready for imputation models to be fit.\n\nWe'll explain the importance of and reasoning behind this design pattern in the next section (2.3). For now, let's return to our `si` `Imputer` and explore the series-imputer used by default for each column. We'll focus on stats for `employment` first.\n\n\n```python\n# get the series imputer for employment column\nemp_series_imputer = si.statistics_[\"employment\"]\n\nprint_header(\"The series-imputer for the 'employment' column in the toy dataset\")\nemp_series_imputer\n```\n\n    The series-imputer for the 'employment' column in the toy dataset\n    -----------------------------------------------------------------\n\n    DefaultPredictiveImputer(cat_imputer=MultinomialLogisticImputer(),\n                 cat_kwgs=None,\n                 num_imputer=PMMImputer(am=None, asd=10, bm=None, bsd=10, fill_value='random', init='auto',\n          neighbors=5, sample=1000, sig=1, tune=1000),\n                 num_kwgs=None)\n\n\n\nThe code above returns the value of `si.statistics_` for the `employment` key. We observe that our `si` `Imputer` fit the `employment` column using the `DefaultPredictiveImputer`. In fact, `si` fits all the columns in our toy dataset with the `DefaultPredictiveImputer`. This occurs because we did not specify a strategy when creating an instance of the `Imputer`. Therefore, the `Imputer` set each column's strategy to `default predictive` - the default strategy deployed when a user does not set the imputation strategy.\n\nThe `DefaultPredictiveImputer` **is an example of a series-imputer**. Specifically, it is the series-imputer that maps to any column with the `default predictive` strategy. As we mentioned above, each series-imputer is a class itself that implements a specific imputation model following a general set of rules to which all series-imputers must adhere. The `DefaultPredictiveImputer` is actually one of the most complex because it has to be flexible enough to fit numeric and categorical columns. In fact, the `DefaultPredictiveImputer` delegates its work to other series-imputers depending on the column type, as showin the code below.\n\n\n```python\nprint_header(\"The series imputer for categorical columns within DefaultPredictiveImputer\")\nprint(emp_series_imputer.cat_imputer)\nprint()\nprint_header(\"The series imputer for numerical columns within DefaultPredictiveImputer\")\nprint(emp_series_imputer.num_imputer)\n```\n\n    The series imputer for categorical columns within DefaultPredictiveImputer\n    --------------------------------------------------------------------------\n    MultinomialLogisticImputer()\n    \n    The series imputer for numerical columns within DefaultPredictiveImputer\n    ------------------------------------------------------------------------\n    PMMImputer(am=None, asd=10, bm=None, bsd=10, fill_value='random', init='auto',\n          neighbors=5, sample=1000, sig=1, tune=1000)\n\n\nThe `DefaultPredictiveImputer` class takes the `cat_imputer` and `num_imputer` arguments. The `cat_imputer` is the series-imputer that the `DefaultPredictiveImputer` uses when it comes across a categorical column (i.e. `object` datatype in `pandas` `DataFrames`). The `num_imputer` is the series-imputer that a `DefaultPredictiveImputer` users when it comes across a numerical column (i.e. `numeric` datatype in `pandas` `Dataframes`). `cat_imputer` is set to `MultinomialLogisticImputer` by default, while `num_imputer` is set to `PMMImputer` by default. `MultinomialLogisticImputer` is the series-imputer for the `multinomial logistic` strategy, while `PMMImputer` is the series-imputer for the `pmm` strategy. Therefore, the `default predictive` strategy is simply an abstraction that chooses between the `multionmial logistic` strategy and `pmm` strategy depending on the column datatype. Behind these strategies are their respective series-imputers. The `DefaultPredictiveImputer` is an abstraction that chooses to implement the `MultinomialLogisticImputer` or the `PMMImputer` depending on the column datatype.\n\nBefore we move to the next section, we'll look at the series-imputer for `age`.\n\n\n```python\nprint_header(\"The series-imputer for the 'employment' column in the toy dataset\")\nsi.statistics_[\"age\"]\n```\n\n    The series-imputer for the 'employment' column in the toy dataset\n    -----------------------------------------------------------------\n\n    DefaultPredictiveImputer(cat_imputer=MultinomialLogisticImputer(),\n                 cat_kwgs=None,\n                 num_imputer=PMMImputer(am=48.53533000546936, asd=10,\n          bm=array([-1.21117e-05,  8.27985e-03, -5.43102e+00,  1.04216e+01,\n            9.20132e+00,  7.98604e+00]),\n          bsd=10, fill_value='random', init='auto', neighbors=5, sample=1000,\n          sig=1, tune=1000),\n                 num_kwgs=None)\n\n\n\nThe astute reader will have already noticed that the `DefaultPredictiveImputer` for `age` is a bit different than the one used for `employment`, even though each column has the same strategy (`default predictive`). The differences are the values of the arguments within the `PMMImputer`. This is an implementation detail when the `PMMImputer` actually fits a dataset, which we will address in a separate tutorial on what each imputation strategy (or series-imputer) actually does. In our case, what's important to remember is that the `default predictive` strategy delegates to either `pmm` or `multinomial logistic` depending on the column type. Because `age` is a numerical column, it's `default predictive` strategy is actually `pmm`, so the `PMMImputer` is evoked to fit the imputation model for `age`. With `employment`, `PMMImputer` is never evoked since the column is categorical. The `MultinomialLogisticImputer` is evoked instead, as the `default predictive` strategy for `employment` delegates to `multinomial logistic`.\n\n#### 2.3 The Design Considerations behind Imputers\n\nIn Part I, section 2, we introduced the **design goals** of `Autoimpute`, noting that we want the package to be easy to use, flexible, and familiar to those with experience using packages in Python's machine learning ecosystem. In this section, we discuss the **design patterns** behind `Autoimpute` that help us realize our goals for the package. In this tutorial, we focus on the design considerations for Imputers only, specifically the `SingleImputer` and its series-imputer counterparts.\n\nLet's start by reviewing the relationship between **strategies and series-imputers**. The `SingleImputer` strategies are easily accessible from the `strategies` class attribute (actually inherited from the `BaseImputer`). Remember, the `strategies` attribute is simply a dictionary that maps strategies to series-imputers. The code below shows a `DataFrame` with the available strategies in the `SingleImputer` and the corresponding series-imputer that maps to each strategy. The final column contains the **classes that each series-imputer inherit from**. Note that we've removed the `default strategies`, because they are merely abstractions that pick between other strategies.\n\n\n```python\ninherited = lambda v: list(map(lambda cls_: cls_.__name__, v().__class__.__bases__))\nseries_imputer_df = pd.DataFrame({\n    \"strategy\": list(si.strategies.keys()), \n    \"series-imputer\": list(map(lambda v: v.__name__, si.strategies.values())),\n    \"inherited from\": [inherited(v) for v in si.strategies.values()]\n})\nno_default = np.where(~series_imputer_df.strategy.str.contains(\"default\"))[0]\nprint_header(\"Displaying the SingleImputer's strategies and corresponding series-imputers\")\nseries_imputer_df.loc[no_default, [\"strategy\", \"series-imputer\", \"inherited from\"]]\n```\n\n    Displaying the SingleImputer's strategies and corresponding series-imputers\n    ---------------------------------------------------------------------------\n\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",class:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"strategy"),r.a.createElement("th",null,"series-imputer"),r.a.createElement("th",null,"inherited from"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,"least squares"),r.a.createElement("td",null,"LeastSquaresImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"stochastic"),r.a.createElement("td",null,"StochasticImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"binary logistic"),r.a.createElement("td",null,"BinaryLogisticImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"multinomial logistic"),r.a.createElement("td",null,"MultinomialLogisticImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"bayesian least squares"),r.a.createElement("td",null,"BayesianLeastSquaresImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"bayesian binary logistic"),r.a.createElement("td",null,"BayesianBinaryLogisticImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"pmm"),r.a.createElement("td",null,"PMMImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"lrd"),r.a.createElement("td",null,"LRDImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"mean"),r.a.createElement("td",null,"MeanImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"median"),r.a.createElement("td",null,"MedianImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"mode"),r.a.createElement("td",null,"ModeImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"random"),r.a.createElement("td",null,"RandomImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"norm"),r.a.createElement("td",null,"NormImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"categorical"),r.a.createElement("td",null,"CategoricalImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"interpolate"),r.a.createElement("td",null,"InterpolateImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"locf"),r.a.createElement("td",null,"LOCFImputer"),r.a.createElement("td",null,"[ISeriesImputer]")),r.a.createElement("tr",null,r.a.createElement("td",null,"nocb"),r.a.createElement("td",null,"NOCBImputer"),r.a.createElement("td",null,"[ISeriesImputer]")))),r.a.createElement(f.a,{source:"\n\nAs we've previously stated, each strategy maps to a series-imputer prefixed by the strategy's name. The series-imputer is responsible for implementing the strategy-specific imputation model. Regardless of what the imputation model actually looks like, each series-imputer must implement the imputation model with just `fit`, `impute`, and `fit_impute` methods. This design is enforced by parent class `ISeriesImputer`. It is an abstract base class that specifies the way any of its children should behave. For those from Java / C# backgrounds, the `ISeriesImputer` assumes the role of **an interface**. The `ISeriesImputer` is the contract to which all series-imputers must adhere.\n\nAs we notice from the console output above, **all series-imputers inherit from `ISeriesImputer`** and therefore implement their imputation model using `fit`, `impute`, and `fit_impute` methods. Enforcing this contract may seem strict, but it makes all series-imputers standardized, and **it allows the `SingleImputer` to delegate its work to a series-imputer** without actually worrying about what the series-imputer does under the hood. Because every series-imputer must follow the \"imputation contract\" the `ISeriesImptuer` enforces, the `SingleImptuer` then knows every series-imputer will have the same methods. Therefore, when a user calls the `fit` method of the `SeriesImputer`, the `SeriesImputer` simply calls the `fit` method of the corresponding series-imputer specified for each column and waits for each series-imputer to respond with an imputation model. \n\nThe delegation design pattern has a number of benefits. First, it isolates dependencies, making it very simple to debug any issue a `SingleImputer` faces when fitting a dataset. Because the `SingleImputer` delegates its work to independent series-imputers, we can easily identify which series-imputer has a problem if an error is thrown. Next, this design is inherently flexible and easy to extend. Adding another imputer strategy requires a few steps, but those steps are quite clear. A user must create a series-imputer and strategy name, and as long as the series-imputer inherits from the `ISeriesImputer` and implements its \"imputation contract\", then `SingleImputer` can use the series-imputer out of the box. Lastly, this delegation pattern is inherently scalable if series-imputers work independently. The `SingleImputer` simply waits for series-imputers to return imputation models, so those imputation models can, in theory, fit in parallel.\n\nWe're currently working to make Imputers more scalable and extensible by default, and these advanced features will be the subject of later tutorials. For now, it's important to understand the relationship between strategies and series imputers, as well as the delegation pattern the `SingleImputer` users to dispatch its work to the proper helper class, or series-imputer, that builds a requested imputation model under a set of rules enforced by the `ISeriesImputer`.\n\n### 3. Customizing a SingleImputer through its Arguments\nIn the last section, we walked through the design of the `SingleImputer`, but we did not pay much attention to any of its arguments. In this section, we explore how we can **customize the SingleImputer** by tuning its arguments. As we tune arguments, we look at how the `statistics_` attribute of the `SingleImputer` changes. We learn that some of the arguments we pass to the `SingleImputer` alter the behavior of the `SingleImputer` itself, while other arguments modify the specific imputation models created by the series-imputers to which the `SingleImputer` delegates its work. The arguments that modify series-imputers give us control over the imputation models within a `SingleImputer`. We'll explore these in depth in the remainder of this tutorial. These arguments include:  \n* **`strategy`**  \n* **`imp_kwgs`**  \n* **`predictors`**  \n\n#### The strategy argument\nThe **strategy** argument is the most important argument within the `SingleImputer`. The list below shows all the strategies available to impute a column within a `DataFrame`. `predictive default` is the default strategy if a user does not specify one. As we observed in section 2, `predictive default` chooses the preferred strategy to use depending on a column's data type (`pmm` for numerical, `multinomial logistic` for categorical). Note that some of these strategies are for categorical data, while others are for numeric data. As we'll see later, the Imputers let the user know whether a strategy will work for a given column when you try to fit the imputation model.\n\n\n```python\nprint_header(\"Strategies Available for Imputation\")\nprint(list(SingleImputer().strategies.keys()))\n```\n\n    Strategies Available for Imputation\n    -----------------------------------\n    ['default predictive', 'least squares', 'stochastic', 'binary logistic', 'multinomial logistic', 'bayesian least squares', 'bayesian binary logistic', 'pmm', 'lrd', 'default univariate', 'default time', 'mean', 'median', 'mode', 'random', 'norm', 'categorical', 'interpolate', 'locf', 'nocb']\n\n\nWe have a wealth of imputation methods at our disposal, and we continue to make more available. That being said, imputation strategies are restricted to the list of strategies provided above. A user cannot even create an instance of an Imputer if the strategy he or she provides is not supported. Improper strategy specification throws a `ValueError`, as shown below. The traceback is removed to keep this tutorial clean, but the error below is clear. The `strategy` argument is validated when instantiating the class.\n\n\n```python\n# proviging a strategy not yet supported or that doens't exist\nprint_header(\"Creating a SingleImputer with an unsupported strategy\")\ntry:\n    SingleImputer(strategy=\"unsupported\")\nexcept ValueError as ve:\n    print(f\"{ve.__class__.__name__}: {ve}\")\n```\n\n    Creating a SingleImputer with an unsupported strategy\n    -----------------------------------------------------\n    ValueError: Strategy unsupported not a valid imputation method.\n     Strategies must be one of ['default predictive', 'least squares', 'stochastic', 'binary logistic', 'multinomial logistic', 'bayesian least squares', 'bayesian binary logistic', 'pmm', 'lrd', 'default univariate', 'default time', 'mean', 'median', 'mode', 'random', 'norm', 'categorical', 'interpolate', 'locf', 'nocb'].\n\n\nSo how can utilize supported strategies? We can set the **strategy** in three ways:  \n* As a **string**, which broadcasts the strategy across every column in the DataFrame\n* As a **list or tuple**, where the position of strategies in the iterator are applied to the corresponding column\n* As a **dictionary**, where the key is the column we want to impute, and the value is the strategy to use\n\nWe advise the **dictionary method**, as it is the most explicit and allows the user to impute all or a subset of the columns in a DataFrame. It is also the least prone to unexpected behavior and errors when trying to fit the imputation model. Let's look at some examples below, where we run into problems with the string and iterator method but have better control with the dictionary method.\n\n\n```python\n# string strategy broadcasts across all strategies\nsi_str = SingleImputer(strategy=\"mean\")\n\n# list strategy, where each item is a corresponding strategy\nsi_list = SingleImputer(strategy=[\"mean\", \"binary logistic\", \"median\"])\n\n# dictionary strategy, where we specify column and strategy together\n# Note that with the dictionary, we can specify a SUBSET of columns to impute\nsi_dict = SingleImputer(strategy={\"gender\":\"categorical\", \"salary\": \"pmm\"})\n```\n\nNote that we instantiated each `SingleImputer` with no issues yet. We provided valid types (string, iterator, or dictionary) for the `strategy` argument, and each `strategy` we provided is one of the strategies supported. So we are able to at least crete an instance of our class. **But that does not necessarily mean the strategies we've chosen will work with the columns of our `DataFrame`**. This is something we cannot validate **until the user fits the `Imputer` to a dataset** because the `Imputer` itself knows nothing about the dataset's columns or the column types until the `Imputer` is fit. We will see how this plays out below when we try to fit sample Imputers to our toy dataset.\n\nFirst, let's try to fit `si_str`, which is a `Imputer` but won't work with our toy data.\n\n\n```python\n# fitting the string strategy, which yields an error\nprint_header(\"Fitting si_str, a SingleImputer that broadcasts strategy='mean'\")\ntry:\n    si_str.fit(toy_df)\nexcept TypeError as te:\n    print(f\"{te.__class__.__name__}: {te}\")\n```\n\n    Fitting si_str, a SingleImputer that broadcasts strategy='mean'\n    ---------------------------------------------------------------\n    TypeError: mean not appropriate for Series employment of type object.\n\n\nWhile a valid imputer, `si_str` failed to fit the dataset. We set the strategy as `mean`, so the `si_str` `Imputer` broadcast `mean` to all columns in our dataset. `mean` worked fine when imputing `age`, the first column, because `age` is numerical. But an error occurred when we tried to take the `mean` of the `employment` column. We cannot take the `mean` of a categorical column such as `employment`, so the `Imputer` throws an error. (Note that the same error would have occurred when the `Imputer` reached the `gender` column.)\n\nTherefore, we must be very careful when setting the strategy with a string since that strategy is broadcast to all columns in the DataFrame. When setting strategy with a string, we must ensure that we want the same strategy for each column, and we must ensure that our DataFrame does not contain any columns that the strategy cannot fit. As we'll see later, we could have specified `mean` for every column except `employment` and `gender` if we had used a dictionary, or we could have specified a different strategy for `employment` and `gender` had we used a list.\n\nNext, we'll attempt to fit `si_list`, an `Imputer` that uses a list of strategies.\n\n\n```python\n# fitting the string strategy, which yields an error\nprint_header(\"Fitting si_list, a SingleImputer with a list of strategies to apply\")\ntry:\n    si_list.fit(toy_df)\nexcept ValueError as ve:\n    print(f\"{ve.__class__.__name__}: {ve}\")\n```\n\n    Fitting si_list, a SingleImputer with a list of strategies to apply\n    -------------------------------------------------------------------\n    ValueError: Length of columns not equal to number of strategies.\n    Length of columns: 5\n    Length of strategies: 3\n\n\nWith `si_list`, a different problem occurs. If we use a list as the value to the `strategy` argument, the list **must contain one strategy per column**. When we created the `Imputer`, the list contained 3 valid strategies, so no problem with instantiation. But when we tried to fit the `Imputer` to the dataset, the `Imputer` noticed the dataset had 5 columns. The `Imputer` does not know how to handle the fourth and fifth column, and the `Imputer` has not been told explicitly to ignore these columns, so a `ValueError` is thrown.\n\nFinally, let's examine `si_dict`:\n\n\n```python\nprint_header(\"Fitting si_dict, a SingleImputer with a dictionary of strategies to apply\")\nsi_dict.fit(toy_df)\n```\n\n    Fitting si_dict, a SingleImputer with a dictionary of strategies to apply\n    -------------------------------------------------------------------------\n\n    SingleImputer(copy=True, imp_kwgs=None, predictors='all', seed=None,\n           strategy={'gender': 'categorical', 'salary': 'pmm'},\n           visit='default')\n\n\n\nThe `si_dict` `Imputer` successfully fit the toy dataset. For `gender`, it used the `categorical` method. For `salary`, it used `pmm`. Because we did not specify any imputation method for `age`, `weight`, or `employment`, these columns are not imputed and are ignored. Not only is the dictionary method more flexible, but it can drastically speed up the time it takes an `Imputer` to fit a model if we have hundreds of columns but only need to impute a couple of them.\n\n**So what did the fit method actually do?** As we learned in section 2, Imputers delegate the work for each column to a series-imputer that maps to the specified strategy. In this case, we specified `pmm` for `salary` and `categorical` for `gender`, so `si_dict` delegated work for `salary` to the `PMMImputer` and delegated work for `gender` to the `CategoricalImputer`. \n\n\n```python\nprint_header(\"Accessing statistics after fitting the si_dict Imputer\")\nsi_dict.statistics_\n```\n\n    Accessing statistics after fitting the si_dict Imputer\n    ------------------------------------------------------\n\n    {'gender': CategoricalImputer(),\n     'salary': PMMImputer(am=887784.0482856919, asd=10,\n           bm=array([  -3475.07484,    -351.17341,  -90835.31078, -197110.63707,\n            -128389.04932,   83669.62554]),\n           bsd=10, fill_value='random', init='auto', neighbors=5, sample=1000,\n           sig=1, tune=1000)}\n\n\n\nNote the difference between the statistics in the code above and the statistics in section 2.1. In section 2.1, all columns including `gender` and `salary` received the `DefaultPredictiveImputer`. Here, `gender` receives the series-imputer `CategoricalImputer`, which maps to the `categorical` strategy; `salary` receives the series-imputer `PMMImputer` which maps to the `pmm` strategy; and the remaining columns receive **no series-imputer at all because we explicitly ignored them.** Both `si` and `si_dict` are instances of the same `SingleImputer`, but each looks very different because we've tuned the `strategy` argument. Because the `strategy` argument maps to a series-imputer that creates each column's imputation model, `si` and `si_dict` end up with completely different `statistics_`. If we actually impute data, each of these Imputers would produce a very different set of imputations. \n\n#### The img_kwgs argument\nObserve that at times, the **series-imputers take arguments of their own**. This occurs because certain strategies may need additional information in order to implement their imputation model. In the example above, the `categorical` strategy has no additional parameters necessary to pass to its series-imputer, while the `pmm` strategy has 10 additional parameters that control the way the `PmmImputer` fits a dataset. While each strategy's respective series-imputer sets default arguments as well, we want to be able to control those arguments to alter how the strategy ultimately works and performs. We can do so using the **imp_kwgs** argument in the `SingleImputer`, which by default is set to `None`.\n\nLet's review our `si_dict` `Imputer`. By default, it's value is set to `None`.\n\n\n```python\nsi_dict\n```\n\n    SingleImputer(copy=True, imp_kwgs=None, predictors='all', seed=None,\n           strategy={'gender': 'categorical', 'salary': 'pmm'},\n           visit='default')\n\n\nWhen `imp_kwgs` is `None`, all series-imputers for given strategies use their default arguments. We observe those default once we've fit our `Imputer` by accessing its statistics.\n\n\n```python\nsi_dict.statistics_\n```\n\n    {'gender': CategoricalImputer(),\n     'salary': PMMImputer(am=887784.0482856919, asd=10,\n           bm=array([  -3475.07484,    -351.17341,  -90835.31078, -197110.63707,\n            -128389.04932,   83669.62554]),\n           bsd=10, fill_value='random', init='auto', neighbors=5, sample=1000,\n           sig=1, tune=1000)}\n\n\n\nWe specified `pmm` (predictive mean matching) as the strategy to use for the `salary` column. `pmm` is a semi-parametric method that borrows logic from bayesian regression, linear regression, and nearest neighbor search. We'll cover details of imputation algorithms in another tutorial, but for now, let's review some of the arguments the `PMMImputer` takes. Specifically, we'll focus on **neighbors** and **fill_value**.  \n* **neighbors** is the number of observations `pmm` will use to determine an imputation value.  \n* If **fill_value** is set to **random**, `pmm` randomly selects one of the `n` neighbors as the imputation. Random is the default.  \n* If the **fill_value** is set to **mean**, `pmm` takes the mean of the `n` neighbors and uses the mean as the imputation.  \n\nWe'll create two new Imputers to demonstrate how we can tweak the behavior of the `PMMImputer` through `imp_kwgs`.\n\n\n```python\n# using the column name\nsi_dict_col = SingleImputer(\n    strategy={\"gender\":\"categorical\", \"salary\": \"pmm\", \"weight\": \"pmm\"},\n    imp_kwgs={\"salary\": {\"neighbors\": 10, \"fill_value\": \"mean\"}}\n)\n\n# using the strategy name\nsi_dict_strat = SingleImputer(\n    strategy={\"gender\":\"categorical\", \"salary\": \"pmm\", \"weight\": \"pmm\"},\n    imp_kwgs={\"pmm\": {\"neighbors\": 10, \"fill_value\": \"mean\"}}\n)\n```\n\n\n```python\n# fit the si_dict_col imputer\nsi_dict_col.fit(toy_df)\n```\n\n    SingleImputer(copy=True,\n           imp_kwgs={'salary': {'neighbors': 10, 'fill_value': 'mean'}},\n           predictors='all', seed=None,\n           strategy={'gender': 'categorical', 'salary': 'pmm', 'weight': 'pmm'},\n           visit='default')\n\n```python\n# fit the si_dict_strat imputer\nsi_dict_strat.fit(toy_df)\n```\n\n    SingleImputer(copy=True,\n           imp_kwgs={'pmm': {'neighbors': 10, 'fill_value': 'mean'}},\n           predictors='all', seed=None,\n           strategy={'gender': 'categorical', 'salary': 'pmm', 'weight': 'pmm'},\n           visit='default')\n\nWe fit two new imputers to the our toy dataset. The first `Imputer`, `si_dict_col`, sets `pmm` as the strategy for both `weight` and `salary`. Additionally, it sets `imp_kwgs` to fine-tune **the pmm algorithm for the salary column only**. Our second imputer, `si_dict_strat`, sets the same strategies, but it sets `imp_kwgs` to fine-tune **any column that uses the pmm algorithm**.\n\nAs a result, the `si_dict_col` `Imputer` uses a customized version of `pmm` for salary but the default version of `pmm` for weight. We can see the differences by accessing the Imputer's statistics, as shown below. The `weight` column has the default number of `neighbors` (5) and the default `fill_value` (random). The `salary` column, on the other hand, uses 10 `neighbors`, and its `fill_value` is set to `mean`.\n\n\n```python\nprint_header(\"PMMImputer for weight\")\nprint(si_dict_col.statistics_[\"weight\"])\nprint_header(\"PMMImputer for salary\")\nprint(si_dict_col.statistics_[\"salary\"])\nprint_header(\"Number of neighbors used for weight vs. salary\")\nprint(\n    {\"number of neighbors for salary\": si_dict_col.statistics_[\"salary\"].neighbors, \n     \"number of neighbors for weight\": si_dict_col.statistics_[\"weight\"].neighbors}\n)\n```\n\n    PMMImputer for weight\n    ---------------------\n    PMMImputer(am=203.908957825882, asd=10,\n          bm=array([ 9.18857e-02, -1.35828e-05, -1.92486e+00,  2.90271e+00,\n            2.08853e+00,  8.95211e+00]),\n          bsd=10, fill_value='random', init='auto', neighbors=5, sample=1000,\n          sig=1, tune=1000)\n    PMMImputer for salary\n    ---------------------\n    PMMImputer(am=887784.0482856919, asd=10,\n          bm=array([  -3475.07484,    -351.17341,  -90835.31078, -197110.63707,\n           -128389.04932,   83669.62554]),\n          bsd=10, fill_value='mean', init='auto', neighbors=10, sample=1000,\n          sig=1, tune=1000)\n    Number of neighbors used for weight vs. salary\n    ----------------------------------------------\n    {'number of neighbors for salary': 10, 'number of neighbors for weight': 5}\n\n\nThe `si_dict_strat` `Imputer` applies `imp_kwgs` to **any column that has pmm as its strategy**. Therefore, the customized `PMMImputer` applies to both the `salary` and the `weight` column, as shown below. The number of neighbors is the same for both columns, as is the `fill_value`.\n\n\n```python\nprint_header(\"PMMImputer for weight\")\nprint(si_dict_strat.statistics_[\"weight\"])\nprint_header(\"PMMImputer for salary\")\nprint(si_dict_strat.statistics_[\"salary\"])\nprint_header(\"Number of neighbors used for weight vs. salary\")\nprint(\n    {\"number of neighbors for salary\": si_dict_strat.statistics_[\"salary\"].neighbors, \n     \"number of neighbors for weight\": si_dict_strat.statistics_[\"weight\"].neighbors}\n)\n```\n\n    PMMImputer for weight\n    ---------------------\n    PMMImputer(am=203.908957825882, asd=10,\n          bm=array([ 9.18857e-02, -1.35828e-05, -1.92486e+00,  2.90271e+00,\n            2.08853e+00,  8.95211e+00]),\n          bsd=10, fill_value='mean', init='auto', neighbors=10, sample=1000,\n          sig=1, tune=1000)\n    PMMImputer for salary\n    ---------------------\n    PMMImputer(am=887784.0482856919, asd=10,\n          bm=array([  -3475.07484,    -351.17341,  -90835.31078, -197110.63707,\n           -128389.04932,   83669.62554]),\n          bsd=10, fill_value='mean', init='auto', neighbors=10, sample=1000,\n          sig=1, tune=1000)\n    Number of neighbors used for weight vs. salary\n    ----------------------------------------------\n    {'number of neighbors for salary': 10, 'number of neighbors for weight': 10}\n\n\nTherefore, we can customize the series-imputer for any given imputation strategy **by column** or **by strategy itself**. While we demonstrated this behavior for `pmm`, the same logic applies to any imputation strategy that takes additional arguments. Below is an example using `interpolate` as an imputation strategy. We'll specify `imp_kwgs` by column.\n\n\n```python\n# interpolate with imp_kwgs using the column name\nsi_interp = SingleImputer(\n    strategy={\"salary\": \"interpolate\", \"weight\": \"interpolate\"},\n    imp_kwgs={\"salary\": {\"fill_strategy\": \"linear\"}, \"weight\": {\"fill_strategy\": \"quadratic\"}}\n)\n\n# fit the imputer\nsi_interp.fit(toy_df)\n```\n\n    SingleImputer(copy=True,\n           imp_kwgs={'salary': {'fill_strategy': 'linear'}, 'weight': {'fill_strategy': 'quadratic'}},\n           predictors='all', seed=None,\n           strategy={'salary': 'interpolate', 'weight': 'interpolate'},\n           visit='default')\n\n\n\n\n```python\nsi_interp.statistics_\n```\n\n    {'salary': InterpolateImputer(end=None, fill_strategy='linear', order=None, start=None),\n     'weight': InterpolateImputer(end=None, fill_strategy='quadratic', order=None,\n               start=None)}\n\n\n\nIn this case, we've created an `Imputer` that uses **linear interpolation** to impute `salary` and **quadratic interpolation** to impute `weight`. This is just another example of how we can use `imp_kwgs` along with a column or strategy to fine-tune the imputation algorithm itself when we create an instance of an `Imputer`.\n\nThe code samples above demonstrate proper usage of the `imp_kwgs` argument. To put it succinctly, **`imp_kwgs` extends the `strategy` argument**. They provide additional control over the imputation model itself should a user want to deviate from a series-imputer's default arguments.\n\n#### The predictors argument\nSo far, we've seen that the `strategy` argument determines which type of imputation model(s) we want to use, while the `imp_kwgs` argument provides additional parameters to specified strategies to control the behavior of their respective series-imputer. These arguments focus on **tuning the imputation model itself**. We also have options to specify **what the imputation model depends on** if the imputation model is multivariate predictive. In other words, we can control what **predictors** a series-imputer uses to fit an imputation model. Unsuprisingly, we control this behavior through the `SeriesImputer` **`predictor`** argument.\n\n\n```python\nsi_all = SingleImputer()\nprint_header(\"Show the SingleImputer as well as its predictor argument\")\nprint(si_all)\nprint(f\"si_all predictors: {si.predictors}\")\n```\n\n    Show the SingleImputer as well as its predictor argument\n    --------------------------------------------------------\n    SingleImputer(copy=True, imp_kwgs=None, predictors='all', seed=None,\n           strategy='default predictive', visit='default')\n    si_all predictors: all\n\n\nAs with the `strategy` and `imp_kwgs` arguments, we have flexibility surrounding how we specify the `predictors` argument. By default, `SingleImputers` set `predictors` to `all`. This default means that **all columns are used** to build an imputation model for strategies that depend on predictors. We could also use a list or a dictionary to specify the predictors we want to use. We demonstrate both below.\n\n#### When are imp_kwgs ignored?\nWhile `imp_kwgs` extend the `strategy` argument, there isn't always something to extend. In some cases, the `imp_kwgs` argument is ignored even if a user explicitly defines it. There are two primary cases where this behavior occurs:  \n\n1. **strategy's series-imputer has no class instance attributes**  \n2. **When using a dictionary to define strategies, the user does not provide a strategy for a column**  \n\n**Mean imputation** is a good example of the first case above. Mean imputation simply takes the mean of observed values in an array. Therefore, Autoimpute's `MeanImputer` does not need any arguments to instantiate it. Therefore, if a user defines arbitrary arguments within the `imp_kwgs` for a column with `mean` as its strategy, the `imp_kwgs` are ignored. This behavior is intended, and an **error is not thrown**. Autoimpute avoids an error in this case because some strategies take arbitrary keyword arguments (a.k.a. $kwargs$) to their series-imputer. Because Python ignores unused $kwargs$, so does Autoimpute. A good example of this is the `MultinomialLogisticImputer` seen in examples above. While the class does not appear to have any arguments, it actually takes $kwargs$. Under the hood, the $kwargs$ are passed to an instance of `sklearn` `LogisticRegression`, which has a number of possible keyword arguments to control how the regression behaves. We'll cover the details of each series-imputer in subsequent tutorials. For now, it's important to understand when and why `imp_kwgs` are ignored. In general, remember that `imp_kwgs` are evaluated **if and only if they logically extend the strategy in question**.\n\nThe second case is a bit more straightforward. If a end user defines strategies in a dictionary and does not provide a strategy for a column, the column is ignored when fitting a dataset. When a column is ignored, it does not matter what its `imp_kwgs` are, as the `Imputer` is not building a model for that column.\n\n\n```python\nprint_header(\"Creating a SingleImputer with a list of predictors for all columns\")\npreds_list = [\"weight\", \"age\", \"salary\"]\nsi_list_pred = SingleImputer(predictors=preds_list)\nprint(si_list_pred)\nprint()\nprint_header(\"Creating a SingleImputer with a dictionary of predictors by column\")\npreds_dict = {\"weight\": [\"age\", \"gender\"], \"salary\": [\"education\", \"age\"]}\nsi_dict_pred = SingleImputer(predictors=preds_dict)\nprint(si_dict_pred)\n```\n\n    Creating a SingleImputer with a list of predictors for all columns\n    ------------------------------------------------------------------\n    SingleImputer(copy=True, imp_kwgs=None,\n           predictors=['weight', 'age', 'salary'], seed=None,\n           strategy='default predictive', visit='default')\n    \n    Creating a SingleImputer with a dictionary of predictors by column\n    ------------------------------------------------------------------\n    SingleImputer(copy=True, imp_kwgs=None,\n           predictors={'weight': ['age', 'gender'], 'salary': ['education', 'age']},\n           seed=None, strategy='default predictive', visit='default')\n\n\nIf we use a list or iterator to specify predictors, **imputation models use the columns within the list as the set of predictors**. The `SingleImputer` will take care of removing a column as a predictor for that column's imputation strategy. For example, when it's time to impute `salary` in the example above, the `SingleImputer` will use `weight` and `age`. Similarly, the `SingleImputer` will use `weight` and `salary` when it's time to impute `age`.\n\nWe can also use a dictionary to specify predictors. As with the `strategy` argument, we specify the predictors column-wise. Unlike `strategy`, **the predictors argument is implicitly set to `all` for any columns we do not provide predictors**. This implicit behavior stems from the fact that **predictors support the strategy argument**. If a user specifies a multivariate strategy for a column but does not specify `predictors`, we cannot ignore the predictors argument, or else the strategy will not work.\n\n#### When are predictors ignored?\nIn certain cases, the `SingleImputer` simply ignores the `predictors` argument. This behavior occurs when `predictors` simply are not relevant. As mentioned above, `predictors` support a `strategy`. So if the `strategy` does not need `predictors`, then the `predictors` are ingored. Let's review two cases where this may occur:  \n\n1. **Univariate strategies**  \n2. **When using a dictionary to define strategies, the user does not provide a strategy for a column**  \n\nThe first case applies to univariate strategies. Similarly to `imp_kwgs`, **predictors are evaluated if and only if they logically support a strategy**. Univariate strategies don't need predictors ever, so the predictors are always ignored. Using the same example as `imp_kwgs`, take the `MeanImputer`. The `MeanImputer` uses observed values in an array to determine the imputation model. Because it never needs predictors to determine the `mean` of an array, the `MeanImputer` ignores predictors in all cases. They are not needed to support mean imputation, so they are politely ignored.\n\nThe second case is the same as we saw with `imp_kwgs`. Columns with no strategy have no imputation model, so obviously they do not need predictors.\n\nThis concludes the second part of this series. In this tutorial, we examined how the `SingleImputer` works under the hood. We reviewed the design patterns that make `Autoimpute Imputers` meet the high level design goals discussed in part I of this series. We then explored some of the arguments a `SingleImputer` takes. Specifically, we addressed the arguments that impact the behavior of respective series-imputers (i.e. the arguments that determine the structure of the imputation models). In the next turorial, we extend these concepts to the `MultipleImputer`. We observe how the `MultipleImputer` is simply multiple `SingleImputers` at work under the hood.\n\n\n",escapeHtml:!1,renderers:{code:E}}))}}]),t}(a.Component),k=function(e){function t(){return Object(l.a)(this,t),Object(m.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(c.a)(t,e),Object(o.a)(t,[{key:"render",value:function(){return r.a.createElement("div",{className:"imputer-III"},r.a.createElement(f.a,{source:'\n\n\n## Getting the Most out of the Imputer Classes: Part III\n---\nThis tutorial is part III of a comprehensive overview of `Autoimpute` Imputers. It includes:  \n\n1. Prepping Environment and Creating Data  \n2. A Quick Review of the `SingleImputer`  \n3. Issues with Single Imputation  \n4. Multiple Imputation in `Autoimpute`  \n5. `MultipleImputer` under the Hood  \n6. Considerations during Multiple Imputation  \n\n### 1. Prepping Environment and Creating Data\nAs with most tutorials, we begin by prepping our environment. Here, we import familiar packages for data analysis as well as the `SingleImputer` and `MultipleImputer` from `Autoimpute`. We also import visualization methods native to `Autoimpute` that help us visually understand the multiple imputation framework. Finally, we generate sample data, sticking with two variables, predictor **x** and response **y**. Only **y** has observations with missing values.\n\n\n```python\n# imports for analyzing missing data\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm, binom\nfrom autoimpute.imputations import SingleImputer, MultipleImputer\nfrom autoimpute.visuals import plot_imp_dists, plot_imp_boxplots, plot_imp_swarm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings("ignore")\nsns.set(context="talk", rc={\'figure.figsize\':(11.7,8.27)})\n\n# helper functions used throughout this project\nprint_header = lambda msg: print(f"{msg}\\n{\'-\'*len(msg)}")\n\n# seed to follow along\nnp.random.seed(654654)\n\n# generate 400 data points\nN = np.arange(400)\n\n# helper function for this data\nvary = lambda v: np.random.choice(np.arange(v))\n\n# create correlated, random variables\na = 2\nb = 1/2\neps = np.array([norm(0, vary(30)).rvs() for n in N])\ny = (a + b*N + eps) / 100                         \nx = (N + norm(10, vary(250)).rvs(len(N))) / 100\n \n# 30% missing in y\ny[binom(1, 0.3).rvs(len(N)) == 1] = np.nan\n\n# collect results in a dataframe \ndata_miss = pd.DataFrame({"y": y, "x": x})\nsns.scatterplot(x="x", y="y", data=data_miss)\nplt.show()\n```\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"imputer-III-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/imputer/imputer-III-scatter.png"}),r.a.createElement(f.a,{source:'\n\n### 2. A Quick Review of the SingleImputer\nThe dataset we create above is missing roughly 30% of the values in **y**. As we\'ve seen in the first two tutorials, we can use the `SingleImputer` in `Autoimpute` to replace the missing values with plausible imputations. In the code below, we create a default instance of the `SingleImputer` and assign it the variable name `si`. Because **y** is numeric, `si` implements the predictive mean matching algorithm by default to generate imputations when we call its `fit_transform` method. \n\n\n```python\n# create an instance of the single imputer and impute the data\nsi = SingleImputer()\nsi_data_full = si.fit_transform(data_miss)\n\n# print the results\nprint_header("Results from SingleImputer running PMM on column y one time")\nconc = pd.concat([data_miss.head(20), si_data_full.head(20)], axis=1)\nconc.columns = ["x", "y_orig", "x_imp", "y_imp"]\nconc[["x", "y_orig", "y_imp"]]\n```\n\n    Auto-assigning NUTS sampler...\n    Initializing NUTS using jitter+adapt_diag...\n    Multiprocess sampling (4 chains in 4 jobs)\n    NUTS: [\u03c3, beta, alpha]\n    Sampling 4 chains: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [00:02<00:00, 2214.42draws/s]\n\n\n    Results from SingleImputer running PMM on column y one time\n    -----------------------------------------------------------\n\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",class:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y original"),r.a.createElement("th",null,"y imputed"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,"0.661921"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.826949")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.190313"),r.a.createElement("td",null,"0.130094"),r.a.createElement("td",null,"0.130094")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.055594"),r.a.createElement("td",null,"0.289676"),r.a.createElement("td",null,"0.289676")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.118157"),r.a.createElement("td",null,"0.354103"),r.a.createElement("td",null,"0.354103")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.149828"),r.a.createElement("td",null,"-0.009910"),r.a.createElement("td",null,"-0.009910")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.545609"),r.a.createElement("td",null,"0.002967"),r.a.createElement("td",null,"0.002967")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.401257"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.311536")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.609314"),r.a.createElement("td",null,"0.058539"),r.a.createElement("td",null,"0.058539")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.053159"),r.a.createElement("td",null,"0.063187"),r.a.createElement("td",null,"0.063187")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.015037"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.118795")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.134157"),r.a.createElement("td",null,"0.086663"),r.a.createElement("td",null,"0.086663")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.221629"),r.a.createElement("td",null,"0.303863"),r.a.createElement("td",null,"0.303863")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.525348"),r.a.createElement("td",null,"0.092465"),r.a.createElement("td",null,"0.092465")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.900651"),r.a.createElement("td",null,"0.068560"),r.a.createElement("td",null,"0.068560")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.801863"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.139153")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.787607"),r.a.createElement("td",null,"0.029584"),r.a.createElement("td",null,"0.029584")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.613273"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.063187")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.502887"),r.a.createElement("td",null,"0.417373"),r.a.createElement("td",null,"0.417373")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.596454"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.864781")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.106348"),r.a.createElement("td",null,"0.068833"),r.a.createElement("td",null,"0.068833")))),r.a.createElement(f.a,{source:'\n\nNext, we compare the first 20 records after imputation to the original dataset. As expected, the new dataset has no missing values. In the following code block, we print the index of the records in **y** that have been imputed. Because **x** had no missing data, no imputations were made. **y**, on the other hand, contains 124 imputations.\n\n\n```python\nprint_header("Index of imputed values in each column")\nprint(si.imputed_)\nprint()\nprint_header("Number of imputations in each column")\nprint([f"{k}: {len(v)}" for k, v in si.imputed_.items()])\n```\n\n    Index of imputed values in each column\n    --------------------------------------\n    {\'x\': [], \'y\': [0, 6, 9, 14, 16, 18, 20, 26, 29, 30, 41, 43, 47, 49, 54, 56, 59, 66, 67, 71, 73, 75, 80, 83, 86, 87, 88, 93, 98, 100, 116, 117, 118, 121, 122, 128, 132, 134, 138, 142, 146, 147, 149, 153, 159, 168, 171, 177, 178, 180, 182, 183, 185, 186, 187, 191, 192, 195, 199, 200, 201, 204, 209, 225, 228, 231, 233, 235, 236, 237, 242, 245, 247, 249, 250, 252, 259, 266, 269, 271, 272, 273, 276, 279, 284, 290, 296, 298, 302, 308, 311, 313, 318, 320, 321, 323, 325, 326, 327, 330, 332, 335, 339, 341, 346, 351, 353, 354, 358, 359, 363, 365, 368, 374, 379, 380, 382, 383, 384, 389, 392, 396, 398, 399]}\n    \n    Number of imputations in each column\n    ------------------------------------\n    [\'x: 0\', \'y: 124\']\n\n\n### 3. Issues with Single Imputation\nAfter performing single imputation, we now have a complete dataset. We may feel that we are ready to deploy machine learning models because all the records are complete. This assumption, however, glosses over a key problem with single imputation.\n\nSingle imputation produces a set of single values for each missing observation in each imputed variable. In our case, the set represents "point estimates" for each missing observation within **y**. If we use a single-imputed dataset downstream in a machine learning model such as linear or logistic regression, the model is not be able to differentiate real values from imputed values and treats both the same. Unfortunately, this process disregards the uncertain nature that governs the true value of the each imputed record. **Instead, we should treat each missing value as a random variable**. Therefore, each missing value should be represented by a distribution of potentially viable imputations rather than a single point estimate. In contrast, observed values have no uncertainty, and therefore, their variance is naturally zero. We do not need to represent observed data using a distribution because we know each observed value with certainty. But the imputed value of missing data points could change if we run the imputation procedure again. Therefore, we should not treat missing data points the same as we treat observed data points, even after imputation takes place.\n\nWhether imputed values change and by how much depends on the imputation method used and the relationship between features in the observed samples. We must consider these factors when assessing the performance of imputation algorithms in the mulitple imputation framework. What\'s important to remmeber now is the way in which we approach missing data points before and after imputation. Single imputation treats missing data points the same as it does observed data points, while multiple imputation treats missing observations as random variables governed by a distribution of possible imputed values. To retain the variance associated with a respective distribution, we must use multiple imputation instead of single imputation.\n\n### 4. Multiple Imputation in Autoimpute\nMultiple imputation strives to solve the issues with single imputation. When we peform multiple imputation, we impute the same dataset multiple times, and we store a separate copy of each imputed dataset. The difference between imputed values in each dataset depends on the structure of the observed data and the imputation algorithm used. Regardless, multiple imputation gives us a framework with which we can treat each missing record as a random variable. We can produce multiple imputations for each missing value and then assess the variance that results from mutliple imputations.\n\nIn the code below, we create an instance of the `MultipleImputer`, named `mi`. We then apply `mi` 5 times (the default in `Autoimpute`). We store the results in another variable, named `mi_data_full`. The variable represents a list of tuples, where the first value in each tuple is the imputation number (1-5 in this case) and the second value in each tuple is the imputed dataset for a given imputation. We then concatenate the **y** Series from each imputation and compare the results to the original dataset. We notice that the imputed values for **y** are often different between the 5 imputations. \n\n\n```python\n# create an instance of the multiple imputer and impute the data\nmi = MultipleImputer(return_list=True)\nmi_data_full = mi.fit_transform(data_miss)\n\n# print the results\nprint_header("Results from MultipleImputer running PMM on column y five times")\nimps = pd.concat([mi_data_full[i][1]["y"].to_frame() for i in range(len(mi_data_full))], axis=1)\nconc = pd.concat([data_miss.head(20), imps.head(20)], axis=1)\nconc.columns = ["x", "y_orig", "y_1", "y_2", "y_3", "y_4", "y_5"]\nconc\n```\n\n    Auto-assigning NUTS sampler...\n    Initializing NUTS using jitter+adapt_diag...\n    Multiprocess sampling (4 chains in 4 jobs)\n    NUTS: [\u03c3, beta, alpha]\n    Sampling 4 chains: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [00:03<00:00, 1964.50draws/s]\n    Auto-assigning NUTS sampler...\n    Initializing NUTS using jitter+adapt_diag...\n    Multiprocess sampling (4 chains in 4 jobs)\n    NUTS: [\u03c3, beta, alpha]\n    Sampling 4 chains: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [00:03<00:00, 1974.05draws/s]\n    Auto-assigning NUTS sampler...\n    Initializing NUTS using jitter+adapt_diag...\n    Multiprocess sampling (4 chains in 4 jobs)\n    NUTS: [\u03c3, beta, alpha]\n    Sampling 4 chains: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [00:02<00:00, 2105.42draws/s]\n    Auto-assigning NUTS sampler...\n    Initializing NUTS using jitter+adapt_diag...\n    Multiprocess sampling (4 chains in 4 jobs)\n    NUTS: [\u03c3, beta, alpha]\n    Sampling 4 chains: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [00:02<00:00, 2150.44draws/s]\n    Auto-assigning NUTS sampler...\n    Initializing NUTS using jitter+adapt_diag...\n    Multiprocess sampling (4 chains in 4 jobs)\n    NUTS: [\u03c3, beta, alpha]\n    Sampling 4 chains: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6000/6000 [00:02<00:00, 2114.16draws/s]\n\n\n    Results from MultipleImputer running PMM on column y five times\n    ---------------------------------------------------------------\n\n\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",class:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"x"),r.a.createElement("th",null,"y original"),r.a.createElement("th",null,"y imp 1"),r.a.createElement("th",null,"y imp 2"),r.a.createElement("th",null,"y imp 3"),r.a.createElement("th",null,"y imp 4"),r.a.createElement("th",null,"y imp 5"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,"0.661921"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.058539"),r.a.createElement("td",null,"0.826949"),r.a.createElement("td",null,"0.826949"),r.a.createElement("td",null,"0.417373"),r.a.createElement("td",null,"0.029584")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.190313"),r.a.createElement("td",null,"0.130094"),r.a.createElement("td",null,"0.130094"),r.a.createElement("td",null,"0.130094"),r.a.createElement("td",null,"0.130094"),r.a.createElement("td",null,"0.130094"),r.a.createElement("td",null,"0.130094")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.055594"),r.a.createElement("td",null,"0.289676"),r.a.createElement("td",null,"0.289676"),r.a.createElement("td",null,"0.289676"),r.a.createElement("td",null,"0.289676"),r.a.createElement("td",null,"0.289676"),r.a.createElement("td",null,"0.289676")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.118157"),r.a.createElement("td",null,"0.354103"),r.a.createElement("td",null,"0.354103"),r.a.createElement("td",null,"0.354103"),r.a.createElement("td",null,"0.354103"),r.a.createElement("td",null,"0.354103"),r.a.createElement("td",null,"0.354103")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.149828"),r.a.createElement("td",null,"-0.009910"),r.a.createElement("td",null,"-0.009910"),r.a.createElement("td",null,"-0.009910"),r.a.createElement("td",null,"-0.009910"),r.a.createElement("td",null,"-0.009910"),r.a.createElement("td",null,"-0.009910")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.545609"),r.a.createElement("td",null,"0.002967"),r.a.createElement("td",null,"0.002967"),r.a.createElement("td",null,"0.002967"),r.a.createElement("td",null,"0.002967"),r.a.createElement("td",null,"0.002967"),r.a.createElement("td",null,"0.002967")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.401257"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.766211"),r.a.createElement("td",null,"0.766211"),r.a.createElement("td",null,"0.469095"),r.a.createElement("td",null,"0.030568"),r.a.createElement("td",null,"0.932830")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.609314"),r.a.createElement("td",null,"0.058539"),r.a.createElement("td",null,"0.058539"),r.a.createElement("td",null,"0.058539"),r.a.createElement("td",null,"0.058539"),r.a.createElement("td",null,"0.058539"),r.a.createElement("td",null,"0.058539")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.053159"),r.a.createElement("td",null,"0.063187"),r.a.createElement("td",null,"0.063187"),r.a.createElement("td",null,"0.063187"),r.a.createElement("td",null,"0.063187"),r.a.createElement("td",null,"0.063187"),r.a.createElement("td",null,"0.063187")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.015037"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.071464"),r.a.createElement("td",null,"0.071464"),r.a.createElement("td",null,"0.208594"),r.a.createElement("td",null,"0.256477"),r.a.createElement("td",null,"0.134639")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.134157"),r.a.createElement("td",null,"0.086663"),r.a.createElement("td",null,"0.086663"),r.a.createElement("td",null,"0.086663"),r.a.createElement("td",null,"0.086663"),r.a.createElement("td",null,"0.086663"),r.a.createElement("td",null,"0.086663")),r.a.createElement("tr",null,r.a.createElement("td",null,"-0.221629"),r.a.createElement("td",null,"0.303863"),r.a.createElement("td",null,"0.303863"),r.a.createElement("td",null,"0.303863"),r.a.createElement("td",null,"0.303863"),r.a.createElement("td",null,"0.303863"),r.a.createElement("td",null,"0.303863")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.525348"),r.a.createElement("td",null,"0.092465"),r.a.createElement("td",null,"0.092465"),r.a.createElement("td",null,"0.092465"),r.a.createElement("td",null,"0.092465"),r.a.createElement("td",null,"0.092465"),r.a.createElement("td",null,"0.092465")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.900651"),r.a.createElement("td",null,"0.068560"),r.a.createElement("td",null,"0.068560"),r.a.createElement("td",null,"0.068560"),r.a.createElement("td",null,"0.068560"),r.a.createElement("td",null,"0.068560"),r.a.createElement("td",null,"0.068560")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.801863"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.029584"),r.a.createElement("td",null,"0.139153"),r.a.createElement("td",null,"0.139153"),r.a.createElement("td",null,"0.736597"),r.a.createElement("td",null,"0.549241")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.787607"),r.a.createElement("td",null,"0.029584"),r.a.createElement("td",null,"0.029584"),r.a.createElement("td",null,"0.029584"),r.a.createElement("td",null,"0.029584"),r.a.createElement("td",null,"0.029584"),r.a.createElement("td",null,"0.029584")),r.a.createElement("tr",null,r.a.createElement("td",null,"-1.613273"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.289676"),r.a.createElement("td",null,"0.063187"),r.a.createElement("td",null,"0.424593"),r.a.createElement("td",null,"0.235977"),r.a.createElement("td",null,"0.424593")),r.a.createElement("tr",null,r.a.createElement("td",null,"0.502887"),r.a.createElement("td",null,"0.417373"),r.a.createElement("td",null,"0.417373"),r.a.createElement("td",null,"0.417373"),r.a.createElement("td",null,"0.417373"),r.a.createElement("td",null,"0.417373"),r.a.createElement("td",null,"0.417373")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.596454"),r.a.createElement("td",null,"NaN"),r.a.createElement("td",null,"0.315037"),r.a.createElement("td",null,"0.776069"),r.a.createElement("td",null,"0.088169"),r.a.createElement("td",null,"0.458626"),r.a.createElement("td",null,"1.192725")),r.a.createElement("tr",null,r.a.createElement("td",null,"1.106348"),r.a.createElement("td",null,"0.068833"),r.a.createElement("td",null,"0.068833"),r.a.createElement("td",null,"0.068833"),r.a.createElement("td",null,"0.068833"),r.a.createElement("td",null,"0.068833"),r.a.createElement("td",null,"0.068833")))),r.a.createElement(f.a,{source:'\n\n\nThe differences between the imputed values demonstrate the uncertainty we have in the true value of each missing data point. We can visually depict the differences between each imputation using a number of helper methods native to `Autoimpute`. In the code sections below, we demonstrate four plots to help us understand the differences between the observed data and the full data after each imputation round.\n\n#### Distribution Plot\nThe distribution plot shows the kernel density estimate of the observed data as well as each complete dataset after each imputation round. Note the distributions are not the same. The differences result from the fact that each missing data point generates different imputed values at each iteration.\n\n\n```python\nplot_imp_dists(mi_data_full, mi, "y")\n```\n\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"imputer-III-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/imputer/imputer-III-dist.png"}),r.a.createElement(f.a,{source:'\n\n#### Box Plot\nThe boxplot also shows the observed distribution for **y** and the distribution after each imputation. These boxplots are another way to visualize the distribution of a variable and how that distribution changes when multiple imputations take place, each of which treats a missing data point as a random variable. \n\n\n```python\nplot_imp_boxplots(mi_data_full, mi, "y", side_by_side=True)\n```\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"imputer-III-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/imputer/imputer-III-box.png"}),r.a.createElement(f.a,{source:'\n\n#### Swarm Plot\nThe swarm plot helps visualize the actual "point estimate" imputed records take in each imputation round. It most clearly demonstrates the differences between each imputation iteration. Some imputed values are the same from iteration to iteration, but others are clearly distinct. For example, imputation round 1 and 5 (below) contain an imputed value at the the upper tail of the distribution of **y**, while imputation round 3 and 4 contain an imputed value at the bottom tail of the sitribution of **y**. \n\n\n```python\nplot_imp_swarm(mi_data_full, mi, "y")\n```\n\n\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"imputer-III-swarm",src:"https://kearnz.github.io/autoimpute-tutorials/img/imputer/imputer-III-swarm.png"}),r.a.createElement(f.a,{source:"\n\n### 5. MultipleImputer under the Hood\nIn the sections above, we focused on the effect of the multiple imputation procedure. We skip over the implementation details because we want to emphasize how multiple imputation handles each missing record. We stress that missing records are now treated as random variables, and we demonstrate that each imputation round essentially draws a record from the distribution that governs each missing record. In this section, we examine how `Autoimpute` implements multiple imputation and how the `MultipleImputer` works under the hood. We begin by printing `mi` to the console.\n\n\n```python\nprint_header(\"Instance of the MultipleImputer\")\nmi\n```\n\n    Instance of the MultipleImputer\n    -------------------------------\n\n    MultipleImputer(imp_kwgs=None, n=5, predictors='all', return_list=True,\n            seed=None, strategy='default predictive', visit='default')\n\n\n\nThe `MultipleImputer` has some familiar arguments. Specifically, it includes the `imp_kwgs`, `predictors`, and `strategy` arguments that we go over in detail in part II These arguments appear in both the `SingleImputer` and `MultipleImputer` because the latter simply implements independent instances of the former for each of the `n` imputations it performs. Let's observe the `statistics_` property of the `MultipleImputer`.\n\n\n```python\nprint_header(\"Statistics property of the MultipleImputer\")\nmi.statistics_\n```\n\n    Statistics property of the MultipleImputer\n    ------------------------------------------\n\n    {1: SingleImputer(copy=True, imp_kwgs=None, predictors={'x': 'all', 'y': 'all'},\n            seed=None, strategy='default predictive', visit='default'),\n     2: SingleImputer(copy=True, imp_kwgs=None, predictors={'x': 'all', 'y': 'all'},\n            seed=None, strategy='default predictive', visit='default'),\n     3: SingleImputer(copy=True, imp_kwgs=None, predictors={'x': 'all', 'y': 'all'},\n            seed=None, strategy='default predictive', visit='default'),\n     4: SingleImputer(copy=True, imp_kwgs=None, predictors={'x': 'all', 'y': 'all'},\n            seed=None, strategy='default predictive', visit='default'),\n     5: SingleImputer(copy=True, imp_kwgs=None, predictors={'x': 'all', 'y': 'all'},\n            seed=None, strategy='default predictive', visit='default')}\n\n\nThe `MultipleImputer` creates a new instance of the `SingleImputer` class for each imputation. Therefore, the key of the `statistics_` dictionary represents the imputation round, and the value represents the `SingleImputer` that handles a given iteration. **The imp_kwgs and strategy arguments are the same for each iteration, but the predictors argument can change**. While we refrain from going into detail in this tutorial, we could pass a list to the `predictors` argument, and each imputation round would use the predictors in the respective position of the list. The `imp_kwgs` and `strategy` must be the same, however, to make the imputation model comparable across each imputation round.\n\nThe only new argument to familarize ourselves with is `n`: the number of imputation rounds we want to perform. We discuss considerations regarding the value of `n` in the next section. For now, note that `n=1` is the same as single imputation. As `n` approaches infinity, we should expect each missing data point to contain enough samples to construct its posterior distribution.\n\n### 6. Considerations during Multiple Imputation\n\nThe multiple imputation procedure essentially performs single imputation numerous times to generate multiple copies of the same dataset with (potentially) different values for the originally missing records. While multiple imputation accounts for the uncertainty in imputations, the method comes with its own challenges that one must consider.\n\n#### How many imputations?\nThe most obvious question that comes up during multiple imputation is how many imputations to perform. If we perform 1, we are essentially mimicking the behavior of the `SingleImputer`. If we perform more than 1, we should see some variance between each iteration. But how many do we need to perform for that variance to be \"sufficient\"? This question is a tough one to answer, and various literature covers the subject in more detail. `Autoimpute` defaults the number of imputations to 5. An end user can control the number of imputations through the **n** argument in the `MultipleImputer`. Note that the number of imputations generally increases the time it takes the `Imputer` to run. Additionally, setting **n** too high can consume a lot of memory, as the `Imputer` returns **n** datasets. To handle this, `Autoimpute` lazily evaluates imputations by default, although this behavior can be controlled through the `return_list` argument. Set it equal to `True` to return all imputations at once.\n\n#### What can change between imputations?\nTo generate variance between imputations, the end user can change the way in which each imputation round proceeds. The end user cannot change the strategy or the imp_kwgs between iterations, as each algorithm needs to be the same in order to make imputations comparable accross imputation rounds. That being said, the end user can change the **predictors** from round to round. Additionally, the end user can specify a **visit sequence** that shuffles the order in which columns are imputed. While \"left-to-right\" is the only method supported right now, we plan to support shuffling soon.\n\n#### How do we analyze multiply imputed data?\nThroughout the tutorial, we note that muliple imputation is necessary to retain the variance that occurs from the imputation procedure. That being said, it's not clear at this point what the end user should do with 5 datasets that have different values. For instance, how should one analyze multiply imputed data in a supervised machine learning pipeline? Luckily, `Autoimpute` extends linear and logistic regression to apply to multiply imputed data. Two classes - the `MiLinearRegression` and `MiLogisticRegression` - take in a missing dataset, produce multiple imputations, and return a supervised machine learning model that pools parameters automatically and provides summary parameter diagnostics. These classes are the subject of a subsequent tutorial.\n\n\n",escapeHtml:!1,renderers:{code:E}}))}}]),t}(a.Component),S=function(e){function t(){return Object(l.a)(this,t),Object(m.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(c.a)(t,e),Object(o.a)(t,[{key:"render",value:function(){return r.a.createElement("div",{className:"end-to-end"},r.a.createElement("div",{className:"ete-header-01"},r.a.createElement(f.a,{source:"\n## An End-to-End Analysis of Missing Data using Autoimpute\n\n* This tutorial demonstrates how an analyst can utilize the `Autoimpute` package to handle missing data from exploration through supervised learning.  \n* It presents **two examples, side by side**. The process for each example is the same, but the underlying datasets (and their missingness) differ.  \n* The example on the left explores data with **MCAR** missingness, while the example on the right examines data with **MAR** missingness.  \n",escapeHtml:!1,renderers:{code:E}})),r.a.createElement("div",{className:"ete-mcar"},r.a.createElement(f.a,{source:"\n## MCAR\n---\n* 500 observations for two features, **predictor x** and **response y**  \n* Correlation between the variables is **0.7**\n* Predictor **x** is fully observed, while response **y** is missing **40%** of the observations\n* The underlying missingness mechanism is **MCAR**  \n* Imputation methods explored: **mean**, **least squares**, **PMM**    \n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n### Imports and Data Preparation\n---\n```python\n'''Handling imports for analysis'''\n%matplotlib inline\n\n# general modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# autoimpute imports - utilities & visuals\nfrom autoimpute.utils import md_pattern, proportions\nfrom autoimpute.visuals import plot_md_locations, plot_md_percent\nfrom autoimpute.visuals import plot_imp_dists, plot_imp_boxplots\nfrom autoimpute.visuals import plot_imp_swarm, plot_imp_strip\nfrom autoimpute.visuals import plot_imp_scatter\n\n# autoimpute imports - imputations & analysis\nfrom autoimpute.imputations import MultipleImputer\nfrom autoimpute.analysis import MiLinearRegression\n\n# reading the full dataset and mcar dataset\nfull = pd.read_csv(\"full.csv\")\nmcar = pd.read_csv(\"mcar.csv\")\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n### Percent Missing by Feature\n---\n```python\n'''MCAR percent plot'''\nplot_md_percent(mcar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarPercent",className:"ete-percent",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-plot-md-percent.png"}),r.a.createElement(f.a,{source:"\n### Location of Missingness by Feature\n---\n```python\n'''MCAR location plot'''\nplot_md_locations(mcar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarLocation",className:"ete-locations",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-plot-md-locations.png"}),r.a.createElement(f.a,{source:"\n### Mean Imputation\n---\n```python\n'''MCAR mean imputation'''\n\n# create the mean imputer\nmi_mean_mcar = MultipleImputer(\n    strategy=\"mean\", n=5, return_list=True, seed=101\n)\n\n# print the mean imputer to console\nprint(mi_mean_mcar)\n\n# perform mean imputation procedure\nimp_mean_mcar = mi_mean_mcar.fit_transform(mcar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarMeanImputer",className:"ete-mean-imputer",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mean-imputer.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after Mean Imputation\n---\n```python\n\'\'\'MCAR distribution plots after mean imputation\'\'\'\n\n# distribution plot for mean imputation\nplot_imp_dists(\n    d=imp_mean_mcar,\n    mi=mi_mean_mcar, \n    col="y",\n    title="Distributions after Mean Imputation: MCAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for mean imputation\nplot_imp_boxplots(\n    d=imp_mean_mcar,\n    mi=mi_mean_mcar,\n    col="y",\n    title="Boxplots after Mean Imputation: MCAR"\n)\n\n# strip plot for mean imputation\nplot_imp_strip(\n    d=imp_mean_mcar,\n    mi=mi_mean_mcar,\n    col="y",\n    title="Imputed vs Observed Dists after Mean Imputation: MCAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarMeanImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-dist.png"}),r.a.createElement("img",{alt:"mcarMeanImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-box.png"}),r.a.createElement("img",{alt:"mcarMeanImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-strip.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after Least Squares Imputation\n---\n```python\n\'\'\'MCAR distribution plots after least squares imputation\'\'\'\n\n# create the least squares imputer\nmi_ls_mcar = MultipleImputer(\n    strategy="least squares", n=5, return_list=True, seed=101\n)\n\n# perform least squares imputation procedure\nimp_ls_mcar = mi_ls_mcar.fit_transform(mcar)\n\n# distribution plot for least squares imputation\nplot_imp_dists(\n    d=imp_ls_mcar,\n    mi=mi_ls_mcar, \n    col="y",\n    title="Distributions after Least Squares Imputation: MCAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for least squares imputation\nplot_imp_boxplots(\n    d=imp_ls_mcar,\n    mi=mi_ls_mcar,\n    col="y",\n    title="Boxplots after Least Squares Imputation: MCAR"\n)\n\n# strip plot for least squares imputation\nplot_imp_strip(\n    d=imp_ls_mcar,\n    mi=mi_ls_mcar,\n    col="y",\n    title="Imputed vs Observed Dists after Least Squares Imputation: MCAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarLsImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-dist.png"}),r.a.createElement("img",{alt:"mcarLsImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-box.png"}),r.a.createElement("img",{alt:"mcarLsImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-strip.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after PMM Imputation\n---\n```python\n\'\'\'MCAR distribution plots after PMM imputation\'\'\'\n\n# create the PMM imputer\nmi_pmm_mcar = MultipleImputer(\n    strategy="pmm", n=5, return_list=True, seed=101\n)\n\n# perform PMM imputation procedure\nimp_pmm_mcar = mi_pmm_mcar.fit_transform(mcar)\n\n# distribution plot for PMM imputation\nplot_imp_dists(\n    d=imp_pmm_mcar,\n    mi=mi_pmm_mcar, \n    col="y",\n    title="Distributions after PMM Imputation: MCAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for PMM imputation\nplot_imp_boxplots(\n    d=imp_pmm_mcar,\n    mi=mi_pmm_mcar,\n    col="y",\n    title="Boxplots after PMM Imputation: MCAR"\n)\n\n# swarm plot for PMM imputation\nplot_imp_swarm(\n    d=imp_pmm_mcar,\n    mi=mi_pmm_mcar,\n    col="y",\n    title="Imputed vs Observed Dists after PMM Imputation: MCAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarPmmOutput",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/pmm-bayes-mcar.png"}),r.a.createElement("img",{alt:"mcarPmmImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-dist.png"}),r.a.createElement("img",{alt:"mcarPmmImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-box.png"}),r.a.createElement("img",{alt:"mcarPmmImputerSwarm",className:"ete-swarm",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-swarm.png"}),r.a.createElement(f.a,{source:'\n### Linear Regression on Multiply Imputed Data\n---\n```python\n\'\'\'Regression after Multiple Imputation on MCAR\'\'\'\n\n# NOTE: Full, Listwise delete, and bias code not included\n\n# create the regression using custom imputers\nlm_mean_mcar = MiLinearRegression(mi=mi_mean_mcar)\nlm_ls_mcar = MiLinearRegression(mi=mi_ls_mcar)\nlm_pmm_mcar = MiLinearRegression(mi=mi_pmm_mcar)\nmodels_mcar = [lm_mean_mcar, lm_ls_mcar, lm_pmm_mcar]\n\n# a bit of manipulation to create one dataframe\nget_coeff = lambda lm_, x: lm_.summary().loc["x"].to_frame()\nres_mcar = pd.concat([get_coeff(lm, "x") for lm in models_mcar], axis=1)\nres_mcar.columns = ["mean", "least squares", "pmm"]\nres_mcar = res_mcar.T[["coefs", "std", "vw", "vb", "vt"]]\n\n# show the results\nres_mcar\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"method"),r.a.createElement("th",null,"coefs"),r.a.createElement("th",null,"std"),r.a.createElement("th",null,"vw"),r.a.createElement("th",null,"vb"),r.a.createElement("th",null,"vt"),r.a.createElement("th",null,"bias"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"full")),r.a.createElement("td",null,"0.70000"),r.a.createElement("td",null,"0.03200"),r.a.createElement("td",null,"0.00102"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00102"),r.a.createElement("td",null,"0%")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"delete")),r.a.createElement("td",null,"0.73915"),r.a.createElement("td",null,"0.04682"),r.a.createElement("td",null,"0.00219"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00219"),r.a.createElement("td",{className:"pos-bias"},"5.6%")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"mean")),r.a.createElement("td",null,"0.39330"),r.a.createElement("td",null,"0.03056"),r.a.createElement("td",null,"0.00093"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00093"),r.a.createElement("td",{className:"neg-bias"},"-43.8%")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"ls")),r.a.createElement("td",null,"0.73915"),r.a.createElement("td",null,"0.02570"),r.a.createElement("td",null,"0.00066"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00066"),r.a.createElement("td",{className:"pos-bias"},"5.6%")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"pmm")),r.a.createElement("td",null,"0.67692"),r.a.createElement("td",null,"0.05404"),r.a.createElement("td",null,"0.00128"),r.a.createElement("td",null,"0.00136"),r.a.createElement("td",null,"0.00292"),r.a.createElement("td",{className:"neg-bias"},"-3.3%")))),r.a.createElement(f.a,{source:'\n### Scatterplots after Imputation Methods\n---\n```python\n\'\'\'Visualizing effect of imputation on regression for scatter\'\'\'\n\n# scatterplot for mean\nplot_imp_scatter(\n    d=mcar, x="x", y="y", strategy="mean", color="y",\n    title="Scatter after Mean Imputation: MCAR"\n)\n\n# scatterplot for least squares\nplot_imp_scatter(\n    d=mcar, x="x", y="y", strategy="least squares", color="y",\n    title="Scatter after Least Squares Imputation: MCAR"\n)\n\n# scatterplot for pmm\nplot_imp_scatter(\n    d=mcar, x="x", y="y", strategy="pmm", color="y",\n    title="Scatter after PMM Imputation: MCAR"\n)\n\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"mcarMeanScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-mean-scatter.png"}),r.a.createElement("img",{alt:"mcarLsScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-ls-scatter.png"}),r.a.createElement("img",{alt:"mcarPmmScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mcar-pmm-scatter.png"})),r.a.createElement("div",{className:"ete-mar"},r.a.createElement(f.a,{source:"\n## MAR\n---\n* 500 observations for two features, **predictor x** and **response y**  \n* Correlation between the variables is **0.7**\n* Predictor **x** is missing **40%** of the observations, while response **y** is fully observed  \n* The underlying missingness mechanism is **MAR**  \n* Imputation methods explored: **mean**, **least squares**, **PMM**  \n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n### Imports and Data Preparation\n---\n```python\n'''Handling imports for analysis'''\n%matplotlib inline\n\n# general modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# autoimpute imports - utilities & visuals\nfrom autoimpute.utils import md_pattern, proportions\nfrom autoimpute.visuals import plot_md_locations, plot_md_percent\nfrom autoimpute.visuals import plot_imp_dists, plot_imp_boxplots\nfrom autoimpute.visuals import plot_imp_swarm, plot_imp_strip\nfrom autoimpute.visuals import plot_imp_scatter\n\n# autoimpute imports - imputations & analysis\nfrom autoimpute.imputations import MultipleImputer\nfrom autoimpute.analysis import MiLinearRegression\n\n# reading the full dataset and mar dataset\nfull = pd.read_csv(\"full.csv\")\nmar = pd.read_csv(\"mar.csv\")\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement(f.a,{source:"\n### Percent Missing by Feature\n---\n```python\n'''MAR percent plot'''\nplot_md_percent(mar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marPercent",className:"ete-percent",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-plot-md-percent.png"}),r.a.createElement(f.a,{source:"\n### Location of Missingness by Feature\n---\n```python\n'''MAR location plot'''\nplot_md_locations(mar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marLocation",className:"ete-locations",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-plot-md-locations.png"}),r.a.createElement(f.a,{source:"\n### Mean Imputation\n---\n```python\n'''MAR mean imputation'''\n\n# create the mean imputer\nmi_mean_mar = MultipleImputer(\n    strategy=\"mean\", n=5, return_list=True, seed=101\n)\n\n# print the mean imputer to console\nprint(mi_mean_mar)\n\n# perform mean imputation procedure\nimp_mean_mar = mi_mean_mar.fit_transform(mar)\n```\n",escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marMeanImputer",className:"ete-mean-imputer",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mean-imputer.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after Mean Imputation\n---\n```python\n\'\'\'MAR distribution plots after mean imputation\'\'\'\n\n# distribution plot for mean imputation\nplot_imp_dists(\n    d=imp_mean_mar,\n    mi=mi_mean_mar, \n    col="x",\n    title="Distributions after Mean Imputation: MAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for mean imputation\nplot_imp_boxplots(\n    d=imp_mean_mar,\n    mi=mi_mean_mar,\n    col="x",\n    title="Boxplots after Mean Imputation: MAR"\n)\n\n# strip plot for mean imputation\nplot_imp_strip(\n    d=imp_mean_mar,\n    mi=mi_mean_mar,\n    col="x",\n    title="Imputed vs Observed Dists after Mean Imputation: MAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marMeanImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-dist.png"}),r.a.createElement("img",{alt:"marMeanImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-box.png"}),r.a.createElement("img",{alt:"marMeanImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-strip.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after Least Squares Imputation\n---\n```python\n\'\'\'MAR distribution plots after least squares imputation\'\'\'\n\n# create the least squares imputer\nmi_ls_mar = MultipleImputer(\n    strategy="least squares", n=5, return_list=True, seed=101\n)\n\n# perform least squares imputation procedure\nimp_ls_mar = mi_ls_mar.fit_transform(mar)\n\n# distribution plot for least squares imputation\nplot_imp_dists(\n    d=imp_ls_mar,\n    mi=mi_ls_mar, \n    col="x",\n    title="Distributions after Least Squares Imputation: MAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for least squares imputation\nplot_imp_boxplots(\n    d=imp_ls_mar,\n    mi=mi_ls_mar,\n    col="x",\n    title="Boxplots after Least Squares Imputation: MAR"\n)\n\n# strip plot for least squares imputation\nplot_imp_strip(\n    d=imp_ls_mar,\n    mi=mi_ls_mar,\n    col="x",\n    title="Imputed vs Observed Dists after Least Squares Imputation: MAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marLsImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-dist.png"}),r.a.createElement("img",{alt:"marLsImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-box.png"}),r.a.createElement("img",{alt:"marLsImputerStrip",className:"ete-strip",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-strip.png"}),r.a.createElement(f.a,{source:'\n### Distribution Plots after PMM Imputation\n---\n```python\n\'\'\'MAR distribution plots after PMM imputation\'\'\'\n\n# create the PMM imputer\nmi_pmm_mar = MultipleImputer(\n    strategy="pmm", n=5, return_list=True, seed=101\n)\n\n# perform PMM imputation procedure\nimp_pmm_mar = mi_pmm_mar.fit_transform(mar)\n\n# distribution plot for PMM imputation\nplot_imp_dists(\n    d=imp_pmm_mar,\n    mi=mi_pmm_mar, \n    col="x",\n    title="Distributions after PMM Imputation: MAR",\n    separate_observed=False,\n    hist_observed=True,\n    hist_imputed=False\n)\n\n# box plot for PMM imputation\nplot_imp_boxplots(\n    d=imp_pmm_mar,\n    mi=mi_pmm_mar,\n    col="x",\n    title="Boxplots after PMM Imputation: MAR"\n)\n\n# swarm plot for PMM imputation\nplot_imp_swarm(\n    d=imp_pmm_mar,\n    mi=mi_pmm_mar,\n    col="x",\n    title="Imputed vs Observed Dists after PMM Imputation: MAR"\n)\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marPmmOutput",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/pmm-bayes-mar.png"}),r.a.createElement("img",{alt:"marPmmImputerDist",className:"ete-dist",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-dist.png"}),r.a.createElement("img",{alt:"marPmmImputerBox",className:"ete-box",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-box.png"}),r.a.createElement("img",{alt:"marPmmImputerSwarm",className:"ete-swarm",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-swarm.png"}),r.a.createElement(f.a,{source:'\n### Linear Regression on Multiply Imputed Data\n---\n```python\n\'\'\'Regression after Multiple Imputation on MAR\'\'\'\n\n# NOTE: Full, Listwise delete, and bias code not included\n\n# create the regression using custom imputers\nlm_mean_mar = MiLinearRegression(mi=mi_mean_mar)\nlm_ls_mar = MiLinearRegression(mi=mi_ls_mar)\nlm_pmm_mar = MiLinearRegression(mi=mi_pmm_mar)\nmodels_mar = [lm_mean_mar, lm_ls_mar, lm_pmm_mar]\n\n# a bit of manipulation to create one dataframe\nget_coeff = lambda lm_, x: lm_.summary().loc["x"].to_frame()\nres_mar = pd.concat([get_coeff(lm, "x") for lm in models_mar], axis=1)\nres_mar.columns = ["mean", "least squares", "pmm"]\nres_mar = res_mar.T[["coefs", "std", "vw", "vb", "vt"]]\n\n# show the results\nres_mar\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("table",{border:"1",className:"dataframe"},r.a.createElement("thead",null,r.a.createElement("tr",null,r.a.createElement("th",null,"method"),r.a.createElement("th",null,"coefs"),r.a.createElement("th",null,"std"),r.a.createElement("th",null,"vw"),r.a.createElement("th",null,"vb"),r.a.createElement("th",null,"vt"),r.a.createElement("th",null,"bias"))),r.a.createElement("tbody",null,r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"full")),r.a.createElement("td",null,"0.70000"),r.a.createElement("td",null,"0.03200"),r.a.createElement("td",null,"0.00102"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00102"),r.a.createElement("td",null,"0%")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"delete")),r.a.createElement("td",null,"0.66180"),r.a.createElement("td",null,"0.03865"),r.a.createElement("td",null,"0.00149"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00149"),r.a.createElement("td",{className:"neg-bias"},"-5.5%")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"mean")),r.a.createElement("td",null,"0.66180"),r.a.createElement("td",null,"0.04896"),r.a.createElement("td",null,"0.00240"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00240"),r.a.createElement("td",{className:"neg-bias"},"-5.5%")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"ls")),r.a.createElement("td",null,"0.86053"),r.a.createElement("td",null,"0.02889"),r.a.createElement("td",null,"0.00083"),r.a.createElement("td",null,"0.00000"),r.a.createElement("td",null,"0.00083"),r.a.createElement("td",{className:"pos-bias"},"22.9%")),r.a.createElement("tr",null,r.a.createElement("td",null,r.a.createElement("b",null,"pmm")),r.a.createElement("td",null,"0.69690"),r.a.createElement("td",null,"0.03707"),r.a.createElement("td",null,"0.00096"),r.a.createElement("td",null,"0.00034"),r.a.createElement("td",null,"0.00137"),r.a.createElement("td",{className:"neg-bias"},"-0.44%")))),r.a.createElement(f.a,{source:'\n### Scatterplots after Imputation Methods\n---\n```python\n\'\'\'Visualizing effect of imputation on regression for scatter\'\'\'\n\n# scatterplot for mean\nplot_imp_scatter(\n    d=mar, x="x", y="y", strategy="mean", color="x",\n    title="Scatter after Mean Imputation: MAR"\n)\n\n# scatterplot for least squares\nplot_imp_scatter(\n    d=mar, x="x", y="y", strategy="least squares", color="x",\n    title="Scatter after Least Squares Imputation: MAR"\n)\n\n# scatterplot for pmm\nplot_imp_scatter(\n    d=mar, x="x", y="y", strategy="pmm", color="x",\n    title="Scatter after PMM Imputation: MAR"\n)\n\n```\n',escapeHtml:!1,renderers:{code:E}}),r.a.createElement("img",{alt:"marMeanScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-mean-scatter.png"}),r.a.createElement("img",{alt:"marLsScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-ls-scatter.png"}),r.a.createElement("img",{alt:"marPmmScatter",className:"ete-scatter",src:"https://kearnz.github.io/autoimpute-tutorials/img/ete/ete-mar-pmm-scatter.png"})))}}]),t}(a.Component),N=function(e){var t=window.location.pathname+window.location.search,n=e.path===t?"nav-item active":"nav-item",a=e.disabled?"nav-link disabled":"nav-link";return r.a.createElement("li",{className:n},r.a.createElement("a",{onClick:function(){return e.onClick()},href:e.path,className:a},e.name,e.path===t?r.a.createElement("span",{className:"sr-only"},"(current)"):""))},T=function(e){function t(e){var n;return Object(l.a)(this,t),(n=Object(m.a)(this,Object(u.a)(t).call(this,e))).state={isToggleOn:!1},n}return Object(c.a)(t,e),Object(o.a)(t,[{key:"showDropdown",value:function(e){e.preventDefault(),this.setState(function(e){return{isToggleOn:!e.isToggleOn}})}},{key:"render",value:function(){var e=this,t="dropdown-menu"+(this.state.isToggleOn?" show":"");return r.a.createElement("li",{className:"nav-item dropdown"},r.a.createElement("a",{className:"nav-link dropdown-toggle",id:"navbarDropdown",role:"button","data-toggle":"dropdown","aria-haspopup":"true","aria-expanded":"false",onClick:function(t){e.showDropdown(t)}},this.props.name),r.a.createElement("div",{className:t,"aria-labelledby":"navbarDropdown"},this.props.children))}}]),t}(a.Component),x=function(e){function t(e){var n;return Object(l.a)(this,t),(n=Object(m.a)(this,Object(u.a)(t).call(this,e))).state={activeKey:1,df:"/autoimpute-tutorials/"},n}return Object(c.a)(t,e),Object(o.a)(t,[{key:"handleClick",value:function(e){this.setState({activeKey:e}),d()(".dropdown-menu").hasClass("show")&&(d()(".dropdown-menu").trigger("click"),d()(".dropdown-menu").removeClass("show"))}},{key:"render",value:function(){return r.a.createElement("div",{className:"main-page"},r.a.createElement("nav",{className:"navbar navbar-expand-lg"},r.a.createElement("a",{className:"navbar-brand",href:this.state.df},"Autoimpute"),r.a.createElement("div",{className:"collapse navbar-collapse",id:"navbarSupportedContent"},r.a.createElement("ul",{className:"navbar-nav mr-auto"},r.a.createElement(N,{name:"Home",onClick:this.handleClick.bind(this,1)}),r.a.createElement(N,{name:"Contact",onClick:this.handleClick.bind(this,2)}),r.a.createElement(T,{name:"Tutorials"},r.a.createElement("a",{className:"dropdown-item",href:this.props.path,onClick:this.handleClick.bind(this,3.1)},"Exploring Missingness"),r.a.createElement("a",{className:"dropdown-item",href:this.props.path,onClick:this.handleClick.bind(this,3.2)},"Imputers: Part I"),r.a.createElement("a",{className:"dropdown-item",href:this.props.path,onClick:this.handleClick.bind(this,3.3)},"Imputers: Part II"),r.a.createElement("a",{className:"dropdown-item",href:this.state.path,onClick:this.handleClick.bind(this,3.4)},"Imputers: Part III"),r.a.createElement("a",{className:"dropdown-item",href:this.state.path,onClick:this.handleClick.bind(this,3.5)},"Comparing Imputation Methods"),r.a.createElement("a",{className:"dropdown-item",href:this.state.path,onClick:this.handleClick.bind(this,3.6)},"End-to-End Analysis"))))),r.a.createElement("div",{className:"content"},1===this.state.activeKey?r.a.createElement(_,null):null,2===this.state.activeKey?r.a.createElement(h,null):null,3.1===this.state.activeKey?"Coming Soon!":null,3.2===this.state.activeKey?r.a.createElement(v,null):null,3.3===this.state.activeKey?r.a.createElement(M,null):null,3.4===this.state.activeKey?r.a.createElement(k,null):null,3.5===this.state.activeKey?r.a.createElement(I,null):null,3.6===this.state.activeKey?r.a.createElement(S,null):null))}}]),t}(r.a.Component),A=function(e){function t(){return Object(l.a)(this,t),Object(m.a)(this,Object(u.a)(t).apply(this,arguments))}return Object(c.a)(t,e),Object(o.a)(t,[{key:"render",value:function(){return r.a.createElement("div",null,r.a.createElement(x,null))}}]),t}(a.Component);Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));n(393);s.a.render(r.a.createElement(A,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then(function(e){e.unregister()})},40:function(e,t,n){}},[[33,1,2]]]);
//# sourceMappingURL=main.f1cd5bab.chunk.js.map